{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221c90b2-5321-43a6-b6d4-bb111080a359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496d7298-d546-4673-aef5-bfcf51863f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json, os\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6431f3cd-dba3-4c27-bb3a-99b336eba6c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Global Logger & Output Directories\n",
    "# -----------------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"OCRProduction\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "OUTPUT_FOLDER = \"/dbfs/mnt/mini-proj-dd/final_results\"\n",
    "CSV_FOLDER = os.path.join(OUTPUT_FOLDER, \"csv\")\n",
    "JSON_FOLDER = os.path.join(OUTPUT_FOLDER, \"json\")\n",
    "os.makedirs(CSV_FOLDER, exist_ok=True)\n",
    "os.makedirs(JSON_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33043d19-605f-4834-b763-a1a940bed780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: name 'read_cropped_section_image' is not defined\nERROR:OCRProduction:name 'read_cropped_section_image' is not defined\nINFO: Processing pipeline: cost_data\nINFO:OCRProduction:Processing pipeline: cost_data\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png with shape (105, 2502, 3)\nINFO:OCRProduction:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png with shape (105, 2502, 3)\nINFO: Cost OCR extraction complete.\nINFO:OCRProduction:Cost OCR extraction complete.\nINFO: COST DataFrame shape: (6, 2)\nINFO:OCRProduction:COST DataFrame shape: (6, 2)\nINFO: cost_data DataFrame shape: (6, 2) (Rows: 6, Columns: 2)\nINFO:OCRProduction:cost_data DataFrame shape: (6, 2) (Rows: 6, Columns: 2)\nINFO: cost_data saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/cost_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/cost_data.csv)\nINFO:OCRProduction:cost_data saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/cost_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/cost_data.csv)\nINFO: Processing pipeline: well_job\nINFO:OCRProduction:Processing pipeline: well_job\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png with shape (309, 2502, 3)\nINFO:OCRProduction:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png with shape (309, 2502, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- COST_DATA DataFrame ---\nShape: (6, 2)\n                        Key        Value\n0       Drilling AFE Amount         None\n1       Daily Drilling Cost  $167,006.63\n2  Cumulative Drilling Cost   $1,747,745\n3      Cumulative Well Cost   $1,914,752\n4            Daily Mud Cost   $54,185.80\n5       Cumulative Mud Cost  $299,370.66\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Well/Job OCR extraction complete.\nINFO:OCRProduction:Well/Job OCR extraction complete.\nINFO: WELL/JOB DataFrame shape: (19, 2)\nINFO:OCRProduction:WELL/JOB DataFrame shape: (19, 2)\nINFO: well_job DataFrame shape: (19, 2) (Rows: 19, Columns: 2)\nINFO:OCRProduction:well_job DataFrame shape: (19, 2) (Rows: 19, Columns: 2)\nINFO: well_job saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/well_job_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/well_job_data.csv)\nINFO:OCRProduction:well_job saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/well_job_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/well_job_data.csv)\nINFO: Processing pipeline: obs_int\nINFO:OCRProduction:Processing pipeline: obs_int\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png with shape (241, 942, 3)\nINFO:OCRProduction:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png with shape (241, 942, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- WELL_JOB DataFrame ---\nShape: (19, 2)\n             Key                     Value\n0      Well Name  Ross Fee 4371-31-7-15 MH\n1       Job Name                  Drilling\n2  Supervisor(s)   CHAD MILLER / ED COOLEY\n3          Field                       XBE\n4    Sec/Twn/Rng              31, 43N, 71W\n5          Phone              307-315-1908\n6          AFE #                    240098\n7          API #              49-005-78911\n8          Email  cyclone39@aec-denver.com\n9     Contractor                          \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: OBS_INT DataFrame shape: (5, 2)\nINFO:OCRProduction:OBS_INT DataFrame shape: (5, 2)\nINFO: obs_int DataFrame shape: (5, 2) (Rows: 5, Columns: 2)\nINFO:OCRProduction:obs_int DataFrame shape: (5, 2) (Rows: 5, Columns: 2)\nINFO: obs_int saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/obs_int_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/obs_int_data.csv)\nINFO:OCRProduction:obs_int saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/obs_int_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/obs_int_data.csv)\nINFO: Processing pipeline: bop\nINFO:OCRProduction:Processing pipeline: bop\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png with shape (71, 2502, 3)\nINFO:OCRProduction:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png with shape (71, 2502, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- OBS_INT DataFrame ---\nShape: (5, 2)\n             Type Number\n0      Stop Cards     14\n1     Hazard ID's      2\n2           JSA's      5\n3  Permit to Work     21\n4          Totals       \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: BOP OCR extraction complete.\nINFO:OCRProduction:BOP OCR extraction complete.\nINFO: BOP DataFrame shape: (3, 2)\nINFO:OCRProduction:BOP DataFrame shape: (3, 2)\nINFO: bop DataFrame shape: (3, 2) (Rows: 3, Columns: 2)\nINFO:OCRProduction:bop DataFrame shape: (3, 2) (Rows: 3, Columns: 2)\nINFO: bop saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/bop_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/bop_data.csv)\nINFO:OCRProduction:bop saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/bop_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/bop_data.csv)\nINFO: Processing pipeline: dir_info\nINFO:OCRProduction:Processing pipeline: dir_info\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png with shape (241, 1200, 3)\nINFO:OCRProduction:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png with shape (241, 1200, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BOP DataFrame ---\nShape: (3, 2)\n                  Key     Value\n0  Last BOP Test Date   6/30/24\n1      Last BOP Drill  7/3/2024\n2       Next BOP Test   7/25/24\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: DIR INFO DataFrame shape: (5, 3)\nINFO:OCRProduction:DIR INFO DataFrame shape: (5, 3)\nINFO: dir_info DataFrame shape: (5, 3) (Rows: 5, Columns: 3)\nINFO:OCRProduction:dir_info DataFrame shape: (5, 3) (Rows: 5, Columns: 3)\nINFO: dir_info saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/dir_info_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/dir_info_data.csv)\nINFO:OCRProduction:dir_info saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/dir_info_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/dir_info_data.csv)\nINFO: Processing pipeline: survey\nINFO:OCRProduction:Processing pipeline: survey\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png with shape (105, 2502, 3)\nINFO:OCRProduction:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png with shape (105, 2502, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIR_INFO DataFrame ---\nShape: (5, 3)\n           Category Daily Cumulative\n0   Circ/Cond Hours              6.8\n1     Sliding Hours   5.8       28.4\n2   Sliding Footage   247       1488\n3    Rotating Hours  17.8       75.9\n4  Rotating Footage  2821      18941\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: SURVEY - Grouped Rows: ['COST DATA COST DATA [BLANK]', 'Drilling AFE Amount: Daily Drilling Cost: $167,006.63 Cumulative Drilling Cost: $1,747,745 Cumulative Well Cost: $1,914,752\\nDaily Mud Cost: $54,185.80 Cumulative Mud Cost: $299,370.66']\nINFO:OCRProduction:SURVEY - Grouped Rows: ['COST DATA COST DATA [BLANK]', 'Drilling AFE Amount: Daily Drilling Cost: $167,006.63 Cumulative Drilling Cost: $1,747,745 Cumulative Well Cost: $1,914,752\\nDaily Mud Cost: $54,185.80 Cumulative Mud Cost: $299,370.66']\nINFO: SURVEY - All extracted lines: ['COST DATA COST DATA [BLANK]', 'Drilling AFE Amount: Daily Drilling Cost: $167,006.63 Cumulative Drilling Cost: $1,747,745 Cumulative Well Cost: $1,914,752', 'Daily Mud Cost: $54,185.80 Cumulative Mud Cost: $299,370.66']\nINFO:OCRProduction:SURVEY - All extracted lines: ['COST DATA COST DATA [BLANK]', 'Drilling AFE Amount: Daily Drilling Cost: $167,006.63 Cumulative Drilling Cost: $1,747,745 Cumulative Well Cost: $1,914,752', 'Daily Mud Cost: $54,185.80 Cumulative Mud Cost: $299,370.66']\nINFO: SURVEY - Data lines to parse: [['COST', 'DATA', 'COST', 'DATA', '[BLANK]'], ['Drilling', 'AFE', 'Amount:', 'Daily', 'Drilling'], ['Daily', 'Mud', 'Cost:', '$54,185.80', 'Cumulative']]\nINFO:OCRProduction:SURVEY - Data lines to parse: [['COST', 'DATA', 'COST', 'DATA', '[BLANK]'], ['Drilling', 'AFE', 'Amount:', 'Daily', 'Drilling'], ['Daily', 'Mud', 'Cost:', '$54,185.80', 'Cumulative']]\nINFO: SURVEY DataFrame shape: (3, 5)\nINFO:OCRProduction:SURVEY DataFrame shape: (3, 5)\nINFO: survey DataFrame shape: (3, 5) (Rows: 3, Columns: 5)\nINFO:OCRProduction:survey DataFrame shape: (3, 5) (Rows: 3, Columns: 5)\nINFO: survey saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/survey_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/survey_data.csv)\nINFO:OCRProduction:survey saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/survey_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/survey_data.csv)\nINFO: Processing pipeline: casing\nINFO:OCRProduction:Processing pipeline: casing\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png with shape (241, 2502, 3)\nINFO:OCRProduction:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png with shape (241, 2502, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SURVEY DataFrame ---\nShape: (3, 5)\n         MD Inclination  Azimuth         DLS         TVD\n0      COST        DATA     COST        DATA     [BLANK]\n1  Drilling         AFE  Amount:       Daily    Drilling\n2     Daily         Mud    Cost:  $54,185.80  Cumulative\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CASING - Line has fewer tokens than expected: ['CASING', 'CASING', '[BLANK]']\nWARNING:OCRProduction:CASING - Line has fewer tokens than expected: ['CASING', 'CASING', '[BLANK]']\nINFO: CASING - Skipping header line: ['Type', 'Size', 'Weight', 'Grade', 'Connection', 'Top', 'MD', 'Bottom', 'MD', 'TOC']\nINFO:OCRProduction:CASING - Skipping header line: ['Type', 'Size', 'Weight', 'Grade', 'Connection', 'Top', 'MD', 'Bottom', 'MD', 'TOC']\nWARNING: CASING - Line has fewer tokens than expected: ['OG']\nWARNING:OCRProduction:CASING - Line has fewer tokens than expected: ['OG']\nINFO: CASING DataFrame shape: (4, 8)\nINFO:OCRProduction:CASING DataFrame shape: (4, 8)\nINFO: casing DataFrame shape: (4, 8) (Rows: 4, Columns: 8)\nINFO:OCRProduction:casing DataFrame shape: (4, 8) (Rows: 4, Columns: 8)\nINFO: casing saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/casing_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/casing_data.csv)\nINFO:OCRProduction:casing saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/casing_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/casing_data.csv)\nINFO: Processing pipeline: consumables\nINFO:OCRProduction:Processing pipeline: consumables\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png with shape (209, 2502, 3)\nINFO:OCRProduction:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png with shape (209, 2502, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CASING DataFrame ---\nShape: (4, 8)\n           Type     Size   Weight  ...   Top MD Bottom MD      TOC\n0     Conductor   16.000    36.94  ...    32.00    108.00       16\n1       Surface   10.750     40.5  ...    31.17   2268.00       30\n2  Intermediate    7.625     29.7  ...    28.89   9857.70     2750\n3       [BLANK]  [BLANK]  [BLANK]  ...  [BLANK]   [BLANK]  [BLANK]\n\n[4 rows x 8 columns]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: CONSUMABLES DataFrame shape: (3, 5)\nINFO:OCRProduction:CONSUMABLES DataFrame shape: (3, 5)\nINFO: consumables DataFrame shape: (3, 5) (Rows: 3, Columns: 5)\nINFO:OCRProduction:consumables DataFrame shape: (3, 5) (Rows: 3, Columns: 5)\nINFO: consumables saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/consumables_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/consumables_data.csv)\nINFO:OCRProduction:consumables saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/consumables_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/consumables_data.csv)\nINFO: Processing pipeline: mud\nINFO:OCRProduction:Processing pipeline: mud\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png with shape (173, 2502, 3)\nINFO:OCRProduction:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png with shape (173, 2502, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONSUMABLES DataFrame ---\nShape: (3, 5)\n  Consumable Daily Received (gal)  ... Cumulative Used (gal) Daily on Hand (gal)\n0       Fuel              [BLANK]  ...                20,626               5,735\n1  CNG (DGE)                1,652  ...                 6,535             [BLANK]\n2   Mud Fuel                8,367  ...                24,150              11,643\n\n[3 rows x 5 columns]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Row 0 text: MUD MUD [BLANK]\nINFO:OCRProduction:Row 0 text: MUD MUD [BLANK]\nINFO: Row 1 text: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO:OCRProduction:Row 1 text: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO: Row 2 text: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO:OCRProduction:Row 2 text: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO: Row 3 text: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO:OCRProduction:Row 3 text: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO: Row 4 text: 4 5 1023 13 481 [BLANK]\nINFO:OCRProduction:Row 4 text: 4 5 1023 13 481 [BLANK]\nINFO: Header1: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO:OCRProduction:Header1: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO: Value1: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO:OCRProduction:Value1: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO: Header2: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO:OCRProduction:Header2: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO: Value2: 4 5 1023 13 481 [BLANK]\nINFO:OCRProduction:Value2: 4 5 1023 13 481 [BLANK]\nINFO: Tokens from data row 1: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00']\nINFO:OCRProduction:Tokens from data row 1: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00']\nINFO: Tokens from data row 2: ['4', '5', '1023', '13', '481', '[BLANK]']\nINFO:OCRProduction:Tokens from data row 2: ['4', '5', '1023', '13', '481', '[BLANK]']\nINFO: Combined tokens: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO:OCRProduction:Combined tokens: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO: Expected token count: 24, tokens extracted: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO:OCRProduction:Expected token count: 24, tokens extracted: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO: Mapped dictionary: {'Type': 'OBM', 'Weight In': '11.5', 'Weight Out': '11.5', 'pH': '[BLANK]', 'CAKE': '3', 'GELS (10s/10m/30m)': {'10s': '8', '10m': '25', '30m': '27'}, 'Oil/Water': '88/12', 'FV': '60.0', 'ES': '753', 'PV': '16', 'YP': '8', 'CL': '31,000', 'Ca': '326,667', 'LGS': '4.47', 'WL': '[BLANK]', 'HTHP Loss': '5.00', '3 RPM': '4', '6 RPM': '5', 'Mud Pits and Hole Volume': '1023', '24 Hr Loss': '13', 'Total Loss': '481', 'Comments': '[BLANK]'}\nINFO:OCRProduction:Mapped dictionary: {'Type': 'OBM', 'Weight In': '11.5', 'Weight Out': '11.5', 'pH': '[BLANK]', 'CAKE': '3', 'GELS (10s/10m/30m)': {'10s': '8', '10m': '25', '30m': '27'}, 'Oil/Water': '88/12', 'FV': '60.0', 'ES': '753', 'PV': '16', 'YP': '8', 'CL': '31,000', 'Ca': '326,667', 'LGS': '4.47', 'WL': '[BLANK]', 'HTHP Loss': '5.00', '3 RPM': '4', '6 RPM': '5', 'Mud Pits and Hole Volume': '1023', '24 Hr Loss': '13', 'Total Loss': '481', 'Comments': '[BLANK]'}\nINFO: mud DataFrame shape: (22, 2) (Rows: 22, Columns: 2)\nINFO:OCRProduction:mud DataFrame shape: (22, 2) (Rows: 22, Columns: 2)\nINFO: mud saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/mud_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/mud_data.csv)\nINFO:OCRProduction:mud saved: JSON(/dbfs/mnt/mini-proj-dd/final_results/json/mud_data.json), CSV(/dbfs/mnt/mini-proj-dd/final_results/csv/mud_data.csv)\nINFO: Processing pipeline: bha\nINFO:OCRProduction:Processing pipeline: bha\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MUD DataFrame ---\nShape: (22, 2)\n                  Key                                   Value\n0                Type                                     OBM\n1           Weight In                                    11.5\n2          Weight Out                                    11.5\n3                  pH                                 [BLANK]\n4                CAKE                                       3\n5  GELS (10s/10m/30m)  {'10s': '8', '10m': '25', '30m': '27'}\n6           Oil/Water                                   88/12\n7                  FV                                    60.0\n8                  ES                                     753\n9                  PV                                      16\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Error in pipeline bha: [Errno 2] No such file or directory: '/Workspace/Repos/divya.dhaipullay@zeussolutionsinc.com/automate_ddr/dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_?BHA.png'\nERROR:OCRProduction:Error in pipeline bha: [Errno 2] No such file or directory: '/Workspace/Repos/divya.dhaipullay@zeussolutionsinc.com/automate_ddr/dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_?BHA.png'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Global Logger & Output Directories\n",
    "# -----------------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"OCRProduction\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "OUTPUT_FOLDER = \"/dbfs/mnt/mini-proj-dd/final_results\"\n",
    "CSV_FOLDER = os.path.join(OUTPUT_FOLDER, \"csv\")\n",
    "JSON_FOLDER = os.path.join(OUTPUT_FOLDER, \"json\")\n",
    "os.makedirs(CSV_FOLDER, exist_ok=True)\n",
    "os.makedirs(JSON_FOLDER, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# COMMON UTILITY FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def safe_read_image(img_path):\n",
    "    local_path = img_path if not img_path.startswith(\"dbfs:\") else img_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {local_path}\")\n",
    "    logger.info(f\"Image loaded from {local_path} with shape {img.shape}\")\n",
    "    return img\n",
    "\n",
    "def preprocess_image(img, debug=False):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 15, 9)\n",
    "    if debug:\n",
    "        logger.info(\"Preprocessing completed (grayscale and threshold applied).\")\n",
    "    return thresh\n",
    "\n",
    "def detect_text_regions(thresh_img, debug=False):\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "    rois.sort(key=lambda b: (b[1], b[0]))\n",
    "    if debug:\n",
    "        logger.info(f\"Detected {len(rois)} text regions.\")\n",
    "    return rois\n",
    "\n",
    "def perform_ocr_on_rois(img, rois, debug=False):\n",
    "    results = []\n",
    "    for (x, y, w, h) in rois:\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        text = pytesseract.image_to_string(roi, config=\"--psm 6\").strip() or \"[BLANK]\"\n",
    "        results.append((x, y, w, h, text))\n",
    "        if debug:\n",
    "            logger.info(f\"OCR Box ({x},{y},{w},{h}): {text}\")\n",
    "    return results\n",
    "\n",
    "def perform_ocr(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return pytesseract.image_to_string(gray, config=\"--psm 6\")\n",
    "\n",
    "def extract_key_value_from_text(text, expected_keys):\n",
    "    combined = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "    combined = re.sub(r'\\s+', ' ', combined)\n",
    "    result = {}\n",
    "    for i, key in enumerate(expected_keys):\n",
    "        if i < len(expected_keys) - 1:\n",
    "            next_key = expected_keys[i+1]\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*?)(?=\\s*' + re.escape(next_key) + r'\\s*:|$)'\n",
    "        else:\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*)'\n",
    "        match = re.search(pattern, combined, re.IGNORECASE)\n",
    "        result[key] = match.group(1).strip() if match and match.group(1).strip() else None\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PARSING HELPERS\n",
    "# -----------------------------------------------------------------------------\n",
    "def group_ocr_results(roi_texts, row_tolerance=10):\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for row in rows:\n",
    "        row.sort(key=lambda c: c[0])\n",
    "        line = \" \".join(cell[4] for cell in row)\n",
    "        row_strings.append(line)\n",
    "    return row_strings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PIPELINE FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def pipeline_cost_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"Cost OCR extraction complete.\")\n",
    "    expected_keys = [\n",
    "        \"Drilling AFE Amount\", \"Daily Drilling Cost\", \"Cumulative Drilling Cost\",\n",
    "        \"Cumulative Well Cost\", \"Daily Mud Cost\", \"Cumulative Mud Cost\"\n",
    "    ]\n",
    "    extracted = extract_key_value_from_text(ocr_text, expected_keys)\n",
    "    df = pd.DataFrame(list(extracted.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"COST DataFrame shape: {df.shape}\")\n",
    "    return {\"COST DATA\": extracted}, df\n",
    "\n",
    "def process_well_job_info(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"Well/Job OCR extraction complete.\")\n",
    "    expected_keys = [\n",
    "        \"Well Name\", \"Job Name\", \"Supervisor(s)\", \"Field\", \"Sec/Twn/Rng\", \"Phone\",\n",
    "        \"AFE #\", \"API #\", \"Email\", \"Contractor\", \"Elevation\", \"RKB\",\n",
    "        \"Spud Date\", \"Days from Spud\", \"Days on Loc\", \"MD/TVD\", \"24 Hr Footage\",\n",
    "        \"Present Operations\", \"Activity Planned\"\n",
    "    ]\n",
    "    combined = \" \".join(line.strip() for line in ocr_text.splitlines() if line.strip())\n",
    "    combined = re.sub(r'\\s+', ' ', combined)\n",
    "    result = {}\n",
    "    for i, key in enumerate(expected_keys):\n",
    "        if i < len(expected_keys) - 1:\n",
    "            next_key = expected_keys[i+1]\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*?)(?=\\s*' + re.escape(next_key) + r'\\s*:|$)'\n",
    "        else:\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*)'\n",
    "        match = re.search(pattern, combined, re.IGNORECASE)\n",
    "        result[key] = match.group(1).strip() if match else \"\"\n",
    "    df = pd.DataFrame(list(result.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"WELL/JOB DataFrame shape: {df.shape}\")\n",
    "    return {\"WELL/JOB INFORMATION\": result}, df\n",
    "\n",
    "def process_obs_int(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    header_str = \"daily numbers: observation & intervention\"\n",
    "    all_texts = [t[4] for t in roi_texts]\n",
    "    types_list, numbers_list = [], []\n",
    "    for txt in all_texts:\n",
    "        clean = txt.strip()\n",
    "        if clean.lower() in [header_str, \"number\", \"[blank]\"]:\n",
    "            continue\n",
    "        try:\n",
    "            float(clean)\n",
    "            numbers_list.append(\"\" if clean.lower() == \"[blank]\" else clean)\n",
    "            continue\n",
    "        except ValueError:\n",
    "            pass\n",
    "        if \"\\n\" in clean:\n",
    "            for line in clean.splitlines():\n",
    "                line = line.strip()\n",
    "                if line and line.lower() != \"[blank]\":\n",
    "                    types_list.append(line)\n",
    "        else:\n",
    "            types_list.append(clean)\n",
    "    expected_count = 5\n",
    "    while len(numbers_list) < expected_count:\n",
    "        numbers_list.append(\"\")\n",
    "    types_list = types_list[:expected_count]\n",
    "    numbers_list = numbers_list[:expected_count]\n",
    "    structured = [{\"Type\": types_list[i], \"Number\": numbers_list[i]} for i in range(expected_count)]\n",
    "    df = pd.DataFrame(structured)\n",
    "    logger.info(f\"OBS_INT DataFrame shape: {df.shape}\")\n",
    "    return {\"DAILY NUMBERS: OBSERVATION & INTERVENTION\": structured}, df\n",
    "\n",
    "def process_bop(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"BOP OCR extraction complete.\")\n",
    "    patterns = {\n",
    "        \"Last BOP Test Date\": r\"Last BOP Test Date\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Last BOP Drill\": r\"Last BOP Drill\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Next BOP Test\": r\"Next BOP Test\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\"\n",
    "    }\n",
    "    result = {}\n",
    "    for key, regex in patterns.items():\n",
    "        match = re.search(regex, ocr_text, re.IGNORECASE)\n",
    "        result[key] = match.group(1) if match else \"\"\n",
    "    df = pd.DataFrame(list(result.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"BOP DataFrame shape: {df.shape}\")\n",
    "    return {\"BOP\": result}, df\n",
    "\n",
    "def build_dir_info_dict_from_rois(roi_texts, debug=False):\n",
    "    all_texts = [t[4] for t in roi_texts]\n",
    "    daily_cum_idx = next((i for i, txt in enumerate(all_texts)\n",
    "                           if \"daily\" in txt.lower() and \"cumulative\" in txt.lower()), None)\n",
    "    if daily_cum_idx is None:\n",
    "        logger.warning(\"Could not find 'Daily Cumulative' bounding box.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    cat_idx = daily_cum_idx + 1\n",
    "    if cat_idx >= len(all_texts):\n",
    "        logger.warning(\"No bounding box after 'Daily Cumulative'.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    categories_box = all_texts[cat_idx]\n",
    "    lines = [ln.strip() for ln in categories_box.split(\"\\n\") if ln.strip()]\n",
    "    if len(lines) < 5:\n",
    "        logger.warning(f\"Expected 5 category lines, got {len(lines)}: {lines}\")\n",
    "    def safe_get(idx):\n",
    "        return all_texts[idx] if 0 <= idx < len(all_texts) else \"\"\n",
    "    structured = []\n",
    "    for i in range(4):\n",
    "        cat_name = lines[i] if i < len(lines) else f\"Unknown Category {i+1}\"\n",
    "        daily_box = safe_get(cat_idx + 1 + (i * 2))\n",
    "        cum_box = safe_get(cat_idx + 2 + (i * 2))\n",
    "        structured.append({\n",
    "            \"Category\": cat_name,\n",
    "            \"Daily\": \"\" if daily_box == \"[BLANK]\" else daily_box,\n",
    "            \"Cumulative\": \"\" if cum_box == \"[BLANK]\" else cum_box\n",
    "        })\n",
    "    last_box = safe_get(cat_idx + 9)\n",
    "    last_cat = lines[4] if len(lines) >= 5 else \"Rotating Footage\"\n",
    "    remainder = last_box.replace(last_cat, \"\").strip()\n",
    "    tokens = remainder.split()\n",
    "    daily_val = tokens[0] if len(tokens) >= 2 else \"\"\n",
    "    cum_val = tokens[1] if len(tokens) >= 2 else \"\"\n",
    "    structured.append({\n",
    "        \"Category\": last_cat,\n",
    "        \"Daily\": \"\" if daily_val == \"[BLANK]\" else daily_val,\n",
    "        \"Cumulative\": \"\" if cum_val == \"[BLANK]\" else cum_val\n",
    "    })\n",
    "    df = pd.DataFrame(structured)\n",
    "    logger.info(f\"DIR INFO DataFrame shape: {df.shape}\")\n",
    "    return {\"DIR INFO\": structured}, df\n",
    "\n",
    "def process_dir_info(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    return build_dir_info_dict_from_rois(roi_texts, debug=debug)\n",
    "\n",
    "def build_survey_dict_from_rois(roi_texts, expected_headers):\n",
    "    row_strings = group_ocr_results(roi_texts)\n",
    "    logger.info(f\"SURVEY - Grouped Rows: {row_strings}\")\n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for subline in line.split(\"\\n\"):\n",
    "            subline = subline.strip()\n",
    "            if subline:\n",
    "                all_lines.append(subline)\n",
    "    logger.info(f\"SURVEY - All extracted lines: {all_lines}\")\n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"md\" in lower_tokens and \"inclination\" in lower_tokens:\n",
    "            logger.info(f\"SURVEY - Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"SURVEY - Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    logger.info(f\"SURVEY - Data lines to parse: {data_lines}\")\n",
    "    survey_list = [{expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "                   for tokens in data_lines]\n",
    "    return survey_list\n",
    "\n",
    "def sort_survey_data(survey_list):\n",
    "    def md_value(row):\n",
    "        try:\n",
    "            return float(row[\"MD\"].replace(\",\", \"\"))\n",
    "        except Exception:\n",
    "            return 0\n",
    "    return sorted(survey_list, key=md_value, reverse=True)\n",
    "\n",
    "def pipeline_survey_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"MD\", \"Inclination\", \"Azimuth\", \"DLS\", \"TVD\"]\n",
    "    survey_list = build_survey_dict_from_rois(roi_texts, expected_headers)\n",
    "    survey_list = sort_survey_data(survey_list)\n",
    "    df = pd.DataFrame(survey_list)\n",
    "    logger.info(f\"SURVEY DataFrame shape: {df.shape}\")\n",
    "    return {\"SURVEY\": survey_list}, df\n",
    "\n",
    "def build_casing_dict_from_rois(roi_texts, expected_headers, debug=False):\n",
    "    row_strings = group_ocr_results(roi_texts)\n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for sub in line.split(\"\\n\"):\n",
    "            sub = sub.strip()\n",
    "            if sub:\n",
    "                all_lines.append(sub)\n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"type\" in lower_tokens and \"size\" in lower_tokens:\n",
    "            logger.info(f\"CASING - Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"CASING - Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    casing_list = [{expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "                   for tokens in data_lines]\n",
    "    return casing_list\n",
    "\n",
    "def pipeline_casing_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"Type\", \"Size\", \"Weight\", \"Grade\", \"Connection\", \"Top MD\", \"Bottom MD\", \"TOC\"]\n",
    "    casing_list = build_casing_dict_from_rois(roi_texts, expected_headers, debug=debug)\n",
    "    df = pd.DataFrame(casing_list)\n",
    "    logger.info(f\"CASING DataFrame shape: {df.shape}\")\n",
    "    return {\"CASING\": casing_list}, df\n",
    "\n",
    "def build_consumables_dict_from_rois(roi_texts, debug=False):\n",
    "    row_strings = group_ocr_results(roi_texts)\n",
    "    data_rows = []\n",
    "    for line in row_strings:\n",
    "        lower_line = line.lower()\n",
    "        if (\"consumable\" in lower_line and \"received\" in lower_line) or \"nun\" in lower_line:\n",
    "            continue\n",
    "        if len(line.split()) < 5:\n",
    "            continue\n",
    "        data_rows.append(line)\n",
    "    consumables_list = []\n",
    "    for line in data_rows:\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "        if len(tokens) > 5:\n",
    "            first = \" \".join(tokens[:-4])\n",
    "            tokens = [first] + tokens[-4:]\n",
    "        if len(tokens) != 5:\n",
    "            logger.warning(f\"CONSUMABLES - Skipping row (unexpected token count): {tokens}\")\n",
    "            continue\n",
    "        consumables_list.append({\n",
    "            \"Consumable\": tokens[0],\n",
    "            \"Daily Received (gal)\": tokens[1],\n",
    "            \"Daily Used (gal)\": tokens[2],\n",
    "            \"Cumulative Used (gal)\": tokens[3],\n",
    "            \"Daily on Hand (gal)\": tokens[4]\n",
    "        })\n",
    "    return consumables_list\n",
    "\n",
    "def pipeline_consumables_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    consumables_list = build_consumables_dict_from_rois(roi_texts, debug=debug)\n",
    "    df = pd.DataFrame(consumables_list)\n",
    "    logger.info(f\"CONSUMABLES DataFrame shape: {df.shape}\")\n",
    "    return {\"CONSUMABLES\": consumables_list}, df\n",
    "\n",
    "def extract_bha_data(image_path, debug=False):\n",
    "    image = Image.open(image_path)\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    patterns = {\n",
    "        \"Drill Pipe Detail\": r\"Drill Pipe Detail:\\s*([^\\n]+)\",\n",
    "        \"Size\": r\"Size:\\s*([\\d.]+)\\b\",\n",
    "        \"Wt./Ft\": r\"Wt\\./Ft:\\s*([\\d.]+)\\b\",\n",
    "        \"Connection\": r\"Connection:\\s*([\\w\\d-]+)\\b\",\n",
    "        \"ID\": r\"ID:\\s*([\\d.]+)\\b\",\n",
    "        \"Drill Bit\": r\"Drill Bit:\\s*([^\\n;]+)\",\n",
    "        \"Motor\": r\"Motor:\\s*([^\\n;]+)\",\n",
    "        \"MWD Tool\": r\"MWD Tool:\\s*([^\\n;]+)\",\n",
    "        \"Monel Collar\": r\"Monel Collar:\\s*([^\\n;]+)\",\n",
    "        \"X-Over\": r\"X-Over:\\s*([^\\n;]+)\",\n",
    "        \"Sub\": r\"Sub:\\s*([^\\n;]+)\",\n",
    "        \"HWDP\": r\"HWDP:\\s*([^\\n;]+)\",\n",
    "        \"Drill Pipe\": r\"Drill Pipe:\\s*([\\d.]+(?:\\\" DP)?)\",\n",
    "        \"Reamer\": r\"Reamer:\\s*([^\\n;]+)\",\n",
    "        \"Shock Sub\": r\"Shock Sub:\\s*([^\\n;]+)\",\n",
    "        \"Total Length\": r\"Total Length:\\s*(\\d+)\\b\"\n",
    "    }\n",
    "    bha_data = {}\n",
    "    for key, pat in patterns.items():\n",
    "        match = re.search(pat, ocr_text)\n",
    "        if match:\n",
    "            bha_data[key] = match.group(1).strip()\n",
    "    if \"Drill Pipe Detail\" in bha_data:\n",
    "        detail = bha_data[\"Drill Pipe Detail\"]\n",
    "        for remove_key in [\"Size\", \"Wt./Ft\", \"Connection\", \"ID\"]:\n",
    "            if remove_key in bha_data:\n",
    "                detail = re.sub(rf\"{remove_key}:\\s*{re.escape(bha_data[remove_key])}\", \"\", detail).strip(\",; \")\n",
    "        bha_data[\"Drill Pipe Detail\"] = detail\n",
    "    structured_data = {\n",
    "        \"BHA\": {\n",
    "            \"Drill Pipe Detail\": bha_data.get(\"Drill Pipe Detail\", \"\"),\n",
    "            \"Size\": bha_data.get(\"Size\", \"\"),\n",
    "            \"Wt./Ft\": bha_data.get(\"Wt./Ft\", \"\"),\n",
    "            \"Connection\": bha_data.get(\"Connection\", \"\"),\n",
    "            \"ID\": bha_data.get(\"ID\", \"\"),\n",
    "            \"BHA #4\": {\n",
    "                \"Drill Bit\": bha_data.get(\"Drill Bit\", \"\"),\n",
    "                \"Motor\": bha_data.get(\"Motor\", \"\"),\n",
    "                \"MWD Tool\": bha_data.get(\"MWD Tool\", \"\"),\n",
    "                \"Monel Collar\": bha_data.get(\"Monel Collar\", \"\"),\n",
    "                \"X-Over\": bha_data.get(\"X-Over\", \"\"),\n",
    "                \"Sub\": bha_data.get(\"Sub\", \"\"),\n",
    "                \"HWDP\": bha_data.get(\"HWDP\", \"\"),\n",
    "                \"Drill Pipe\": bha_data.get(\"Drill Pipe\", \"\"),\n",
    "                \"Reamer\": bha_data.get(\"Reamer\", \"\"),\n",
    "                \"Shock Sub\": bha_data.get(\"Shock Sub\", \"\")\n",
    "            },\n",
    "            \"Total Length\": bha_data.get(\"Total Length\", \"\")\n",
    "        }\n",
    "    }\n",
    "    if debug:\n",
    "        logger.info(\"Extracted BHA data:\")\n",
    "        logger.info(json.dumps(structured_data, indent=4))\n",
    "    return structured_data\n",
    "\n",
    "def pipeline_bha_data(debug=False):\n",
    "    image_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_?BHA.png\"  # Adjust as needed\n",
    "    bha_json = extract_bha_data(image_path, debug=debug)\n",
    "    img = safe_read_image(image_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    pump_data = parse_pumps_table(ocr_text)\n",
    "    circ_data = parse_drilling_circ_rates(ocr_text)\n",
    "    pumps_df = pd.DataFrame(pump_data)\n",
    "    circ_df = pd.DataFrame(circ_data)\n",
    "    bha_df = pd.DataFrame([bha_json])\n",
    "    logger.info(f\"BHA DataFrame shape: {bha_df.shape}\")\n",
    "    combined = {\"BHA\": bha_json, \"Pumps\": pump_data, \"DrillingCircRates\": circ_data}\n",
    "    return combined, pumps_df, circ_df, bha_df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Additional Parsing Functions for Pumps and Drilling/Circ Rates\n",
    "# -----------------------------------------------------------------------------\n",
    "def parse_pumps_table(ocr_text):\n",
    "    pump_pattern = re.compile(\n",
    "        r\"^(\\d+)?\\s*(BOMCO)\\s+(TRIPLEX)\\s+(\\d+)?\\s*(\\d+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s*$\",\n",
    "        re.IGNORECASE)\n",
    "    pumps = []\n",
    "    for line in ocr_text.splitlines():\n",
    "        line = line.strip()\n",
    "        match = pump_pattern.match(line)\n",
    "        if match:\n",
    "            (number, model, pump_type, hhp, efficiency, stroke, liner,\n",
    "             p_rating, p_limit, spm_rating, spm_limit) = match.groups()\n",
    "            pumps.append({\n",
    "                \"Number\": number if number else \"\",\n",
    "                \"Model\": model,\n",
    "                \"Type\": pump_type,\n",
    "                \"HHP\": hhp if hhp else \"\",\n",
    "                \"Efficiency\": efficiency,\n",
    "                \"Stroke(in)\": stroke,\n",
    "                \"Liner(in)\": liner,\n",
    "                \"P-Rating(psi)\": p_rating,\n",
    "                \"P-Limit(psi)\": p_limit,\n",
    "                \"SPM Rating\": spm_rating,\n",
    "                \"SPM Limit\": spm_limit\n",
    "            })\n",
    "    return pumps\n",
    "\n",
    "def parse_drilling_circ_rates(ocr_text):\n",
    "    circ_pattern = re.compile(\n",
    "        r\"Drilling/Circ\\s+Rate\\s+(\\d+)\\s+(\\d+)\\s+PSI\\s*@\\s*(\\d+)\\s*SPM\\s*([\\d.]+)\\s+Gal/Stoke\\s+([\\d.]+)\\s+GPM\\s+([\\d.]+)\\s+BPM\\s+([\\d.]+)\\s+DC\\s+([\\d.]+)\\s+DP\",\n",
    "        re.IGNORECASE)\n",
    "    circ_rates = []\n",
    "    for line in ocr_text.splitlines():\n",
    "        line = line.strip()\n",
    "        match = circ_pattern.search(line)\n",
    "        if match:\n",
    "            rate_id, pressure, spm, gal_stroke, gpm, bpm, dc, dp = match.groups()\n",
    "            circ_rates.append({\n",
    "                \"RateID\": rate_id,\n",
    "                \"Pressure(PSI)\": pressure,\n",
    "                \"SPM\": spm,\n",
    "                \"Gal/Stoke\": gal_stroke,\n",
    "                \"GPM\": gpm,\n",
    "                \"BPM\": bpm,\n",
    "                \"DC\": dc,\n",
    "                \"DP\": dp\n",
    "            })\n",
    "    return circ_rates\n",
    "\n",
    "\n",
    "def pipeline_mud_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\n",
    "        \"Type\", \"Weight In\", \"Weight Out\", \"pH\", \"CAKE\",\n",
    "        \"GELS (10s/10m/30m)\", \"Oil/Water\", \"FV\", \"ES\", \"PV\",\n",
    "        \"YP\", \"CL\", \"Ca\", \"LGS\", \"WL\", \"HTHP Loss\", \"3 RPM\",\n",
    "        \"6 RPM\", \"Mud Pits and Hole Volume\", \"24 Hr Loss\",\n",
    "        \"Total Loss\", \"Comments\"\n",
    "    ]\n",
    "    mud_dict = build_mud_dict_from_rois(roi_texts, expected_headers)\n",
    "    return {\"MUD\": mud_dict}, pd.DataFrame(list(mud_dict.items()), columns=[\"Key\", \"Value\"])\n",
    "\n",
    "# mud ---------------------------------------------------------------------\n",
    "# parse_value_row_tokens\n",
    "# ---------------------------------------------------------------------\n",
    "def parse_value_row_tokens(expected_headers, tokens):\n",
    "    \"\"\"\n",
    "    Map a flat list of tokens to the expected headers.\n",
    "    For \"GELS (10s/10m/30m)\", consume 3 tokens and create a sub-dictionary.\n",
    "    Expected token count = (number of headers - 1) + 3.\n",
    "    \"\"\"\n",
    "    expected_token_count = (len(expected_headers) - 1) + 3\n",
    "    logger.info(f\"Expected token count: {expected_token_count}, tokens extracted: {tokens}\")\n",
    "    \n",
    "    # Pad or trim tokens as needed.\n",
    "    if len(tokens) < expected_token_count:\n",
    "        tokens += [\"[BLANK]\"] * (expected_token_count - len(tokens))\n",
    "        logger.warning(\"Not enough tokens. Padding with [BLANK].\")\n",
    "    elif len(tokens) > expected_token_count:\n",
    "        tokens = tokens[:expected_token_count]\n",
    "        logger.warning(\"Too many tokens. Trimming the extra tokens.\")\n",
    "    \n",
    "    result = {}\n",
    "    idx = 0\n",
    "    for header in expected_headers:\n",
    "        if header == \"GELS (10s/10m/30m)\":\n",
    "            gels_tokens = tokens[idx:idx+3]\n",
    "            result[header] = {\n",
    "                \"10s\": gels_tokens[0],\n",
    "                \"10m\": gels_tokens[1],\n",
    "                \"30m\": gels_tokens[2]\n",
    "            }\n",
    "            idx += 3\n",
    "        else:\n",
    "            result[header] = tokens[idx]\n",
    "            idx += 1\n",
    "    logger.info(f\"Mapped dictionary: {result}\")\n",
    "    return result\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# build_mud_dict_from_rois\n",
    "# ---------------------------------------------------------------------\n",
    "def build_mud_dict_from_rois(roi_texts, expected_headers):\n",
    "    \"\"\"\n",
    "    Group OCR results into rows based on the y coordinate.\n",
    "    Identify header rows and corresponding data rows.\n",
    "    \n",
    "    In our case, we expect:\n",
    "      - A header row (with labels) followed by a data row,\n",
    "      - Then a second header row (for the remaining fields) followed by a second data row.\n",
    "    \n",
    "    We then combine the two data rows' tokens and map them to expected_headers.\n",
    "    \"\"\"\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "\n",
    "    # Group by row based on y coordinate.\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "\n",
    "    # Sort each row by x coordinate and log its text.\n",
    "    row_strings = []\n",
    "    for i, row_cells in enumerate(rows):\n",
    "        row_cells.sort(key=lambda c: c[0])\n",
    "        line_text = \" \".join(cell[4] for cell in row_cells)\n",
    "        row_strings.append(line_text)\n",
    "        logger.info(f\"Row {i} text: {line_text}\")\n",
    "\n",
    "    # Based on OCR output expectations:\n",
    "    # Row 1: header row 1 (first set of labels)\n",
    "    # Row 2: data row 1 (first set of values)\n",
    "    # Row 3: header row 2 (remaining labels)\n",
    "    # Row 4: data row 2 (remaining values)\n",
    "    header1_line = None\n",
    "    value1_line = None\n",
    "    header2_line = None\n",
    "    value2_line = None\n",
    "\n",
    "    for i, r_text in enumerate(row_strings):\n",
    "        if \"Type\" in r_text and not header1_line:\n",
    "            header1_line = r_text\n",
    "            if i + 1 < len(row_strings):\n",
    "                value1_line = row_strings[i+1]\n",
    "        elif header1_line and not header2_line and any(kw in r_text for kw in [\"RPM\", \"Mud\", \"Loss\", \"Comments\"]):\n",
    "            header2_line = r_text\n",
    "            if i + 1 < len(row_strings):\n",
    "                value2_line = row_strings[i+1]\n",
    "            break\n",
    "\n",
    "    logger.info(f\"Header1: {header1_line}\")\n",
    "    logger.info(f\"Value1: {value1_line}\")\n",
    "    logger.info(f\"Header2: {header2_line}\")\n",
    "    logger.info(f\"Value2: {value2_line}\")\n",
    "\n",
    "    if value1_line is None:\n",
    "        logger.error(\"No data row found for header1!\")\n",
    "        return {}\n",
    "\n",
    "    # Split the data rows into tokens.\n",
    "    tokens1 = value1_line.split()\n",
    "    tokens2 = value2_line.split() if value2_line else []\n",
    "    logger.info(f\"Tokens from data row 1: {tokens1}\")\n",
    "    logger.info(f\"Tokens from data row 2: {tokens2}\")\n",
    "\n",
    "    # Combine tokens from both data rows.\n",
    "    combined_tokens = tokens1 + tokens2\n",
    "    logger.info(f\"Combined tokens: {combined_tokens}\")\n",
    "\n",
    "    # Map the tokens to the expected headers.\n",
    "    return parse_value_row_tokens(expected_headers, combined_tokens)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# main_pipeline\n",
    "# ---------------------------------------------------------------------\n",
    "def main_pipeline():\n",
    "    expected_headers = [\n",
    "        \"Type\", \"Weight In\", \"Weight Out\", \"pH\", \"CAKE\",\n",
    "        \"GELS (10s/10m/30m)\", \"Oil/Water\", \"FV\", \"ES\", \"PV\",\n",
    "        \"YP\", \"CL\", \"Ca\", \"LGS\", \"WL\", \"HTHP Loss\", \"3 RPM\",\n",
    "        \"6 RPM\", \"Mud Pits and Hole Volume\", \"24 Hr Loss\",\n",
    "        \"Total Loss\", \"Comments\"\n",
    "    ]\n",
    "    # Use a DBFS URI for the input image\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png\"\n",
    "    try:\n",
    "        img = read_cropped_section_image(section_path)\n",
    "        logger.info(\"Image loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        return\n",
    "\n",
    "    show_image(\"Original Cropped Section\", img, size=(12,12))\n",
    "    thresh_img = preprocess_image(img, debug=True)\n",
    "    rois = detect_text_regions(thresh_img, debug=True)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=True)\n",
    "    \n",
    "    # Build the mud dictionary using the combined data rows.\n",
    "    mud_dict = build_mud_dict_from_rois(roi_texts, expected_headers)\n",
    "    final_dict = {\"MUD\": mud_dict}\n",
    "    logger.info(\"===== FINAL EXTRACTED MUD DICTIONARY =====\")\n",
    "    logger.info(json.dumps(final_dict, indent=4))\n",
    "    \n",
    "    df_final = pd.DataFrame(list(mud_dict.items()), columns=[\"Key\", \"Value\"])\n",
    "    print(\"----- Extracted DataFrame -----\")\n",
    "    print(df_final)\n",
    "    \n",
    "    # Define the output folder using a DBFS URI and create it with dbutils.fs.mkdirs\n",
    "    output_folder_dbfs = \"dbfs:/mnt/mini-proj-dd/final_ocr_results\"\n",
    "    dbutils.fs.mkdirs(output_folder_dbfs)\n",
    "    # Instead of writing using OS-level functions, convert the DataFrame to CSV text\n",
    "    # and write it directly to DBFS.\n",
    "    out_file = output_folder_dbfs + \"/page_1_section_3_ocr.csv\"\n",
    "    csv_data = df_final.to_csv(index=False)\n",
    "    dbutils.fs.put(out_file, csv_data, overwrite=True)\n",
    "    logger.info(f\"Final DataFrame saved to {out_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MAIN RUNNER: STANDARDIZED PIPELINE EXECUTION\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_pipeline(name, pipeline_info, debug=False):\n",
    "    try:\n",
    "        logger.info(f\"Processing pipeline: {name}\")\n",
    "        output, df = pipeline_info[\"func\"](debug)\n",
    "        logger.info(f\"{name} DataFrame shape: {df.shape} (Rows: {df.shape[0]}, Columns: {df.shape[1]})\")\n",
    "        json_path = os.path.join(JSON_FOLDER, pipeline_info[\"json\"])\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(output, f, indent=4)\n",
    "        csv_path = os.path.join(CSV_FOLDER, pipeline_info[\"csv\"])\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"{name} saved: JSON({json_path}), CSV({csv_path})\")\n",
    "        print(f\"--- {name.upper()} DataFrame ---\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(df.head(10))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in pipeline {name}: {e}\")\n",
    "\n",
    "def main():\n",
    "    debug = False  # Set True for detailed logging\n",
    "    # Define all pipelines in sequence\n",
    "    pipelines = {\n",
    "        \"cost_data\": {\n",
    "            \"func\": pipeline_cost_data,\n",
    "            \"csv\": \"cost_data.csv\",\n",
    "            \"json\": \"cost_data.json\"\n",
    "        },\n",
    "        \"well_job\": {\n",
    "            \"func\": lambda d: process_well_job_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\", d),\n",
    "            \"csv\": \"well_job_data.csv\",\n",
    "            \"json\": \"well_job_data.json\"\n",
    "        },\n",
    "        \"obs_int\": {\n",
    "            \"func\": lambda d: process_obs_int(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\", d),\n",
    "            \"csv\": \"obs_int_data.csv\",\n",
    "            \"json\": \"obs_int_data.json\"\n",
    "        },\n",
    "        \"bop\": {\n",
    "            \"func\": lambda d: process_bop(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\", d),\n",
    "            \"csv\": \"bop_data.csv\",\n",
    "            \"json\": \"bop_data.json\"\n",
    "        },\n",
    "        \"dir_info\": {\n",
    "            \"func\": lambda d: process_dir_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\", d),\n",
    "            \"csv\": \"dir_info_data.csv\",\n",
    "            \"json\": \"dir_info_data.json\"\n",
    "        },\n",
    "        \"survey\": {\n",
    "            \"func\": pipeline_survey_data,\n",
    "            \"csv\": \"survey_data.csv\",\n",
    "            \"json\": \"survey_data.json\"\n",
    "        },\n",
    "        \"casing\": {\n",
    "            \"func\": pipeline_casing_data,\n",
    "            \"csv\": \"casing_data.csv\",\n",
    "            \"json\": \"casing_data.json\"\n",
    "        },\n",
    "        \"consumables\": {\n",
    "            \"func\": pipeline_consumables_data,\n",
    "            \"csv\": \"consumables_data.csv\",\n",
    "            \"json\": \"consumables_data.json\"\n",
    "        },\n",
    "        \"mud\": {\n",
    "            \"func\": pipeline_mud_data,\n",
    "            \"csv\": \"mud_data.csv\",\n",
    "            \"json\": \"mud_data.json\"\n",
    "        },\n",
    "        \"bha\": {\n",
    "            \"func\": pipeline_bha_data,\n",
    "            \"csv\": \"bha_data.csv\",\n",
    "            \"json\": \"bha_data.json\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, pipe in pipelines.items():\n",
    "        run_pipeline(name, pipe, debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13389e7-770d-44a2-b489-d03b4160e713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4749686794680557>, line 911\u001B[0m\n",
       "\u001B[1;32m    906\u001B[0m         run_pipeline(name, pipe, debug)\n",
       "\u001B[1;32m    910\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m--> 911\u001B[0m     main()\n",
       "\n",
       "File \u001B[0;32m<command-4749686794680557>, line 900\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m    846\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmain\u001B[39m():\n",
       "\u001B[1;32m    847\u001B[0m     debug \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# Set True for detailed logging\u001B[39;00m\n",
       "\u001B[1;32m    848\u001B[0m     pipelines \u001B[38;5;241m=\u001B[39m {\n",
       "\u001B[1;32m    849\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcost_data\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[1;32m    850\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: pipeline_cost_data,\n",
       "\u001B[1;32m    851\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcost_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    852\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcost_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    853\u001B[0m         },\n",
       "\u001B[1;32m    854\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwell_job\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[1;32m    855\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mlambda\u001B[39;00m d: process_well_job_info(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\u001B[39m\u001B[38;5;124m\"\u001B[39m, d),\n",
       "\u001B[1;32m    856\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwell_job_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    857\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwell_job_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    858\u001B[0m         },\n",
       "\u001B[1;32m    859\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobs_int\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[1;32m    860\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mlambda\u001B[39;00m d: process_obs_int(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\u001B[39m\u001B[38;5;124m\"\u001B[39m, d),\n",
       "\u001B[1;32m    861\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobs_int_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    862\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobs_int_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    863\u001B[0m         },\n",
       "\u001B[1;32m    864\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbop\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[1;32m    865\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mlambda\u001B[39;00m d: process_bop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\u001B[39m\u001B[38;5;124m\"\u001B[39m, d),\n",
       "\u001B[1;32m    866\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbop_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    867\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbop_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    868\u001B[0m         },\n",
       "\u001B[1;32m    869\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdir_info\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[1;32m    870\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mlambda\u001B[39;00m d: process_dir_info(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\u001B[39m\u001B[38;5;124m\"\u001B[39m, d),\n",
       "\u001B[1;32m    871\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdir_info_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    872\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdir_info_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    873\u001B[0m         },\n",
       "\u001B[1;32m    874\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msurvey\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[1;32m    875\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: pipeline_survey_data,\n",
       "\u001B[1;32m    876\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msurvey_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    877\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msurvey_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    878\u001B[0m         },\n",
       "\u001B[1;32m    879\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcasing\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[1;32m    880\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: pipeline_casing_data,\n",
       "\u001B[1;32m    881\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcasing_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    882\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcasing_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    883\u001B[0m         },\n",
       "\u001B[1;32m    884\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsumables\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[1;32m    885\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: pipeline_consumables_data,\n",
       "\u001B[1;32m    886\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsumables_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    887\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsumables_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    888\u001B[0m         },\n",
       "\u001B[1;32m    889\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmud\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[1;32m    890\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: pipeline_mud_data,\n",
       "\u001B[1;32m    891\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmud_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    892\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmud_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    893\u001B[0m         },\n",
       "\u001B[1;32m    894\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbha\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[1;32m    895\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: extract_bha_data,\n",
       "\u001B[1;32m    896\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbha_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    897\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbha_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    898\u001B[0m         },\n",
       "\u001B[1;32m    899\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpumps\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n",
       "\u001B[0;32m--> 900\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: parse_pumps_table \u001B[38;5;241m+\u001B[39m parse_drilling_circ_rates,\n",
       "\u001B[1;32m    901\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpumps_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    902\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpumps_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    903\u001B[0m         }\n",
       "\u001B[1;32m    904\u001B[0m     }\n",
       "\u001B[1;32m    905\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m name, pipe \u001B[38;5;129;01min\u001B[39;00m pipelines\u001B[38;5;241m.\u001B[39mitems():\n",
       "\u001B[1;32m    906\u001B[0m         run_pipeline(name, pipe, debug)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for +: 'function' and 'function'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "TypeError",
        "evalue": "unsupported operand type(s) for +: 'function' and 'function'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: unsupported operand type(s) for +: 'function' and 'function'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-4749686794680557>, line 911\u001B[0m\n\u001B[1;32m    906\u001B[0m         run_pipeline(name, pipe, debug)\n\u001B[1;32m    910\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 911\u001B[0m     main()\n",
        "File \u001B[0;32m<command-4749686794680557>, line 900\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    846\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmain\u001B[39m():\n\u001B[1;32m    847\u001B[0m     debug \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# Set True for detailed logging\u001B[39;00m\n\u001B[1;32m    848\u001B[0m     pipelines \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    849\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcost_data\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    850\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: pipeline_cost_data,\n\u001B[1;32m    851\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcost_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    852\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcost_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    853\u001B[0m         },\n\u001B[1;32m    854\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwell_job\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    855\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mlambda\u001B[39;00m d: process_well_job_info(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\u001B[39m\u001B[38;5;124m\"\u001B[39m, d),\n\u001B[1;32m    856\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwell_job_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    857\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwell_job_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    858\u001B[0m         },\n\u001B[1;32m    859\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobs_int\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    860\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mlambda\u001B[39;00m d: process_obs_int(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\u001B[39m\u001B[38;5;124m\"\u001B[39m, d),\n\u001B[1;32m    861\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobs_int_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    862\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mobs_int_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    863\u001B[0m         },\n\u001B[1;32m    864\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbop\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    865\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mlambda\u001B[39;00m d: process_bop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\u001B[39m\u001B[38;5;124m\"\u001B[39m, d),\n\u001B[1;32m    866\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbop_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    867\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbop_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    868\u001B[0m         },\n\u001B[1;32m    869\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdir_info\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    870\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mlambda\u001B[39;00m d: process_dir_info(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\u001B[39m\u001B[38;5;124m\"\u001B[39m, d),\n\u001B[1;32m    871\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdir_info_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    872\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdir_info_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    873\u001B[0m         },\n\u001B[1;32m    874\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msurvey\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    875\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: pipeline_survey_data,\n\u001B[1;32m    876\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msurvey_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    877\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msurvey_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    878\u001B[0m         },\n\u001B[1;32m    879\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcasing\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    880\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: pipeline_casing_data,\n\u001B[1;32m    881\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcasing_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    882\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcasing_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    883\u001B[0m         },\n\u001B[1;32m    884\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsumables\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    885\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: pipeline_consumables_data,\n\u001B[1;32m    886\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsumables_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    887\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsumables_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    888\u001B[0m         },\n\u001B[1;32m    889\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmud\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    890\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: pipeline_mud_data,\n\u001B[1;32m    891\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmud_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    892\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmud_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    893\u001B[0m         },\n\u001B[1;32m    894\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbha\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[1;32m    895\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: extract_bha_data,\n\u001B[1;32m    896\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbha_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    897\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbha_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    898\u001B[0m         },\n\u001B[1;32m    899\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpumps\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\n\u001B[0;32m--> 900\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfunc\u001B[39m\u001B[38;5;124m\"\u001B[39m: parse_pumps_table \u001B[38;5;241m+\u001B[39m parse_drilling_circ_rates,\n\u001B[1;32m    901\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpumps_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    902\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpumps_data.json\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    903\u001B[0m         }\n\u001B[1;32m    904\u001B[0m     }\n\u001B[1;32m    905\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m name, pipe \u001B[38;5;129;01min\u001B[39;00m pipelines\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    906\u001B[0m         run_pipeline(name, pipe, debug)\n",
        "\u001B[0;31mTypeError\u001B[0m: unsupported operand type(s) for +: 'function' and 'function'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import cv2, os\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Global Logger & Output Directories\n",
    "# -----------------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"OCRProduction\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "OUTPUT_FOLDER = \"/dbfs/mnt/mini-proj-dd/final_results\"\n",
    "CSV_FOLDER = os.path.join(OUTPUT_FOLDER, \"csv\")\n",
    "JSON_FOLDER = os.path.join(OUTPUT_FOLDER, \"json\")\n",
    "os.makedirs(CSV_FOLDER, exist_ok=True)\n",
    "os.makedirs(JSON_FOLDER, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Common Utility Functions (No image display)\n",
    "# -----------------------------------------------------------------------------\n",
    "def safe_read_image(img_path):\n",
    "    \"\"\"Read an image from a local or DBFS path.\"\"\"\n",
    "    local_path = img_path if not img_path.startswith(\"dbfs:\") else img_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {local_path}\")\n",
    "    logger.info(f\"Image loaded from {local_path} with shape {img.shape}\")\n",
    "    return img\n",
    "\n",
    "def preprocess_image(img, debug=False):\n",
    "    \"\"\"Convert image to grayscale and apply adaptive thresholding.\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 15, 9)\n",
    "    if debug:\n",
    "        logger.info(\"Preprocessing completed (grayscale and threshold applied).\")\n",
    "    return thresh\n",
    "\n",
    "def detect_text_regions(thresh_img, debug=False):\n",
    "    \"\"\"Detect text regions (bounding boxes) from the thresholded image.\"\"\"\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "    rois.sort(key=lambda b: (b[1], b[0]))\n",
    "    if debug:\n",
    "        logger.info(f\"Detected {len(rois)} text regions.\")\n",
    "    return rois\n",
    "\n",
    "def perform_ocr_on_rois(img, rois, debug=False):\n",
    "    \"\"\"Perform OCR on each detected region and return list of (x,y,w,h,text).\"\"\"\n",
    "    results = []\n",
    "    for i, (x, y, w, h) in enumerate(rois):\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        text = pytesseract.image_to_string(roi, config=\"--psm 6\").strip() or \"[BLANK]\"\n",
    "        results.append((x, y, w, h, text))\n",
    "        if debug:\n",
    "            logger.info(f\"OCR Box {i}: {text}\")\n",
    "    return results\n",
    "\n",
    "def perform_ocr(img):\n",
    "    \"\"\"Perform OCR on the entire image.\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return pytesseract.image_to_string(gray, config=\"--psm 6\")\n",
    "\n",
    "def extract_key_value_from_text(text, expected_keys):\n",
    "    \"\"\"Extract key-value pairs from OCR text given expected keys.\"\"\"\n",
    "    combined = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "    combined = re.sub(r'\\s+', ' ', combined)\n",
    "    result = {}\n",
    "    for i, key in enumerate(expected_keys):\n",
    "        if i < len(expected_keys) - 1:\n",
    "            next_key = expected_keys[i+1]\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*?)(?=\\s*' + re.escape(next_key) + r'\\s*:|$)'\n",
    "        else:\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*)'\n",
    "        match = re.search(pattern, combined, re.IGNORECASE)\n",
    "        result[key] = match.group(1).strip() if match and match.group(1).strip() else None\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# [PARSE FUNCTIONS]\n",
    "# These functions (e.g. build_bit_info_dict_from_rois, process_well_job_info, etc.)\n",
    "# are your specialized parsers. They have been left mostly intact.\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_bit_info_dict_from_rois(roi_texts, debug=False):\n",
    "    row_tolerance = 10\n",
    "    grouped_rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            grouped_rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        grouped_rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for i, row in enumerate(grouped_rows):\n",
    "        row.sort(key=lambda cell: cell[0])\n",
    "        line = \" \".join(cell[4] for cell in row)\n",
    "        row_strings.append(line.replace(\"\\n\", \" \").strip())\n",
    "        if debug:\n",
    "            logger.info(f\"BIT - Grouped Row {i}: {row_strings[-1]}\")\n",
    "    if len(row_strings) < 3:\n",
    "        logger.warning(\"Not enough rows found for BIT layout.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    data_lines = row_strings[3:]\n",
    "    final_columns = [\n",
    "        \"Bit #\", \"Size\", \"Make\", \"Model\", \"Serial #\",\n",
    "        \"Nozzle-(Number x Size)\", \"Nozzle-TFA\",\n",
    "        \"Depth-In\", \"Depth-Out\", \"Depth-Feet\", \"Depth-ROP\",\n",
    "        \"Hours-Total\", \"Hours-On Btm\",\n",
    "        \"Dull Grade-I\", \"Dull Grade-O1\", \"Dull Grade-D\", \"Dull Grade-L\",\n",
    "        \"Dull Grade-B\", \"Dull Grade-G\", \"Dull Grade-O2\", \"Dull Grade-RP\"\n",
    "    ]\n",
    "    structured_data = []\n",
    "    for line in data_lines:\n",
    "        tokens = line.split()\n",
    "        if len(tokens) < 21:\n",
    "            tokens += [\"\"] * (21 - len(tokens))\n",
    "        elif len(tokens) > 21:\n",
    "            tokens = tokens[:21]\n",
    "        row_dict = {final_columns[i]: tokens[i] for i in range(21)}\n",
    "        structured_data.append(row_dict)\n",
    "        if debug:\n",
    "            logger.info(f\"BIT - Parsed row: {row_dict}\")\n",
    "    df = pd.DataFrame(structured_data)\n",
    "    logger.info(f\"BIT DataFrame shape: {df.shape}\")\n",
    "    return {\"BIT DETAILS\": df.to_dict(orient='records')}, df\n",
    "\n",
    "def process_well_job_info(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    if debug:\n",
    "        logger.info(\"Well/Job OCR extraction complete.\")\n",
    "        logger.info(f\"OCR Text: {ocr_text}\")\n",
    "    expected_keys = [\n",
    "        \"Well Name\", \"Job Name\", \"Supervisor(s)\", \"Field\", \"Sec/Twn/Rng\", \"Phone\",\n",
    "        \"AFE #\", \"API #\", \"Email\", \"Contractor\", \"Elevation\", \"RKB\",\n",
    "        \"Spud Date\", \"Days from Spud\", \"Days on Loc\", \"MD/TVD\", \"24 Hr Footage\",\n",
    "        \"Present Operations\", \"Activity Planned\"\n",
    "    ]\n",
    "    combined = \" \".join(line.strip() for line in ocr_text.splitlines() if line.strip())\n",
    "    combined = re.sub(r'\\s+', ' ', combined)\n",
    "    result = {}\n",
    "    for i, key in enumerate(expected_keys):\n",
    "        if i < len(expected_keys) - 1:\n",
    "            next_key = expected_keys[i+1]\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*?)(?=\\s*' + re.escape(next_key) + r'\\s*:|$)'\n",
    "        else:\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*)'\n",
    "        match = re.search(pattern, combined, re.IGNORECASE)\n",
    "        result[key] = match.group(1).strip() if match else \"\"\n",
    "    df = pd.DataFrame(list(result.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"WELL/JOB DataFrame shape: {df.shape}\")\n",
    "    return {\"WELL/JOB INFORMATION\": result}, df\n",
    "def build_obs_int_data_from_rois(roi_texts):\n",
    "    \"\"\"\n",
    "    Dynamically builds structured data from OCR results for the\n",
    "    \"DAILY NUMBERS: OBSERVATION & INTERVENTION\" section.\n",
    "    \n",
    "    Processing rules:\n",
    "      - Skip header texts (\"DAILY NUMBERS: OBSERVATION & INTERVENTION\" or \"Number\").\n",
    "      - If an OCR box has multiple lines, split it into separate type entries.\n",
    "      - If an OCR box is numeric (or a blank marker like \"[BLANK]\"),\n",
    "        assign its value to the earliest record that has no number yet.\n",
    "      - If no unpaired type exists when a numeric value is found, a new record is created.\n",
    "      - Finally, any record with an empty \"Type\" is filtered out.\n",
    "    \n",
    "    This approach avoids hard-coding the expected count and ensures that no blank key/value pair is included.\n",
    "    \"\"\"\n",
    "    header_str = \"daily numbers: observation & intervention\"\n",
    "    records = []\n",
    "\n",
    "    # Process each OCR box in order.\n",
    "    for (_, _, _, _, raw_text) in roi_texts:\n",
    "        text = raw_text.strip()\n",
    "        low_text = text.lower()\n",
    "        \n",
    "        # Skip header texts and column labels.\n",
    "        if low_text in [header_str, \"number\"]:\n",
    "            continue\n",
    "        \n",
    "        # If the text contains newlines, treat each line as a separate type.\n",
    "        if \"\\n\" in text:\n",
    "            for line in text.splitlines():\n",
    "                line = line.strip()\n",
    "                if not line or line.lower() in [header_str, \"number\", \"[blank]\"]:\n",
    "                    continue\n",
    "                records.append({\"Type\": line, \"Number\": None})\n",
    "        else:\n",
    "            # Determine if the text represents a number.\n",
    "            is_numeric = False\n",
    "            value = text\n",
    "            if text == \"\" or low_text == \"[blank]\":\n",
    "                is_numeric = True\n",
    "                value = \"\"\n",
    "            else:\n",
    "                try:\n",
    "                    float(text)\n",
    "                    is_numeric = True\n",
    "                except ValueError:\n",
    "                    is_numeric = False\n",
    "\n",
    "            if is_numeric:\n",
    "                # Pair the number with the earliest record without a number.\n",
    "                for rec in records:\n",
    "                    if rec[\"Number\"] is None:\n",
    "                        rec[\"Number\"] = value\n",
    "                        break\n",
    "                else:\n",
    "                    # No pending record exists; create one with an empty type.\n",
    "                    records.append({\"Type\": \"\", \"Number\": value})\n",
    "            else:\n",
    "                # Non-numeric text is treated as a type.\n",
    "                records.append({\"Type\": text, \"Number\": None})\n",
    "\n",
    "    # Clean up: replace any None values with an empty string.\n",
    "    for rec in records:\n",
    "        if rec[\"Number\"] is None:\n",
    "            rec[\"Number\"] = \"\"\n",
    "\n",
    "    # Filter out any records where the \"Type\" is empty.\n",
    "    records = [rec for rec in records if rec[\"Type\"].strip() != \"\"]\n",
    "\n",
    "    # Log the structured data.\n",
    "    logging.getLogger(\"daily_numbersExtractor\").info(f\"Structured Data: {records}\")\n",
    "\n",
    "    # Save the results as CSV and JSON.\n",
    "    df = pd.DataFrame(records)\n",
    "    csv_filename = \"obs_int_data.csv\"\n",
    "    json_filename = \"obs_int_data.json\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    logging.getLogger(\"daily_numbersExtractor\").info(f\"Data saved successfully as CSV: {csv_filename}\")\n",
    "\n",
    "    with open(json_filename, \"w\") as json_file:\n",
    "        json.dump(records, json_file, indent=4)\n",
    "    logging.getLogger(\"daily_numbersExtractor\").info(f\"Data saved successfully in JSON format: {json_filename}\")\n",
    "\n",
    "    return records, df\n",
    "\n",
    "def process_obs_int(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    header_str = \"daily numbers: observation & intervention\"\n",
    "    all_texts = [t[4] for t in roi_texts]\n",
    "    types_list, numbers_list = [], []\n",
    "    for txt in all_texts:\n",
    "        clean = txt.strip()\n",
    "        if clean.lower() in [header_str, \"number\", \"[blank]\"]:\n",
    "            continue\n",
    "        try:\n",
    "            float(clean)\n",
    "            numbers_list.append(\"\" if clean.lower() == \"[blank]\" else clean)\n",
    "            continue\n",
    "        except ValueError:\n",
    "            pass\n",
    "        if \"\\n\" in clean:\n",
    "            for line in clean.splitlines():\n",
    "                line = line.strip()\n",
    "                if line and line.lower() != \"[blank]\":\n",
    "                    types_list.append(line)\n",
    "        else:\n",
    "            types_list.append(clean)\n",
    "    expected_count = 5\n",
    "    while len(numbers_list) < expected_count:\n",
    "        numbers_list.append(\"\")\n",
    "    types_list = types_list[:expected_count]\n",
    "    numbers_list = numbers_list[:expected_count]\n",
    "    structured = [{\"Type\": types_list[i], \"Number\": numbers_list[i]} for i in range(expected_count)]\n",
    "    df = pd.DataFrame(structured)\n",
    "    logger.info(f\"OBS_INT DataFrame shape: {df.shape}\")\n",
    "    return {\"DAILY NUMBERS: OBSERVATION & INTERVENTION\": structured}, df\n",
    "\n",
    "def pipeline_cost_data(debug=False):\n",
    "    \"\"\"Extract Cost Data from a specified image.\"\"\"\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png\"\n",
    "    try:\n",
    "        img = safe_read_image(section_path)\n",
    "        logger.info(\"Cost image loaded.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(e)\n",
    "        return {}, pd.DataFrame()\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"Cost OCR extraction complete.\")\n",
    "    expected_keys = [\n",
    "        \"Drilling AFE Amount\", \"Daily Drilling Cost\", \"Cumulative Drilling Cost\",\n",
    "        \"Cumulative Well Cost\", \"Daily Mud Cost\", \"Cumulative Mud Cost\"\n",
    "    ]\n",
    "    extracted = extract_key_value_from_text(ocr_text, expected_keys)\n",
    "    final_dict = {\"COST DATA\": extracted}\n",
    "    logger.info(json.dumps(final_dict, indent=4))\n",
    "    df = pd.DataFrame(list(extracted.items()), columns=[\"Key\", \"Value\"])\n",
    "    return final_dict, df\n",
    "\n",
    "def process_bop(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    if debug:\n",
    "        logger.info(\"BOP OCR extraction complete.\")\n",
    "        logger.info(f\"OCR Text: {ocr_text}\")\n",
    "    patterns = {\n",
    "        \"Last BOP Test Date\": r\"Last BOP Test Date\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Last BOP Drill\": r\"Last BOP Drill\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Next BOP Test\": r\"Next BOP Test\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\"\n",
    "    }\n",
    "    result = {}\n",
    "    for key, regex in patterns.items():\n",
    "        match = re.search(regex, ocr_text, re.IGNORECASE)\n",
    "        result[key] = match.group(1) if match else \"\"\n",
    "    df = pd.DataFrame(list(result.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"BOP DataFrame shape: {df.shape}\")\n",
    "    return {\"BOP\": result}, df\n",
    "\n",
    "def build_dir_info_dict_from_rois(roi_texts, debug=False):\n",
    "    all_texts = [t[4] for t in roi_texts]\n",
    "    daily_cum_idx = next((i for i, txt in enumerate(all_texts)\n",
    "                           if \"daily\" in txt.lower() and \"cumulative\" in txt.lower()), None)\n",
    "    if daily_cum_idx is None:\n",
    "        logger.warning(\"Could not find 'Daily Cumulative' bounding box.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    cat_idx = daily_cum_idx + 1\n",
    "    if cat_idx >= len(all_texts):\n",
    "        logger.warning(\"No bounding box after 'Daily Cumulative'.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    categories_box = all_texts[cat_idx]\n",
    "    lines = [ln.strip() for ln in categories_box.split(\"\\n\") if ln.strip()]\n",
    "    if len(lines) < 5:\n",
    "        logger.warning(f\"Expected 5 category lines, got {len(lines)}: {lines}\")\n",
    "    def safe_get(idx):\n",
    "        return all_texts[idx] if 0 <= idx < len(all_texts) else \"\"\n",
    "    structured = []\n",
    "    for i in range(4):\n",
    "        cat_name = lines[i] if i < len(lines) else f\"Unknown Category {i+1}\"\n",
    "        daily_box = safe_get(cat_idx + 1 + (i * 2))\n",
    "        cum_box = safe_get(cat_idx + 2 + (i * 2))\n",
    "        structured.append({\n",
    "            \"Category\": cat_name,\n",
    "            \"Daily\": \"\" if daily_box == \"[BLANK]\" else daily_box,\n",
    "            \"Cumulative\": \"\" if cum_box == \"[BLANK]\" else cum_box\n",
    "        })\n",
    "    last_box = safe_get(cat_idx + 9)\n",
    "    last_cat = lines[4] if len(lines) >= 5 else \"Rotating Footage\"\n",
    "    remainder = last_box.replace(last_cat, \"\").strip()\n",
    "    tokens = remainder.split()\n",
    "    daily_val = tokens[0] if len(tokens) >= 2 else \"\"\n",
    "    cum_val = tokens[1] if len(tokens) >= 2 else \"\"\n",
    "    structured.append({\n",
    "        \"Category\": last_cat,\n",
    "        \"Daily\": \"\" if daily_val == \"[BLANK]\" else daily_val,\n",
    "        \"Cumulative\": \"\" if cum_val == \"[BLANK]\" else cum_val\n",
    "    })\n",
    "    df = pd.DataFrame(structured)\n",
    "    logger.info(f\"DIR INFO DataFrame shape: {df.shape}\")\n",
    "    return {\"DIR INFO\": structured}, df\n",
    "\n",
    "def process_dir_info(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    return build_dir_info_dict_from_rois(roi_texts, debug=debug)\n",
    "\n",
    "def build_survey_dict_from_rois(roi_texts, expected_headers):\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row.sort(key=lambda c: c[0])\n",
    "        line = \" \".join(cell[4] for cell in row)\n",
    "        row_strings.append(line)\n",
    "        logger.info(f\"SURVEY - Grouped Row {i}: {line}\")\n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for subline in line.split(\"\\n\"):\n",
    "            subline = subline.strip()\n",
    "            if subline:\n",
    "                all_lines.append(subline)\n",
    "    logger.info(f\"SURVEY - All extracted lines: {all_lines}\")\n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"md\" in lower_tokens and \"inclination\" in lower_tokens:\n",
    "            logger.info(f\"SURVEY - Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"SURVEY - Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    logger.info(f\"SURVEY - Data lines to parse: {data_lines}\")\n",
    "    survey_list = []\n",
    "    for tokens in data_lines:\n",
    "        row_dict = {expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "        survey_list.append(row_dict)\n",
    "    return survey_list\n",
    "\n",
    "def sort_survey_data(survey_list):\n",
    "    def md_value(row):\n",
    "        try:\n",
    "            return float(row[\"MD\"].replace(\",\", \"\"))\n",
    "        except Exception:\n",
    "            return 0\n",
    "    return sorted(survey_list, key=md_value, reverse=True)\n",
    "\n",
    "def pipeline_survey_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"MD\", \"Inclination\", \"Azimuth\", \"DLS\", \"TVD\"]\n",
    "    survey_list = build_survey_dict_from_rois(roi_texts, expected_headers)\n",
    "    survey_list = sort_survey_data(survey_list)\n",
    "    df = pd.DataFrame(survey_list)\n",
    "    logger.info(f\"SURVEY DataFrame shape: {df.shape}\")\n",
    "    return {\"SURVEY\": survey_list}, df\n",
    "\n",
    "def build_casing_dict_from_rois(roi_texts, expected_headers, debug=False):\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row.sort(key=lambda c: c[0])\n",
    "        line = \" \".join(cell[4] for cell in row).strip()\n",
    "        row_strings.append(line)\n",
    "        if debug:\n",
    "            logger.info(f\"CASING - Grouped Row {i}: {line}\")\n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for sub in line.split(\"\\n\"):\n",
    "            sub = sub.strip()\n",
    "            if sub:\n",
    "                all_lines.append(sub)\n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"type\" in lower_tokens and \"size\" in lower_tokens:\n",
    "            logger.info(f\"CASING - Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"CASING - Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    casing_list = []\n",
    "    for tokens in data_lines:\n",
    "        row_dict = {expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "        casing_list.append(row_dict)\n",
    "    return casing_list\n",
    "\n",
    "def pipeline_casing_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"Type\", \"Size\", \"Weight\", \"Grade\", \"Connection\", \"Top MD\", \"Bottom MD\", \"TOC\"]\n",
    "    casing_list = build_casing_dict_from_rois(roi_texts, expected_headers, debug=debug)\n",
    "    df = pd.DataFrame(casing_list)\n",
    "    logger.info(f\"CASING DataFrame shape: {df.shape}\")\n",
    "    return {\"CASING\": casing_list}, df\n",
    "\n",
    "def build_consumables_dict_from_rois(roi_texts, debug=False):\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    grouped_rows = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row.sort(key=lambda cell: cell[0])\n",
    "        line = \" \".join(cell[4] for cell in row).strip()\n",
    "        grouped_rows.append(line)\n",
    "        if debug:\n",
    "            logger.info(f\"CONSUMABLES - Grouped Row {i}: {line}\")\n",
    "    data_rows = []\n",
    "    for line in grouped_rows:\n",
    "        lower_line = line.lower()\n",
    "        if (\"consumable\" in lower_line and \"received\" in lower_line) or \"nun\" in lower_line:\n",
    "            continue\n",
    "        if len(line.split()) < 5:\n",
    "            continue\n",
    "        data_rows.append(line)\n",
    "    consumables_list = []\n",
    "    for line in data_rows:\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "        if len(tokens) > 5:\n",
    "            first = \" \".join(tokens[:-4])\n",
    "            tokens = [first] + tokens[-4:]\n",
    "        if len(tokens) != 5:\n",
    "            logger.warning(f\"CONSUMABLES - Skipping row (unexpected token count): {tokens}\")\n",
    "            continue\n",
    "        row_dict = {\n",
    "            \"Consumable\": tokens[0],\n",
    "            \"Daily Received (gal)\": tokens[1],\n",
    "            \"Daily Used (gal)\": tokens[2],\n",
    "            \"Cumulative Used (gal)\": tokens[3],\n",
    "            \"Daily on Hand (gal)\": tokens[4]\n",
    "        }\n",
    "        consumables_list.append(row_dict)\n",
    "    return consumables_list\n",
    "\n",
    "def pipeline_consumables_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    consumables_list = build_consumables_dict_from_rois(roi_texts, debug=debug)\n",
    "    df = pd.DataFrame(consumables_list)\n",
    "    logger.info(f\"CONSUMABLES DataFrame shape: {df.shape}\")\n",
    "    return {\"CONSUMABLES\": consumables_list}, df\n",
    "\n",
    "def extract_bha_data(image_path, debug=False):\n",
    "    image = Image.open(image_path)\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    patterns = {\n",
    "        \"Drill Pipe Detail\": r\"Drill Pipe Detail:\\s*([^\\n]+)\",\n",
    "        \"Size\": r\"Size:\\s*([\\d.]+)\\b\",\n",
    "        \"Wt./Ft\": r\"Wt\\./Ft:\\s*([\\d.]+)\\b\",\n",
    "        \"Connection\": r\"Connection:\\s*([\\w\\d-]+)\\b\",\n",
    "        \"ID\": r\"ID:\\s*([\\d.]+)\\b\",\n",
    "        \"Drill Bit\": r\"Drill Bit:\\s*([^\\n;]+)\",\n",
    "        \"Motor\": r\"Motor:\\s*([^\\n;]+)\",\n",
    "        \"MWD Tool\": r\"MWD Tool:\\s*([^\\n;]+)\",\n",
    "        \"Monel Collar\": r\"Monel Collar:\\s*([^\\n;]+)\",\n",
    "        \"X-Over\": r\"X-Over:\\s*([^\\n;]+)\",\n",
    "        \"Sub\": r\"Sub:\\s*([^\\n;]+)\",\n",
    "        \"HWDP\": r\"HWDP:\\s*([^\\n;]+)\",\n",
    "        \"Drill Pipe\": r\"Drill Pipe:\\s*([\\d.]+(?:\\\" DP)?)\",\n",
    "        \"Reamer\": r\"Reamer:\\s*([^\\n;]+)\",\n",
    "        \"Shock Sub\": r\"Shock Sub:\\s*([^\\n;]+)\",\n",
    "        \"Total Length\": r\"Total Length:\\s*(\\d+)\\b\"\n",
    "    }\n",
    "    bha_data = {}\n",
    "    for key, pat in patterns.items():\n",
    "        match = re.search(pat, ocr_text)\n",
    "        if match:\n",
    "            bha_data[key] = match.group(1).strip()\n",
    "    if \"Drill Pipe Detail\" in bha_data:\n",
    "        detail = bha_data[\"Drill Pipe Detail\"]\n",
    "        for remove_key in [\"Size\", \"Wt./Ft\", \"Connection\", \"ID\"]:\n",
    "            if remove_key in bha_data:\n",
    "                detail = re.sub(rf\"{remove_key}:\\s*{re.escape(bha_data[remove_key])}\", \"\", detail).strip(\",; \")\n",
    "        bha_data[\"Drill Pipe Detail\"] = detail\n",
    "    structured_data = {\n",
    "        \"BHA\": {\n",
    "            \"Drill Pipe Detail\": bha_data.get(\"Drill Pipe Detail\", \"\"),\n",
    "            \"Size\": bha_data.get(\"Size\", \"\"),\n",
    "            \"Wt./Ft\": bha_data.get(\"Wt./Ft\", \"\"),\n",
    "            \"Connection\": bha_data.get(\"Connection\", \"\"),\n",
    "            \"ID\": bha_data.get(\"ID\", \"\"),\n",
    "            \"BHA #4\": {\n",
    "                \"Drill Bit\": bha_data.get(\"Drill Bit\", \"\"),\n",
    "                \"Motor\": bha_data.get(\"Motor\", \"\"),\n",
    "                \"MWD Tool\": bha_data.get(\"MWD Tool\", \"\"),\n",
    "                \"Monel Collar\": bha_data.get(\"Monel Collar\", \"\"),\n",
    "                \"X-Over\": bha_data.get(\"X-Over\", \"\"),\n",
    "                \"Sub\": bha_data.get(\"Sub\", \"\"),\n",
    "                \"HWDP\": bha_data.get(\"HWDP\", \"\"),\n",
    "                \"Drill Pipe\": bha_data.get(\"Drill Pipe\", \"\"),\n",
    "                \"Reamer\": bha_data.get(\"Reamer\", \"\"),\n",
    "                \"Shock Sub\": bha_data.get(\"Shock Sub\", \"\")\n",
    "            },\n",
    "            \"Total Length\": bha_data.get(\"Total Length\", \"\")\n",
    "        }\n",
    "    }\n",
    "    if debug:\n",
    "        logger.info(\"Extracted BHA data:\")\n",
    "        logger.info(json.dumps(structured_data, indent=4))\n",
    "    return structured_data\n",
    "\n",
    "def pipeline_bha_data(debug=False):\n",
    "    image_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_?BHA.png\"  # Adjust as needed\n",
    "    bha_json = extract_bha_data(image_path, debug=debug)\n",
    "    img = safe_read_image(image_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    pump_pattern = re.compile(\n",
    "        r\"^(\\d+)?\\s*(BOMCO)\\s+(TRIPLEX)\\s+(\\d+)?\\s*(\\d+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s*$\",\n",
    "        re.IGNORECASE)\n",
    "    pumps = []\n",
    "    for line in ocr_text.splitlines():\n",
    "        line = line.strip()\n",
    "        match = pump_pattern.match(line)\n",
    "        if match:\n",
    "            (number, model, pump_type, hhp, efficiency, stroke,\n",
    "             liner, p_rating, p_limit, spm_rating, spm_limit) = match.groups()\n",
    "            pumps.append({\n",
    "                \"Number\": number if number else \"\",\n",
    "                \"Model\": model,\n",
    "                \"Type\": pump_type,\n",
    "                \"HHP\": hhp if hhp else \"\",\n",
    "                \"Efficiency\": efficiency,\n",
    "                \"Stroke(in)\": stroke,\n",
    "                \"Liner(in)\": liner,\n",
    "                \"P-Rating(psi)\": p_rating,\n",
    "                \"P-Limit(psi)\": p_limit,\n",
    "                \"SPM Rating\": spm_rating,\n",
    "                \"SPM Limit\": spm_limit\n",
    "            })\n",
    "    circ_pattern = re.compile(\n",
    "        r\"Drilling/Circ\\s+Rate\\s+(\\d+)\\s+(\\d+)\\s+PSI\\s*@\\s*(\\d+)\\s*SPM\\s*([\\d.]+)\\s+Gal/Stoke\\s+([\\d.]+)\\s+GPM\\s+([\\d.]+)\\s+BPM\\s+([\\d.]+)\\s+DC\\s+([\\d.]+)\\s+DP\",\n",
    "        re.IGNORECASE)\n",
    "    circ_rates = []\n",
    "    for line in ocr_text.splitlines():\n",
    "        line = line.strip()\n",
    "        match = circ_pattern.search(line)\n",
    "        if match:\n",
    "            rate_id, pressure, spm, gal_stroke, gpm, bpm, dc, dp = match.groups()\n",
    "            circ_rates.append({\n",
    "                \"RateID\": rate_id,\n",
    "                \"Pressure(PSI)\": pressure,\n",
    "                \"SPM\": spm,\n",
    "                \"Gal/Stoke\": gal_stroke,\n",
    "                \"GPM\": gpm,\n",
    "                \"BPM\": bpm,\n",
    "                \"DC\": dc,\n",
    "                \"DP\": dp\n",
    "            })\n",
    "    pumps_df = pd.DataFrame(pumps)\n",
    "    circ_df = pd.DataFrame(circ_rates)\n",
    "    bha_df = pd.DataFrame([bha_json])\n",
    "    logger.info(f\"BHA DataFrame shape: {bha_df.shape}\")\n",
    "    combined = {\"BHA\": bha_json, \"Pumps\": pumps, \"DrillingCircRates\": circ_rates}\n",
    "    return combined, pumps_df, circ_df, bha_df\n",
    "\n",
    "def build_mud_dict_from_rois(roi_texts, expected_headers, debug=False):\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row.sort(key=lambda c: c[0])\n",
    "        line_text = \" \".join(cell[4] for cell in row)\n",
    "        row_strings.append(line_text)\n",
    "        if debug:\n",
    "            logger.info(f\"MUD - Row {i} text: {line_text}\")\n",
    "    tokens1 = row_strings[1].split() if len(row_strings) > 1 else []\n",
    "    tokens2 = row_strings[2].split() if len(row_strings) > 2 else []\n",
    "    combined_tokens = tokens1 + tokens2\n",
    "    def parse_value_row_tokens(expected_headers, tokens):\n",
    "        expected_token_count = (len(expected_headers) - 1) + 3\n",
    "        if len(tokens) < expected_token_count:\n",
    "            tokens += [\"[BLANK]\"] * (expected_token_count - len(tokens))\n",
    "        elif len(tokens) > expected_token_count:\n",
    "            tokens = tokens[:expected_token_count]\n",
    "        result = {}\n",
    "        idx = 0\n",
    "        for header in expected_headers:\n",
    "            if header == \"GELS (10s/10m/30m)\":\n",
    "                gels_tokens = tokens[idx:idx+3]\n",
    "                result[header] = {\"10s\": gels_tokens[0], \"10m\": gels_tokens[1], \"30m\": gels_tokens[2]}\n",
    "                idx += 3\n",
    "            else:\n",
    "                result[header] = tokens[idx]\n",
    "                idx += 1\n",
    "        return result\n",
    "    mapped = parse_value_row_tokens(expected_headers, combined_tokens)\n",
    "    return mapped\n",
    "\n",
    "def pipeline_mud_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"Type\", \"Weight In\", \"Weight Out\", \"pH\", \"CAKE\",\n",
    "                        \"GELS (10s/10m/30m)\", \"Oil/Water\", \"FV\", \"ES\", \"PV\",\n",
    "                        \"YP\", \"CL\", \"Ca\", \"LGS\", \"WL\", \"HTHP Loss\", \"3 RPM\",\n",
    "                        \"6 RPM\", \"Mud Pits and Hole Volume\", \"24 Hr Loss\",\n",
    "                        \"Total Loss\", \"Comments\"]\n",
    "    mud_dict = build_mud_dict_from_rois(roi_texts, expected_headers, debug=debug)\n",
    "    df = pd.DataFrame([mud_dict])\n",
    "    logger.info(f\"MUD DataFrame shape: {df.shape}\")\n",
    "    return {\"MUD\": mud_dict}, df\n",
    "import pytesseract\n",
    "import re\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def extract_bha_data(image_path):\n",
    "    # Load image and perform OCR\n",
    "    image = Image.open(image_path)\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    \n",
    "    # Define regex patterns to extract key values without repetition\n",
    "    patterns = {\n",
    "        \"Drill Pipe Detail\": r\"Drill Pipe Detail:\\s*([^\\n]+)\",  # Extracts full text but **won't duplicate fields**\n",
    "        \"Size\": r\"Size:\\s*([\\d.]+)\\b\",\n",
    "        \"Wt./Ft\": r\"Wt\\./Ft:\\s*([\\d.]+)\\b\",\n",
    "        \"Connection\": r\"Connection:\\s*([\\w\\d-]+)\\b\",\n",
    "        \"ID\": r\"ID:\\s*([\\d.]+)\\b\",\n",
    "        \"Drill Bit\": r\"Drill Bit:\\s*([^\\n;]+)\",\n",
    "        \"Motor\": r\"Motor:\\s*([^\\n;]+)\",\n",
    "        \"MWD Tool\": r\"MWD Tool:\\s*([^\\n;]+)\",\n",
    "        \"Monel Collar\": r\"Monel Collar:\\s*([^\\n;]+)\",\n",
    "        \"X-Over\": r\"X-Over:\\s*([^\\n;]+)\",\n",
    "        \"Sub\": r\"Sub:\\s*([^\\n;]+)\",\n",
    "        \"HWDP\": r\"HWDP:\\s*([^\\n;]+)\",\n",
    "        \"Drill Pipe\": r\"Drill Pipe:\\s*([\\d.]+(?:\\\" DP)?)\",  \n",
    "        \"Reamer\": r\"Reamer:\\s*([^\\n;]+)\",\n",
    "        \"Shock Sub\": r\"Shock Sub:\\s*([^\\n;]+)\",\n",
    "        \"Total Length\": r\"Total Length:\\s*(\\d+)\\b\"\n",
    "    }\n",
    "    \n",
    "    # Extract data\n",
    "    bha_data = {}\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, ocr_text)\n",
    "        if match:\n",
    "            bha_data[key] = match.group(1).strip()\n",
    "    \n",
    "    # **Fix duplication issue:** Remove Size, Wt./Ft, Connection, ID from `\"Drill Pipe Detail\"`\n",
    "    if \"Drill Pipe Detail\" in bha_data:\n",
    "        detail = bha_data[\"Drill Pipe Detail\"]\n",
    "        for remove_key in [\"Size\", \"Wt./Ft\", \"Connection\", \"ID\"]:\n",
    "            if remove_key in bha_data:\n",
    "                detail = re.sub(rf\"{remove_key}:\\s*{re.escape(bha_data[remove_key])}\", \"\", detail).strip(\",; \")\n",
    "        bha_data[\"Drill Pipe Detail\"] = detail  # Store cleaned version\n",
    "\n",
    "    # **Final structured JSON without repetition**\n",
    "    structured_data = {\n",
    "        \"BHA\": {\n",
    "            \"Drill Pipe Detail\": bha_data.get(\"Drill Pipe Detail\", \"\"),\n",
    "            \"Size\": bha_data.get(\"Size\", \"\"),\n",
    "            \"Wt./Ft\": bha_data.get(\"Wt./Ft\", \"\"),\n",
    "            \"Connection\": bha_data.get(\"Connection\", \"\"),\n",
    "            \"ID\": bha_data.get(\"ID\", \"\"),\n",
    "            \"BHA #4\": {\n",
    "                \"Drill Bit\": bha_data.get(\"Drill Bit\", \"\"),\n",
    "                \"Motor\": bha_data.get(\"Motor\", \"\"),\n",
    "                \"MWD Tool\": bha_data.get(\"MWD Tool\", \"\"),\n",
    "                \"Monel Collar\": bha_data.get(\"Monel Collar\", \"\"),\n",
    "                \"X-Over\": bha_data.get(\"X-Over\", \"\"),\n",
    "                \"Sub\": bha_data.get(\"Sub\", \"\"),\n",
    "                \"HWDP\": bha_data.get(\"HWDP\", \"\"),\n",
    "                \"Drill Pipe\": bha_data.get(\"Drill Pipe\", \"\"),\n",
    "                \"Reamer\": bha_data.get(\"Reamer\", \"\"),\n",
    "                \"Shock Sub\": bha_data.get(\"Shock Sub\", \"\")\n",
    "            },\n",
    "            \"Total Length\": bha_data.get(\"Total Length\", \"\")  # ✅ Now correctly placed at the end\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return structured_data\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Parse Pumps Table\n",
    "# ------------------------------------------------------------------\n",
    "def parse_pumps_table(ocr_text):\n",
    "    \"\"\"\n",
    "    Parses the pumps table from the OCR text.\n",
    "    Expected lines look like:\n",
    "      Number Model Type   HHP  Efficiency  Stroke(in)  Liner(in)  P-Rating(psi)  P-Limit(psi)  SPM Rating  SPM Limit\n",
    "      1      BOMCO TRIPLEX 1600 95       12.000       4.75       7500           7100          120         110\n",
    "      2      BOMCO TRIPLEX 1600 95       12.000       4.75       7500           7100          120         110\n",
    "      (possibly missing fields in some rows)\n",
    "    \"\"\"\n",
    "\n",
    "    # We’ll search for lines that look like:\n",
    "    #   <Number> BOMCO TRIPLEX <HHP> <Eff> <Stroke> <Liner> <P-Rating> <P-Limit> <SPM Rating> <SPM Limit>\n",
    "    #   or possibly missing the Number or HHP.\n",
    "    # We'll capture them with a regex that checks for 8-11 columns.\n",
    "    # You can refine further as needed.\n",
    "    pump_pattern = re.compile(\n",
    "        r\"^(\\d+)?\\s*\"               # Number (optional)\n",
    "        r\"(BOMCO)\\s+(TRIPLEX)\\s+\"    # Model, Type\n",
    "        r\"(\\d+)?\\s*\"                 # HHP (optional)\n",
    "        r\"(\\d+)\\s+\"                  # Efficiency\n",
    "        r\"([\\d.]+)\\s+\"               # Stroke(in)\n",
    "        r\"([\\d.]+)\\s+\"               # Liner(in)\n",
    "        r\"(\\d+)\\s+\"                  # P-Rating(psi)\n",
    "        r\"(\\d+)\\s+\"                  # P-Limit(psi)\n",
    "        r\"(\\d+)\\s+\"                  # SPM Rating\n",
    "        r\"(\\d+)\\s*$\",                # SPM Limit\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    lines = ocr_text.splitlines()\n",
    "    pumps = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        match = pump_pattern.match(line)\n",
    "        if match:\n",
    "            # Extract fields\n",
    "            number, model, pump_type, hhp, efficiency, stroke, liner, p_rating, p_limit, spm_rating, spm_limit = match.groups()\n",
    "\n",
    "            # Store as dictionary\n",
    "            pumps.append({\n",
    "                \"Number\": number if number else \"\",\n",
    "                \"Model\": model,\n",
    "                \"Type\": pump_type,\n",
    "                \"HHP\": hhp if hhp else \"\",\n",
    "                \"Efficiency\": efficiency,\n",
    "                \"Stroke(in)\": stroke,\n",
    "                \"Liner(in)\": liner,\n",
    "                \"P-Rating(psi)\": p_rating,\n",
    "                \"P-Limit(psi)\": p_limit,\n",
    "                \"SPM Rating\": spm_rating,\n",
    "                \"SPM Limit\": spm_limit\n",
    "            })\n",
    "\n",
    "    return pumps\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Parse Drilling/Circ Rates\n",
    "# ------------------------------------------------------------------\n",
    "def parse_drilling_circ_rates(ocr_text):\n",
    "    \"\"\"\n",
    "    Parses lines like:\n",
    "      Drilling/Circ Rate 1 4325 PSI @ 134 SPM 2.63 Gal/Stoke 351.76 GPM 8.38 BPM 468.11 DC 340.61 DP\n",
    "      Drilling/Circ Rate 2 4475 PSI @ 134 SPM 2.63 Gal/Stoke 351.76 GPM 8.38 BPM 468.11 DC 340.61 DP\n",
    "    We'll store them in a structured list of dicts.\n",
    "    \"\"\"\n",
    "\n",
    "    # We'll define a pattern capturing Rate #, Pressure, SPM, Gal/Stoke, GPM, BPM, DC, DP, etc.\n",
    "    # Example line:\n",
    "    #   Drilling/Circ Rate 1 4325 PSI @ 134 SPM 2.63 Gal/Stoke 351.76 GPM 8.38 BPM 468.11 DC 340.61 DP\n",
    "    circ_pattern = re.compile(\n",
    "        r\"Drilling/Circ\\s+Rate\\s+(\\d+)\\s+(\\d+)\\s+PSI\\s*@\\s*(\\d+)\\s*SPM\\s*([\\d.]+)\\s+Gal/Stoke\\s+([\\d.]+)\\s+GPM\\s+([\\d.]+)\\s+BPM\\s+([\\d.]+)\\s+DC\\s+([\\d.]+)\\s+DP\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    lines = ocr_text.splitlines()\n",
    "    circ_rates = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        match = circ_pattern.search(line)\n",
    "        if match:\n",
    "            rate_id, pressure, spm, gal_stroke, gpm, bpm, dc, dp = match.groups()\n",
    "            circ_rates.append({\n",
    "                \"RateID\": rate_id,\n",
    "                \"Pressure(PSI)\": pressure,\n",
    "                \"SPM\": spm,\n",
    "                \"Gal/Stoke\": gal_stroke,\n",
    "                \"GPM\": gpm,\n",
    "                \"BPM\": bpm,\n",
    "                \"DC\": dc,\n",
    "                \"DP\": dp\n",
    "            })\n",
    "\n",
    "    return circ_rates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main Function: Run All Pipelines in Sequence & Save Outputs\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_pipeline(name, pipeline_info, debug=False):\n",
    "    try:\n",
    "        logger.info(f\"Processing pipeline: {name}\")\n",
    "        output_json, df = pipeline_info[\"func\"](debug)\n",
    "        logger.info(f\"{name} DataFrame shape: {df.shape} (Rows: {df.shape[0]}, Columns: {df.shape[1]})\")\n",
    "        json_path = os.path.join(JSON_FOLDER, pipeline_info[\"json\"])\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(output_json, f, indent=4)\n",
    "        csv_path = os.path.join(CSV_FOLDER, pipeline_info[\"csv\"])\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"{name} saved: JSON({json_path}), CSV({csv_path})\")\n",
    "        print(f\"--- {name.upper()} DataFrame ---\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(df.head(10))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in pipeline {name}: {e}\")\n",
    "\n",
    "def main():\n",
    "    debug = False  # Set True for detailed logging\n",
    "    pipelines = {\n",
    "        \"cost_data\": {\n",
    "            \"func\": pipeline_cost_data,\n",
    "            \"csv\": \"cost_data.csv\",\n",
    "            \"json\": \"cost_data.json\"\n",
    "        },\n",
    "        \"well_job\": {\n",
    "            \"func\": lambda d: process_well_job_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\", d),\n",
    "            \"csv\": \"well_job_data.csv\",\n",
    "            \"json\": \"well_job_data.json\"\n",
    "        },\n",
    "        \"obs_int\": {\n",
    "            \"func\": lambda d: process_obs_int(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\", d),\n",
    "            \"csv\": \"obs_int_data.csv\",\n",
    "            \"json\": \"obs_int_data.json\"\n",
    "        },\n",
    "        \"bop\": {\n",
    "            \"func\": lambda d: process_bop(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\", d),\n",
    "            \"csv\": \"bop_data.csv\",\n",
    "            \"json\": \"bop_data.json\"\n",
    "        },\n",
    "        \"dir_info\": {\n",
    "            \"func\": lambda d: process_dir_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\", d),\n",
    "            \"csv\": \"dir_info_data.csv\",\n",
    "            \"json\": \"dir_info_data.json\"\n",
    "        },\n",
    "        \"survey\": {\n",
    "            \"func\": pipeline_survey_data,\n",
    "            \"csv\": \"survey_data.csv\",\n",
    "            \"json\": \"survey_data.json\"\n",
    "        },\n",
    "        \"casing\": {\n",
    "            \"func\": pipeline_casing_data,\n",
    "            \"csv\": \"casing_data.csv\",\n",
    "            \"json\": \"casing_data.json\"\n",
    "        },\n",
    "        \"consumables\": {\n",
    "            \"func\": pipeline_consumables_data,\n",
    "            \"csv\": \"consumables_data.csv\",\n",
    "            \"json\": \"consumables_data.json\"\n",
    "        },\n",
    "        \"mud\": {\n",
    "            \"func\": pipeline_mud_data,\n",
    "            \"csv\": \"mud_data.csv\",\n",
    "            \"json\": \"mud_data.json\"\n",
    "        },\n",
    "        \"bha\": {\n",
    "            \"func\": extract_bha_data,\n",
    "            \"csv\": \"bha_data.csv\",\n",
    "            \"json\": \"bha_data.json\"\n",
    "        },\n",
    "        \"pumps\": {\n",
    "            \"func\": parse_pumps_table + parse_drilling_circ_rates, # append them both\n",
    "            \"csv\": \"pumps_data.csv\",\n",
    "            \"json\": \"pumps_data.json\"\n",
    "        }\n",
    "    }\n",
    "    for name, pipe in pipelines.items():\n",
    "        run_pipeline(name, pipe, debug)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e32e64-5933-4252-8c70-32e4644c668c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7760541951788028,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "preprocess_sections_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
