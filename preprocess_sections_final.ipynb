{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221c90b2-5321-43a6-b6d4-bb111080a359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496d7298-d546-4673-aef5-bfcf51863f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e55589f5-8cb3-4585-9ecb-3c5e0e94e45e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Logger Setup\n",
    "# ---------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"UnifiedExtractor\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility Functions\n",
    "# ---------------------------------------------------------------------\n",
    "def dbfs_to_local_path(dbfs_path):\n",
    "    \"\"\"Convert a DBFS URI to a local path.\"\"\"\n",
    "    if dbfs_path.startswith(\"dbfs:/\"):\n",
    "        return \"/dbfs/\" + dbfs_path[len(\"dbfs:/\"):]\n",
    "    return dbfs_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ff3e0a-8066-4a06-af13-149944ca548f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_cropped_section_image(section_path):\n",
    "    \"\"\"Read an image from a cropped section (handles DBFS paths).\"\"\"\n",
    "    local_path = section_path\n",
    "    if local_path.startswith(\"dbfs:\"):\n",
    "        local_path = local_path.replace(\"dbfs:\", \"\")\n",
    "    if local_path.startswith(\"/mnt/\"):\n",
    "        local_path = \"/dbfs\" + local_path\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"OpenCV failed to load image: {local_path}\")\n",
    "    logger.info(f\"Image loaded from {local_path} with shape {img.shape}\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cad0f93-61a6-4cb9-81ec-03b6fa129e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# COMMON UTILITY FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def safe_read_image(img_path):\n",
    "    local_path = img_path if not img_path.startswith(\"dbfs:\") else img_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {local_path}\")\n",
    "    logger.info(f\"Image loaded from {local_path} with shape {img.shape}\")\n",
    "    return img\n",
    "\n",
    "def preprocess_image(img, debug=False):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 15, 9)\n",
    "    if debug:\n",
    "        logger.info(\"Preprocessing completed (grayscale and threshold applied).\")\n",
    "    return thresh\n",
    "\n",
    "def detect_text_regions(thresh_img, debug=False):\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "    rois.sort(key=lambda b: (b[1], b[0]))\n",
    "    if debug:\n",
    "        logger.info(f\"Detected {len(rois)} text regions.\")\n",
    "    return rois\n",
    "\n",
    "def perform_ocr_on_rois(img, rois, debug=False):\n",
    "    results = []\n",
    "    for (x, y, w, h) in rois:\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        text = pytesseract.image_to_string(roi, config=\"--psm 6\").strip() or \"[BLANK]\"\n",
    "        results.append((x, y, w, h, text))\n",
    "        if debug:\n",
    "            logger.info(f\"OCR Box ({x},{y},{w},{h}): {text}\")\n",
    "    return results\n",
    "\n",
    "def perform_ocr(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return pytesseract.image_to_string(gray, config=\"--psm 6\")\n",
    "\n",
    "# def perform_ocr(img):\n",
    "#     \"\"\"\n",
    "#     Performs OCR on the given PIL image using pytesseract.\n",
    "#     Returns raw text.\n",
    "#     \"\"\"\n",
    "#     text = pytesseract.image_to_string(img)\n",
    "#     logger.info(\"OCR extraction complete.\")\n",
    "#     return text\n",
    "\n",
    "def extract_key_value_from_text(text, expected_keys):\n",
    "    combined = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "    combined = re.sub(r'\\s+', ' ', combined)\n",
    "    result = {}\n",
    "    for i, key in enumerate(expected_keys):\n",
    "        if i < len(expected_keys) - 1:\n",
    "            next_key = expected_keys[i+1]\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*?)(?=\\s*' + re.escape(next_key) + r'\\s*:|$)'\n",
    "        else:\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*)'\n",
    "        match = re.search(pattern, combined, re.IGNORECASE)\n",
    "        result[key] = match.group(1).strip() if match and match.group(1).strip() else None\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ae68a4-313d-43fc-9204-bfb2585e8707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# PARSING HELPERS\n",
    "# -----------------------------------------------------------------------------\n",
    "def group_ocr_results(roi_texts, row_tolerance=10):\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for row in rows:\n",
    "        row.sort(key=lambda c: c[0])\n",
    "        line = \" \".join(cell[4] for cell in row)\n",
    "        row_strings.append(line)\n",
    "    return row_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadc8fe9-dc15-4306-9227-a20582ece1e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_casing_dict_from_rois(roi_texts, expected_headers, debug=False):\n",
    "    row_strings = group_ocr_results(roi_texts)\n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for sub in line.split(\"\\n\"):\n",
    "            sub = sub.strip()\n",
    "            if sub:\n",
    "                all_lines.append(sub)\n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"type\" in lower_tokens and \"size\" in lower_tokens:\n",
    "            logger.info(f\"CASING - Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"CASING - Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    casing_list = [{expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "                   for tokens in data_lines]\n",
    "    return casing_list\n",
    "\n",
    "def process_casing_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"Type\", \"Size\", \"Weight\", \"Grade\", \"Connection\", \"Top MD\", \"Bottom MD\", \"TOC\"]\n",
    "    casing_list = build_casing_dict_from_rois(roi_texts, expected_headers, debug=debug)\n",
    "    df = pd.DataFrame(casing_list)\n",
    "    logger.info(f\"CASING DataFrame shape: {df.shape}\")\n",
    "    return {\"CASING\": casing_list}, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef3e812-d63d-43ec-b60a-977218a06b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# process FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def process_cost_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"Cost OCR extraction complete.\")\n",
    "    expected_keys = [\n",
    "        \"Drilling AFE Amount\", \"Daily Drilling Cost\", \"Cumulative Drilling Cost\",\n",
    "        \"Cumulative Well Cost\", \"Daily Mud Cost\", \"Cumulative Mud Cost\"\n",
    "    ]\n",
    "    extracted = extract_key_value_from_text(ocr_text, expected_keys)\n",
    "    df = pd.DataFrame(list(extracted.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"COST DataFrame shape: {df.shape}\")\n",
    "    return {\"COST DATA\": extracted}, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf9b7c9-14c0-4098-b9fc-45ad2d35fc69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_well_job_info(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"Well/Job OCR extraction complete.\")\n",
    "    expected_keys = [\n",
    "        \"Well Name\", \"Job Name\", \"Supervisor(s)\", \"Field\", \"Sec/Twn/Rng\", \"Phone\",\n",
    "        \"AFE #\", \"API #\", \"Email\", \"Contractor\", \"Elevation\", \"RKB\",\n",
    "        \"Spud Date\", \"Days from Spud\", \"Days on Loc\", \"MD/TVD\", \"24 Hr Footage\",\n",
    "        \"Present Operations\", \"Activity Planned\"\n",
    "    ]\n",
    "    combined = \" \".join(line.strip() for line in ocr_text.splitlines() if line.strip())\n",
    "    combined = re.sub(r'\\s+', ' ', combined)\n",
    "    result = {}\n",
    "    for i, key in enumerate(expected_keys):\n",
    "        if i < len(expected_keys) - 1:\n",
    "            next_key = expected_keys[i+1]\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*?)(?=\\s*' + re.escape(next_key) + r'\\s*:|$)'\n",
    "        else:\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*)'\n",
    "        match = re.search(pattern, combined, re.IGNORECASE)\n",
    "        result[key] = match.group(1).strip() if match else \"\"\n",
    "    df = pd.DataFrame(list(result.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"WELL/JOB DataFrame shape: {df.shape}\")\n",
    "    return {\"WELL/JOB INFORMATION\": result}, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323dfced-6c83-4009-9dd2-e4055653b649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_obs_int(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    header_str = \"daily numbers: observation & intervention\"\n",
    "    all_texts = [t[4] for t in roi_texts]\n",
    "    types_list, numbers_list = [], []\n",
    "    for txt in all_texts:\n",
    "        clean = txt.strip()\n",
    "        if clean.lower() in [header_str, \"number\", \"[blank]\"]:\n",
    "            continue\n",
    "        try:\n",
    "            float(clean)\n",
    "            numbers_list.append(\"\" if clean.lower() == \"[blank]\" else clean)\n",
    "            continue\n",
    "        except ValueError:\n",
    "            pass\n",
    "        if \"\\n\" in clean:\n",
    "            for line in clean.splitlines():\n",
    "                line = line.strip()\n",
    "                if line and line.lower() != \"[blank]\":\n",
    "                    types_list.append(line)\n",
    "        else:\n",
    "            types_list.append(clean)\n",
    "    expected_count = 5\n",
    "    while len(numbers_list) < expected_count:\n",
    "        numbers_list.append(\"\")\n",
    "    types_list = types_list[:expected_count]\n",
    "    numbers_list = numbers_list[:expected_count]\n",
    "    structured = [{\"Type\": types_list[i], \"Number\": numbers_list[i]} for i in range(expected_count)]\n",
    "    df = pd.DataFrame(structured)\n",
    "    logger.info(f\"DAILY NUMBERS: OBSERVATION & INTERVENTION DataFrame shape: {df.shape}\")\n",
    "    return {\"DAILY NUMBERS: OBSERVATION & INTERVENTION\": structured}, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ee02c77-95e4-4b45-bff3-daf07be6717e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_bop(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"BOP OCR extraction complete.\")\n",
    "    patterns = {\n",
    "        \"Last BOP Test Date\": r\"Last BOP Test Date\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Last BOP Drill\": r\"Last BOP Drill\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Next BOP Test\": r\"Next BOP Test\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\"\n",
    "    }\n",
    "    result = {}\n",
    "    for key, regex in patterns.items():\n",
    "        match = re.search(regex, ocr_text, re.IGNORECASE)\n",
    "        result[key] = match.group(1) if match else \"\"\n",
    "    df = pd.DataFrame(list(result.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"BOP DataFrame shape: {df.shape}\")\n",
    "    return {\"BOP\": result}, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007662d6-c29b-4de9-aef9-d6c7a24337a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_dir_info_dict_from_rois(roi_texts, debug=False):\n",
    "    all_texts = [t[4] for t in roi_texts]\n",
    "    daily_cum_idx = next((i for i, txt in enumerate(all_texts)\n",
    "                           if \"daily\" in txt.lower() and \"cumulative\" in txt.lower()), None)\n",
    "    if daily_cum_idx is None:\n",
    "        logger.warning(\"Could not find 'Daily Cumulative' bounding box.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    cat_idx = daily_cum_idx + 1\n",
    "    if cat_idx >= len(all_texts):\n",
    "        logger.warning(\"No bounding box after 'Daily Cumulative'.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    categories_box = all_texts[cat_idx]\n",
    "    lines = [ln.strip() for ln in categories_box.split(\"\\n\") if ln.strip()]\n",
    "    if len(lines) < 5:\n",
    "        logger.warning(f\"Expected 5 category lines, got {len(lines)}: {lines}\")\n",
    "    def safe_get(idx):\n",
    "        return all_texts[idx] if 0 <= idx < len(all_texts) else \"\"\n",
    "    structured = []\n",
    "    for i in range(4):\n",
    "        cat_name = lines[i] if i < len(lines) else f\"Unknown Category {i+1}\"\n",
    "        daily_box = safe_get(cat_idx + 1 + (i * 2))\n",
    "        cum_box = safe_get(cat_idx + 2 + (i * 2))\n",
    "        structured.append({\n",
    "            \"Category\": cat_name,\n",
    "            \"Daily\": \"\" if daily_box == \"[BLANK]\" else daily_box,\n",
    "            \"Cumulative\": \"\" if cum_box == \"[BLANK]\" else cum_box\n",
    "        })\n",
    "    last_box = safe_get(cat_idx + 9)\n",
    "    last_cat = lines[4] if len(lines) >= 5 else \"Rotating Footage\"\n",
    "    remainder = last_box.replace(last_cat, \"\").strip()\n",
    "    tokens = remainder.split()\n",
    "    daily_val = tokens[0] if len(tokens) >= 2 else \"\"\n",
    "    cum_val = tokens[1] if len(tokens) >= 2 else \"\"\n",
    "    structured.append({\n",
    "        \"Category\": last_cat,\n",
    "        \"Daily\": \"\" if daily_val == \"[BLANK]\" else daily_val,\n",
    "        \"Cumulative\": \"\" if cum_val == \"[BLANK]\" else cum_val\n",
    "    })\n",
    "    df = pd.DataFrame(structured)\n",
    "    logger.info(f\"DIR INFO DataFrame shape: {df.shape}\")\n",
    "    return {\"DIR INFO\": structured}, df\n",
    "\n",
    "def process_dir_info(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    return build_dir_info_dict_from_rois(roi_texts, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f309641-a8e1-4fe6-bc45-8a9752862d77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_consumables_dict_from_rois(roi_texts, debug=False):\n",
    "    row_strings = group_ocr_results(roi_texts)\n",
    "    data_rows = []\n",
    "    for line in row_strings:\n",
    "        lower_line = line.lower()\n",
    "        if (\"consumable\" in lower_line and \"received\" in lower_line) or \"nun\" in lower_line:\n",
    "            continue\n",
    "        if len(line.split()) < 5:\n",
    "            continue\n",
    "        data_rows.append(line)\n",
    "    consumables_list = []\n",
    "    for line in data_rows:\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "        if len(tokens) > 5:\n",
    "            first = \" \".join(tokens[:-4])\n",
    "            tokens = [first] + tokens[-4:]\n",
    "        if len(tokens) != 5:\n",
    "            logger.warning(f\"CONSUMABLES - Skipping row (unexpected token count): {tokens}\")\n",
    "            continue\n",
    "        consumables_list.append({\n",
    "            \"Consumable\": tokens[0],\n",
    "            \"Daily Received (gal)\": tokens[1],\n",
    "            \"Daily Used (gal)\": tokens[2],\n",
    "            \"Cumulative Used (gal)\": tokens[3],\n",
    "            \"Daily on Hand (gal)\": tokens[4]\n",
    "        })\n",
    "    return consumables_list\n",
    "\n",
    "def process_consumables_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    consumables_list = build_consumables_dict_from_rois(roi_texts, debug=debug)\n",
    "    df = pd.DataFrame(consumables_list)\n",
    "    logger.info(f\"CONSUMABLES DataFrame shape: {df.shape}\")\n",
    "    return {\"CONSUMABLES\": consumables_list}, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a9eea98-76aa-4aa0-a2b5-264e218ed9d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_bha_data(image_path, debug=False):\n",
    "    image = Image.open(image_path)\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    patterns = {\n",
    "        \"Drill Pipe Detail\": r\"Drill Pipe Detail:\\s*([^\\n]+)\",\n",
    "        \"Size\": r\"Size:\\s*([\\d.]+)\\b\",\n",
    "        \"Wt./Ft\": r\"Wt\\./Ft:\\s*([\\d.]+)\\b\",\n",
    "        \"Connection\": r\"Connection:\\s*([\\w\\d-]+)\\b\",\n",
    "        \"ID\": r\"ID:\\s*([\\d.]+)\\b\",\n",
    "        \"Drill Bit\": r\"Drill Bit:\\s*([^\\n;]+)\",\n",
    "        \"Motor\": r\"Motor:\\s*([^\\n;]+)\",\n",
    "        \"MWD Tool\": r\"MWD Tool:\\s*([^\\n;]+)\",\n",
    "        \"Monel Collar\": r\"Monel Collar:\\s*([^\\n;]+)\",\n",
    "        \"X-Over\": r\"X-Over:\\s*([^\\n;]+)\",\n",
    "        \"Sub\": r\"Sub:\\s*([^\\n;]+)\",\n",
    "        \"HWDP\": r\"HWDP:\\s*([^\\n;]+)\",\n",
    "        \"Drill Pipe\": r\"Drill Pipe:\\s*([\\d.]+(?:\\\" DP)?)\",\n",
    "        \"Reamer\": r\"Reamer:\\s*([^\\n;]+)\",\n",
    "        \"Shock Sub\": r\"Shock Sub:\\s*([^\\n;]+)\",\n",
    "        \"Total Length\": r\"Total Length:\\s*(\\d+)\\b\"\n",
    "    }\n",
    "    bha_data = {}\n",
    "    for key, pat in patterns.items():\n",
    "        match = re.search(pat, ocr_text)\n",
    "        if match:\n",
    "            bha_data[key] = match.group(1).strip()\n",
    "    if \"Drill Pipe Detail\" in bha_data:\n",
    "        detail = bha_data[\"Drill Pipe Detail\"]\n",
    "        for remove_key in [\"Size\", \"Wt./Ft\", \"Connection\", \"ID\"]:\n",
    "            if remove_key in bha_data:\n",
    "                detail = re.sub(rf\"{remove_key}:\\s*{re.escape(bha_data[remove_key])}\", \"\", detail).strip(\",; \")\n",
    "        bha_data[\"Drill Pipe Detail\"] = detail\n",
    "    structured_data = {\n",
    "        \"BHA\": {\n",
    "            \"Drill Pipe Detail\": bha_data.get(\"Drill Pipe Detail\", \"\"),\n",
    "            \"Size\": bha_data.get(\"Size\", \"\"),\n",
    "            \"Wt./Ft\": bha_data.get(\"Wt./Ft\", \"\"),\n",
    "            \"Connection\": bha_data.get(\"Connection\", \"\"),\n",
    "            \"ID\": bha_data.get(\"ID\", \"\"),\n",
    "            \"BHA #4\": {\n",
    "                \"Drill Bit\": bha_data.get(\"Drill Bit\", \"\"),\n",
    "                \"Motor\": bha_data.get(\"Motor\", \"\"),\n",
    "                \"MWD Tool\": bha_data.get(\"MWD Tool\", \"\"),\n",
    "                \"Monel Collar\": bha_data.get(\"Monel Collar\", \"\"),\n",
    "                \"X-Over\": bha_data.get(\"X-Over\", \"\"),\n",
    "                \"Sub\": bha_data.get(\"Sub\", \"\"),\n",
    "                \"HWDP\": bha_data.get(\"HWDP\", \"\"),\n",
    "                \"Drill Pipe\": bha_data.get(\"Drill Pipe\", \"\"),\n",
    "                \"Reamer\": bha_data.get(\"Reamer\", \"\"),\n",
    "                \"Shock Sub\": bha_data.get(\"Shock Sub\", \"\")\n",
    "            },\n",
    "            \"Total Length\": bha_data.get(\"Total Length\", \"\")\n",
    "        }\n",
    "    }\n",
    "    if debug:\n",
    "        logger.info(\"Extracted BHA data:\")\n",
    "        logger.info(json.dumps(structured_data, indent=4))\n",
    "    return structured_data\n",
    "\n",
    "def process_bha_data(debug=False):\n",
    "    image_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_11.png\"  # Adjust as needed\n",
    "    bha_json = extract_bha_data(image_path, debug=debug)\n",
    "    img = safe_read_image(image_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    pump_data = parse_pumps_table(ocr_text)\n",
    "    circ_data = parse_drilling_circ_rates(ocr_text)\n",
    "    pumps_df = pd.DataFrame(pump_data)\n",
    "    circ_df = pd.DataFrame(circ_data)\n",
    "    bha_df = pd.DataFrame([bha_json])\n",
    "    logger.info(f\"BHA DataFrame shape: {bha_df.shape}\")\n",
    "    combined = {\"BHA\": bha_json, \"Pumps\": pump_data, \"DrillingCircRates\": circ_data}\n",
    "    return combined, pumps_df, circ_df, bha_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33043d19-605f-4834-b763-a1a940bed780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_mud_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\n",
    "        \"Type\", \"Weight In\", \"Weight Out\", \"pH\", \"CAKE\",\n",
    "        \"GELS (10s/10m/30m)\", \"Oil/Water\", \"FV\", \"ES\", \"PV\",\n",
    "        \"YP\", \"CL\", \"Ca\", \"LGS\", \"WL\", \"HTHP Loss\", \"3 RPM\",\n",
    "        \"6 RPM\", \"Mud Pits and Hole Volume\", \"24 Hr Loss\",\n",
    "        \"Total Loss\", \"Comments\"\n",
    "    ]\n",
    "    mud_dict = build_mud_dict_from_rois(roi_texts, expected_headers)\n",
    "    return {\"MUD\": mud_dict}, pd.DataFrame(list(mud_dict.items()), columns=[\"Key\", \"Value\"])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# parse_value_row_tokens for mud section\n",
    "# ---------------------------------------------------------------------\n",
    "def parse_value_row_tokens(expected_headers, tokens):\n",
    "    \"\"\"\n",
    "    Map a flat list of tokens to the expected headers.\n",
    "    For \"GELS (10s/10m/30m)\", consume 3 tokens and create a sub-dictionary.\n",
    "    Expected token count = (number of headers - 1) + 3.\n",
    "    \"\"\"\n",
    "    expected_token_count = (len(expected_headers) - 1) + 3\n",
    "    logger.info(f\"Expected token count: {expected_token_count}, tokens extracted: {tokens}\")\n",
    "    \n",
    "    # Pad or trim tokens as needed.\n",
    "    if len(tokens) < expected_token_count:\n",
    "        tokens += [\"[BLANK]\"] * (expected_token_count - len(tokens))\n",
    "        logger.warning(\"Not enough tokens. Padding with [BLANK].\")\n",
    "    elif len(tokens) > expected_token_count:\n",
    "        tokens = tokens[:expected_token_count]\n",
    "        logger.warning(\"Too many tokens. Trimming the extra tokens.\")\n",
    "    \n",
    "    result = {}\n",
    "    idx = 0\n",
    "    for header in expected_headers:\n",
    "        if header == \"GELS (10s/10m/30m)\":\n",
    "            gels_tokens = tokens[idx:idx+3]\n",
    "            result[header] = {\n",
    "                \"10s\": gels_tokens[0],\n",
    "                \"10m\": gels_tokens[1],\n",
    "                \"30m\": gels_tokens[2]\n",
    "            }\n",
    "            idx += 3\n",
    "        else:\n",
    "            result[header] = tokens[idx]\n",
    "            idx += 1\n",
    "    logger.info(f\"Mapped dictionary: {result}\")\n",
    "    return result\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# build_mud_dict_from_rois\n",
    "# ---------------------------------------------------------------------\n",
    "def build_mud_dict_from_rois(roi_texts, expected_headers):\n",
    "    \"\"\"\n",
    "    Group OCR results into rows based on the y coordinate.\n",
    "    Identify header rows and corresponding data rows.\n",
    "    \n",
    "    In our case, we expect:\n",
    "      - A header row (with labels) followed by a data row,\n",
    "      - Then a second header row (for the remaining fields) followed by a second data row.\n",
    "    \n",
    "    We then combine the two data rows' tokens and map them to expected_headers.\n",
    "    \"\"\"\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "\n",
    "    # Group by row based on y coordinate.\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "\n",
    "    # Sort each row by x coordinate and log its text.\n",
    "    row_strings = []\n",
    "    for i, row_cells in enumerate(rows):\n",
    "        row_cells.sort(key=lambda c: c[0])\n",
    "        line_text = \" \".join(cell[4] for cell in row_cells)\n",
    "        row_strings.append(line_text)\n",
    "        logger.info(f\"Row {i} text: {line_text}\")\n",
    "\n",
    "    # Based on OCR output expectations:\n",
    "    # Row 1: header row 1 (first set of labels)\n",
    "    # Row 2: data row 1 (first set of values)\n",
    "    # Row 3: header row 2 (remaining labels)\n",
    "    # Row 4: data row 2 (remaining values)\n",
    "    header1_line = None\n",
    "    value1_line = None\n",
    "    header2_line = None\n",
    "    value2_line = None\n",
    "\n",
    "    for i, r_text in enumerate(row_strings):\n",
    "        if \"Type\" in r_text and not header1_line:\n",
    "            header1_line = r_text\n",
    "            if i + 1 < len(row_strings):\n",
    "                value1_line = row_strings[i+1]\n",
    "        elif header1_line and not header2_line and any(kw in r_text for kw in [\"RPM\", \"Mud\", \"Loss\", \"Comments\"]):\n",
    "            header2_line = r_text\n",
    "            if i + 1 < len(row_strings):\n",
    "                value2_line = row_strings[i+1]\n",
    "            break\n",
    "\n",
    "    logger.info(f\"Header1: {header1_line}\")\n",
    "    logger.info(f\"Value1: {value1_line}\")\n",
    "    logger.info(f\"Header2: {header2_line}\")\n",
    "    logger.info(f\"Value2: {value2_line}\")\n",
    "\n",
    "    if value1_line is None:\n",
    "        logger.error(\"No data row found for header1!\")\n",
    "        return {}\n",
    "\n",
    "    # Split the data rows into tokens.\n",
    "    tokens1 = value1_line.split()\n",
    "    tokens2 = value2_line.split() if value2_line else []\n",
    "    logger.info(f\"Tokens from data row 1: {tokens1}\")\n",
    "    logger.info(f\"Tokens from data row 2: {tokens2}\")\n",
    "\n",
    "    # Combine tokens from both data rows.\n",
    "    combined_tokens = tokens1 + tokens2\n",
    "    logger.info(f\"Combined tokens: {combined_tokens}\")\n",
    "\n",
    "    # Map the tokens to the expected headers.\n",
    "    return parse_value_row_tokens(expected_headers, combined_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e59c2a4f-59fe-428d-9f58-7a93077c9c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def build_survey_dict_from_rois(roi_texts, expected_headers):\n",
    "    row_strings = group_ocr_results(roi_texts)\n",
    "    logger.info(f\"SURVEY - Grouped Rows: {row_strings}\")\n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for subline in line.split(\"\\n\"):\n",
    "            subline = subline.strip()\n",
    "            if subline:\n",
    "                all_lines.append(subline)\n",
    "    logger.info(f\"SURVEY - All extracted lines: {all_lines}\")\n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"md\" in lower_tokens and \"inclination\" in lower_tokens:\n",
    "            logger.info(f\"SURVEY - Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"SURVEY - Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    logger.info(f\"SURVEY - Data lines to parse: {data_lines}\")\n",
    "    survey_list = [{expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "                   for tokens in data_lines]\n",
    "    return survey_list\n",
    "\n",
    "def sort_survey_data(survey_list):\n",
    "    def md_value(row):\n",
    "        try:\n",
    "            return float(row[\"MD\"].replace(\",\", \"\"))\n",
    "        except Exception:\n",
    "            return 0\n",
    "    return sorted(survey_list, key=md_value, reverse=True)\n",
    "\n",
    "def process_survey_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"MD\", \"Inclination\", \"Azimuth\", \"DLS\", \"TVD\"]\n",
    "    survey_list = build_survey_dict_from_rois(roi_texts, expected_headers)\n",
    "    survey_list = sort_survey_data(survey_list)\n",
    "    df = pd.DataFrame(survey_list)\n",
    "    logger.info(f\"SURVEY DataFrame shape: {df.shape}\")\n",
    "    return {\"SURVEY\": survey_list}, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f611b34-6920-4ce4-be46-0c3a62ee0e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Pumps Extraction process\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "def parse_pumps_table(ocr_text):\n",
    "    \"\"\"\n",
    "    Parses the pumps table from OCR text using a regex.\n",
    "    Expected format: Number BOMCO TRIPLEX [HHP] Efficiency Stroke(in) Liner(in) P-Rating P-Limit SPM_Rating SPM_Limit\n",
    "    \"\"\"\n",
    "    pump_pattern = re.compile(\n",
    "        r\"^(\\d+)?\\s*\"               # Number (optional)\n",
    "        r\"(BOMCO)\\s+(TRIPLEX)\\s+\"    # Model, Type\n",
    "        r\"(\\d+)?\\s*\"                # HHP (optional)\n",
    "        r\"(\\d+)\\s+\"                 # Efficiency\n",
    "        r\"([\\d.]+)\\s+\"              # Stroke\\(in\\)\n",
    "        r\"([\\d.]+)\\s+\"              # Liner\\(in\\)\n",
    "        r\"(\\d+)\\s+\"                 # P-Rating\\(psi\\)\n",
    "        r\"(\\d+)\\s+\"                 # P-Limit\\(psi\\)\n",
    "        r\"(\\d+)\\s+\"                 # SPM Rating\n",
    "        r\"(\\d+)\\s*$\",               # SPM Limit\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    pumps = []\n",
    "    lines = ocr_text.splitlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        match = pump_pattern.match(line)\n",
    "        if match:\n",
    "            number, model, pump_type, hhp, efficiency, stroke, liner, p_rating, p_limit, spm_rating, spm_limit = match.groups()\n",
    "            pumps.append({\n",
    "                \"Number\": number if number else \"\",\n",
    "                \"Model\": model,\n",
    "                \"Type\": pump_type,\n",
    "                \"HHP\": hhp if hhp else \"\",\n",
    "                \"Efficiency\": efficiency,\n",
    "                \"Stroke(in)\": stroke,\n",
    "                \"Liner(in)\": liner,\n",
    "                \"P-Rating(psi)\": p_rating,\n",
    "                \"P-Limit(psi)\": p_limit,\n",
    "                \"SPM Rating\": spm_rating,\n",
    "                \"SPM Limit\": spm_limit\n",
    "            })\n",
    "    return pumps\n",
    "\n",
    "def parse_drilling_circ_rates(ocr_text):\n",
    "    \"\"\"\n",
    "    Parses drilling/circ rates from OCR text.\n",
    "    This version splits the text into segments starting with \"Drilling/Circ Rate <n>\"\n",
    "    then combines the lines in each segment and applies a regex with DOTALL.\n",
    "    \"\"\"\n",
    "    circ_rates = []\n",
    "    \n",
    "    # Split the OCR text into segments where each segment begins with \"Drilling/Circ Rate\" followed by a digit\n",
    "    segments = re.split(r\"(?=Drilling/Circ Rate \\d+)\", ocr_text)\n",
    "    \n",
    "    # Regex pattern to capture the numbers:\n",
    "    # Group 1: Rate ID (the number after \"Drilling/Circ Rate\")\n",
    "    # Group 2: Pressure (number preceding \"PS!\" or \"PSI\")\n",
    "    # Group 3: SPM value (number after \"@\")\n",
    "    # Group 4: Gal/Stoke value\n",
    "    # Group 5: GPM value\n",
    "    # Group 6: BPM value\n",
    "    # Group 7: DC value\n",
    "    # Group 8: DP value\n",
    "    pattern = re.compile(\n",
    "        r\"Drilling/Circ Rate\\s+(\\d+).*?\"       # Rate ID\n",
    "        r\"(\\d+)\\s+PS[!I].*?\"                   # Pressure\n",
    "        r\"@\\s*(\\d+).*?\"                        # SPM value\n",
    "        r\"([\\d.]+)\\s+Gal/Stoke.*?\"              # Gal/Stoke\n",
    "        r\"([\\d.]+)\\s+GPM.*?\"                    # GPM\n",
    "        r\"([\\d.]+)\\s+BPM.*?\"                    # BPM\n",
    "        r\"([\\d.]+)\\s+DC.*?\"                     # DC\n",
    "        r\"([\\d.]+)\\s+DP\",                      # DP\n",
    "        re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    \n",
    "    # Process each segment individually\n",
    "    for seg in segments:\n",
    "        seg = seg.strip()\n",
    "        if not seg.startswith(\"Drilling/Circ Rate\"):\n",
    "            continue  # Skip any header or unrelated segments\n",
    "        # Replace newline characters with spaces to form a continuous string\n",
    "        seg_clean = \" \".join(seg.splitlines())\n",
    "        match = pattern.search(seg_clean)\n",
    "        if match:\n",
    "            rate_id, pressure, spm, gal_stroke, gpm, bpm, dc, dp = match.groups()\n",
    "            circ_rates.append({\n",
    "                \"RateID\": rate_id,\n",
    "                \"Pressure(PSI)\": pressure,\n",
    "                \"SPM\": spm,\n",
    "                \"Gal/Stoke\": gal_stroke,\n",
    "                \"GPM\": gpm,\n",
    "                \"BPM\": bpm,\n",
    "                \"DC\": dc,\n",
    "                \"DP\": dp\n",
    "            })\n",
    "        else:\n",
    "            # Optional: log a warning if no match is found for a segment\n",
    "            print(f\"Warning: No match found in segment:\\n{seg_clean}\")\n",
    "            \n",
    "    return circ_rates\n",
    "\n",
    "\n",
    "def process_pumps(pumps_img_path, debug=False):\n",
    "    \"\"\"\n",
    "    Processes the pumps section:\n",
    "    #   - Reads image using PIL,\n",
    "      - Performs OCR,\n",
    "      - Parses pumps table and drilling/circ rates,\n",
    "      - Returns combined results as JSON and a DataFrame.\n",
    "    \"\"\"\n",
    "    pil_img = read_pil_image(pumps_img_path)\n",
    "    ocr_text = perform_ocr(pil_img)\n",
    "    if debug:\n",
    "        logger.info(\"Pumps OCR Text:\\n\" + ocr_text)\n",
    "    pumps = parse_pumps_table(ocr_text)\n",
    "    circ_rates = parse_drilling_circ_rates(ocr_text)\n",
    "    final_data = {\"Pumps\": pumps, \"DrillingCircRates\": circ_rates}\n",
    "    df_pumps = pd.DataFrame(pumps)\n",
    "    df_circ = pd.DataFrame(circ_rates)\n",
    "    if not df_pumps.empty and not df_circ.empty:\n",
    "        df = pd.concat([df_pumps, df_circ], axis=0, ignore_index=True)\n",
    "    elif not df_pumps.empty:\n",
    "        df = df_pumps\n",
    "    else:\n",
    "        df = df_circ\n",
    "    return final_data, df\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # Main process Function\n",
    "# # ---------------------------------------------------------------------\n",
    "# def main():\n",
    "#     # Define image paths (adjust as needed)\n",
    "#     survey_img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_4.png\"\n",
    "#     mud_img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png\"\n",
    "#     pumps_img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_12.png\"\n",
    "    \n",
    "#     # Process Survey Section\n",
    "#     try:\n",
    "#         survey_json, survey_df = process_survey(survey_img_path, debug=True)\n",
    "#         logger.info(\"===== SURVEY DATA =====\")\n",
    "#         logger.info(json.dumps(survey_json, indent=4))\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Survey processing failed: {e}\")\n",
    "    \n",
    "#     # Process Mud Section\n",
    "#     try:\n",
    "#         mud_json, mud_df = process_mud(mud_img_path, debug=True)\n",
    "#         logger.info(\"===== MUD DATA =====\")\n",
    "#         logger.info(json.dumps(mud_json, indent=4))\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Mud processing failed: {e}\")\n",
    "    \n",
    "#     # Process Pumps Section\n",
    "#     try:\n",
    "#         pumps_json, pumps_df = process_pumps(pumps_img_path, debug=True)\n",
    "#         logger.info(\"===== PUMPS DATA =====\")\n",
    "#         logger.info(json.dumps(pumps_json, indent=4))\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Pumps processing failed: {e}\")\n",
    "    \n",
    "#     # Save outputs to disk (example output folder)\n",
    "#     output_folder = dbfs_to_local_path(\"dbfs:/mnt/mini-proj-dd/final_results\")\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "#     # Save Survey Data\n",
    "#     survey_json_path = os.path.join(output_folder, \"survey_data.json\")\n",
    "#     survey_csv_path = os.path.join(output_folder, \"survey_data.csv\")\n",
    "#     with open(survey_json_path, \"w\") as f:\n",
    "#         json.dump(survey_json, f, indent=4)\n",
    "#     survey_df.to_csv(survey_csv_path, index=False)\n",
    "#     logger.info(f\"Survey data saved to {survey_json_path} and {survey_csv_path}\")\n",
    "    \n",
    "#     # Save Mud Data\n",
    "#     mud_json_path = os.path.join(output_folder, \"mud_data.json\")\n",
    "#     mud_csv_path = os.path.join(output_folder, \"mud_data.csv\")\n",
    "#     with open(mud_json_path, \"w\") as f:\n",
    "#         json.dump(mud_json, f, indent=4)\n",
    "#     mud_df.to_csv(mud_csv_path, index=False)\n",
    "#     logger.info(f\"Mud data saved to {mud_json_path} and {mud_csv_path}\")\n",
    "    \n",
    "#     # Save Pumps Data\n",
    "#     pumps_json_path = os.path.join(output_folder, \"pumps_data.json\")\n",
    "#     pumps_csv_path = os.path.join(output_folder, \"pumps_data.csv\")\n",
    "#     with open(pumps_json_path, \"w\") as f:\n",
    "#         json.dump(pumps_json, f, indent=4)\n",
    "#     pumps_df.to_csv(pumps_csv_path, index=False)\n",
    "#     logger.info(f\"Pumps data saved to {pumps_json_path} and {pumps_csv_path}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d1cfd68-84c5-4f5e-a498-6783368e05fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Running process: cost_data\nINFO:UnifiedExtractor:Running process: cost_data\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png with shape (105, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png with shape (105, 2502, 3)\nINFO: Cost OCR extraction complete.\nINFO:UnifiedExtractor:Cost OCR extraction complete.\nINFO: COST DataFrame shape: (6, 2)\nINFO:UnifiedExtractor:COST DataFrame shape: (6, 2)\nINFO: cost_data JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/cost_data.json\nINFO:UnifiedExtractor:cost_data JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/cost_data.json\nINFO: cost_data CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/cost_data.csv\nINFO:UnifiedExtractor:cost_data CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/cost_data.csv\nINFO: cost_data output JSON:\n{\n    \"COST DATA\": {\n        \"Drilling AFE Amount\": null,\n        \"Daily Drilling Cost\": \"$167,006.63\",\n        \"Cumulative Drilling Cost\": \"$1,747,745\",\n        \"Cumulative Well Cost\": \"$1,914,752\",\n        \"Daily Mud Cost\": \"$54,185.80\",\n        \"Cumulative Mud Cost\": \"$299,370.66\"\n    }\n}\nINFO:UnifiedExtractor:cost_data output JSON:\n{\n    \"COST DATA\": {\n        \"Drilling AFE Amount\": null,\n        \"Daily Drilling Cost\": \"$167,006.63\",\n        \"Cumulative Drilling Cost\": \"$1,747,745\",\n        \"Cumulative Well Cost\": \"$1,914,752\",\n        \"Daily Mud Cost\": \"$54,185.80\",\n        \"Cumulative Mud Cost\": \"$299,370.66\"\n    }\n}\nINFO: Running process: well_job\nINFO:UnifiedExtractor:Running process: well_job\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png with shape (309, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png with shape (309, 2502, 3)\nINFO: Well/Job OCR extraction complete.\nINFO:UnifiedExtractor:Well/Job OCR extraction complete.\nINFO: WELL/JOB DataFrame shape: (19, 2)\nINFO:UnifiedExtractor:WELL/JOB DataFrame shape: (19, 2)\nINFO: well_job JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/well_job_data.json\nINFO:UnifiedExtractor:well_job JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/well_job_data.json\nINFO: well_job CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/well_job_data.csv\nINFO:UnifiedExtractor:well_job CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/well_job_data.csv\nINFO: well_job output JSON:\n{\n    \"WELL/JOB INFORMATION\": {\n        \"Well Name\": \"Ross Fee 4371-31-7-15 MH\",\n        \"Job Name\": \"Drilling\",\n        \"Supervisor(s)\": \"CHAD MILLER / ED COOLEY\",\n        \"Field\": \"XBE\",\n        \"Sec/Twn/Rng\": \"31, 43N, 71W\",\n        \"Phone\": \"307-315-1908\",\n        \"AFE #\": \"240098\",\n        \"API #\": \"49-005-78911\",\n        \"Email\": \"cyclone39@aec-denver.com\",\n        \"Contractor\": \"\",\n        \"Elevation\": \"4913.5\",\n        \"RKB\": \"27.5\",\n        \"Spud Date\": \"6/4/2024\",\n        \"Days from Spud\": \"7.67\",\n        \"Days on Loc\": \"34\",\n        \"MD/TVD\": \"20537 FT/10719 FT\",\n        \"24 Hr Footage\": \"3068\",\n        \"Present Operations\": \"DRILLING LATERAL @ 20,537'.\",\n        \"Activity Planned\": \"DRILL LATERAL SECTION TO PLANNED TD @ ~21,226', PUMP TD SWEEPS & CHC, SOOH & L/D DRILL PIPE.\"\n    }\n}\nINFO:UnifiedExtractor:well_job output JSON:\n{\n    \"WELL/JOB INFORMATION\": {\n        \"Well Name\": \"Ross Fee 4371-31-7-15 MH\",\n        \"Job Name\": \"Drilling\",\n        \"Supervisor(s)\": \"CHAD MILLER / ED COOLEY\",\n        \"Field\": \"XBE\",\n        \"Sec/Twn/Rng\": \"31, 43N, 71W\",\n        \"Phone\": \"307-315-1908\",\n        \"AFE #\": \"240098\",\n        \"API #\": \"49-005-78911\",\n        \"Email\": \"cyclone39@aec-denver.com\",\n        \"Contractor\": \"\",\n        \"Elevation\": \"4913.5\",\n        \"RKB\": \"27.5\",\n        \"Spud Date\": \"6/4/2024\",\n        \"Days from Spud\": \"7.67\",\n        \"Days on Loc\": \"34\",\n        \"MD/TVD\": \"20537 FT/10719 FT\",\n        \"24 Hr Footage\": \"3068\",\n        \"Present Operations\": \"DRILLING LATERAL @ 20,537'.\",\n        \"Activity Planned\": \"DRILL LATERAL SECTION TO PLANNED TD @ ~21,226', PUMP TD SWEEPS & CHC, SOOH & L/D DRILL PIPE.\"\n    }\n}\nINFO: Running process: obs_int\nINFO:UnifiedExtractor:Running process: obs_int\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png with shape (241, 942, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png with shape (241, 942, 3)\nINFO: DAILY NUMBERS: OBSERVATION & INTERVENTION DataFrame shape: (5, 2)\nINFO:UnifiedExtractor:DAILY NUMBERS: OBSERVATION & INTERVENTION DataFrame shape: (5, 2)\nINFO: obs_int JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/obs_int_data.json\nINFO:UnifiedExtractor:obs_int JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/obs_int_data.json\nINFO: obs_int CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/obs_int_data.csv\nINFO:UnifiedExtractor:obs_int CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/obs_int_data.csv\nINFO: obs_int output JSON:\n{\n    \"DAILY NUMBERS: OBSERVATION & INTERVENTION\": [\n        {\n            \"Type\": \"Stop Cards\",\n            \"Number\": \"14\"\n        },\n        {\n            \"Type\": \"Hazard ID's\",\n            \"Number\": \"2\"\n        },\n        {\n            \"Type\": \"JSA's\",\n            \"Number\": \"5\"\n        },\n        {\n            \"Type\": \"Permit to Work\",\n            \"Number\": \"21\"\n        },\n        {\n            \"Type\": \"Totals\",\n            \"Number\": \"\"\n        }\n    ]\n}\nINFO:UnifiedExtractor:obs_int output JSON:\n{\n    \"DAILY NUMBERS: OBSERVATION & INTERVENTION\": [\n        {\n            \"Type\": \"Stop Cards\",\n            \"Number\": \"14\"\n        },\n        {\n            \"Type\": \"Hazard ID's\",\n            \"Number\": \"2\"\n        },\n        {\n            \"Type\": \"JSA's\",\n            \"Number\": \"5\"\n        },\n        {\n            \"Type\": \"Permit to Work\",\n            \"Number\": \"21\"\n        },\n        {\n            \"Type\": \"Totals\",\n            \"Number\": \"\"\n        }\n    ]\n}\nINFO: Running process: bop\nINFO:UnifiedExtractor:Running process: bop\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png with shape (71, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png with shape (71, 2502, 3)\nINFO: BOP OCR extraction complete.\nINFO:UnifiedExtractor:BOP OCR extraction complete.\nINFO: BOP DataFrame shape: (3, 2)\nINFO:UnifiedExtractor:BOP DataFrame shape: (3, 2)\nINFO: bop JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/bop_data.json\nINFO:UnifiedExtractor:bop JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/bop_data.json\nINFO: bop CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/bop_data.csv\nINFO:UnifiedExtractor:bop CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/bop_data.csv\nINFO: bop output JSON:\n{\n    \"BOP\": {\n        \"Last BOP Test Date\": \"6/30/24\",\n        \"Last BOP Drill\": \"7/3/2024\",\n        \"Next BOP Test\": \"7/25/24\"\n    }\n}\nINFO:UnifiedExtractor:bop output JSON:\n{\n    \"BOP\": {\n        \"Last BOP Test Date\": \"6/30/24\",\n        \"Last BOP Drill\": \"7/3/2024\",\n        \"Next BOP Test\": \"7/25/24\"\n    }\n}\nINFO: Running process: dir_info\nINFO:UnifiedExtractor:Running process: dir_info\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png with shape (241, 1200, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png with shape (241, 1200, 3)\nINFO: DIR INFO DataFrame shape: (5, 3)\nINFO:UnifiedExtractor:DIR INFO DataFrame shape: (5, 3)\nINFO: dir_info JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/dir_info_data.json\nINFO:UnifiedExtractor:dir_info JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/dir_info_data.json\nINFO: dir_info CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/dir_info_data.csv\nINFO:UnifiedExtractor:dir_info CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/dir_info_data.csv\nINFO: dir_info output JSON:\n{\n    \"DIR INFO\": [\n        {\n            \"Category\": \"Circ/Cond Hours\",\n            \"Daily\": \"\",\n            \"Cumulative\": \"6.8\"\n        },\n        {\n            \"Category\": \"Sliding Hours\",\n            \"Daily\": \"5.8\",\n            \"Cumulative\": \"28.4\"\n        },\n        {\n            \"Category\": \"Sliding Footage\",\n            \"Daily\": \"247\",\n            \"Cumulative\": \"1488\"\n        },\n        {\n            \"Category\": \"Rotating Hours\",\n            \"Daily\": \"17.8\",\n            \"Cumulative\": \"75.9\"\n        },\n        {\n            \"Category\": \"Rotating Footage\",\n            \"Daily\": \"2821\",\n            \"Cumulative\": \"18941\"\n        }\n    ]\n}\nINFO:UnifiedExtractor:dir_info output JSON:\n{\n    \"DIR INFO\": [\n        {\n            \"Category\": \"Circ/Cond Hours\",\n            \"Daily\": \"\",\n            \"Cumulative\": \"6.8\"\n        },\n        {\n            \"Category\": \"Sliding Hours\",\n            \"Daily\": \"5.8\",\n            \"Cumulative\": \"28.4\"\n        },\n        {\n            \"Category\": \"Sliding Footage\",\n            \"Daily\": \"247\",\n            \"Cumulative\": \"1488\"\n        },\n        {\n            \"Category\": \"Rotating Hours\",\n            \"Daily\": \"17.8\",\n            \"Cumulative\": \"75.9\"\n        },\n        {\n            \"Category\": \"Rotating Footage\",\n            \"Daily\": \"2821\",\n            \"Cumulative\": \"18941\"\n        }\n    ]\n}\nINFO: Running process: survey\nINFO:UnifiedExtractor:Running process: survey\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png with shape (105, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png with shape (105, 2502, 3)\nINFO: SURVEY - Grouped Rows: ['COST DATA COST DATA [BLANK]', 'Drilling AFE Amount: Daily Drilling Cost: $167,006.63 Cumulative Drilling Cost: $1,747,745 Cumulative Well Cost: $1,914,752\\nDaily Mud Cost: $54,185.80 Cumulative Mud Cost: $299,370.66']\nINFO:UnifiedExtractor:SURVEY - Grouped Rows: ['COST DATA COST DATA [BLANK]', 'Drilling AFE Amount: Daily Drilling Cost: $167,006.63 Cumulative Drilling Cost: $1,747,745 Cumulative Well Cost: $1,914,752\\nDaily Mud Cost: $54,185.80 Cumulative Mud Cost: $299,370.66']\nINFO: SURVEY - All extracted lines: ['COST DATA COST DATA [BLANK]', 'Drilling AFE Amount: Daily Drilling Cost: $167,006.63 Cumulative Drilling Cost: $1,747,745 Cumulative Well Cost: $1,914,752', 'Daily Mud Cost: $54,185.80 Cumulative Mud Cost: $299,370.66']\nINFO:UnifiedExtractor:SURVEY - All extracted lines: ['COST DATA COST DATA [BLANK]', 'Drilling AFE Amount: Daily Drilling Cost: $167,006.63 Cumulative Drilling Cost: $1,747,745 Cumulative Well Cost: $1,914,752', 'Daily Mud Cost: $54,185.80 Cumulative Mud Cost: $299,370.66']\nINFO: SURVEY - Data lines to parse: [['COST', 'DATA', 'COST', 'DATA', '[BLANK]'], ['Drilling', 'AFE', 'Amount:', 'Daily', 'Drilling'], ['Daily', 'Mud', 'Cost:', '$54,185.80', 'Cumulative']]\nINFO:UnifiedExtractor:SURVEY - Data lines to parse: [['COST', 'DATA', 'COST', 'DATA', '[BLANK]'], ['Drilling', 'AFE', 'Amount:', 'Daily', 'Drilling'], ['Daily', 'Mud', 'Cost:', '$54,185.80', 'Cumulative']]\nINFO: SURVEY DataFrame shape: (3, 5)\nINFO:UnifiedExtractor:SURVEY DataFrame shape: (3, 5)\nINFO: survey JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/survey_data.json\nINFO:UnifiedExtractor:survey JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/survey_data.json\nINFO: survey CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/survey_data.csv\nINFO:UnifiedExtractor:survey CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/survey_data.csv\nINFO: survey output JSON:\n{\n    \"SURVEY\": [\n        {\n            \"MD\": \"COST\",\n            \"Inclination\": \"DATA\",\n            \"Azimuth\": \"COST\",\n            \"DLS\": \"DATA\",\n            \"TVD\": \"[BLANK]\"\n        },\n        {\n            \"MD\": \"Drilling\",\n            \"Inclination\": \"AFE\",\n            \"Azimuth\": \"Amount:\",\n            \"DLS\": \"Daily\",\n            \"TVD\": \"Drilling\"\n        },\n        {\n            \"MD\": \"Daily\",\n            \"Inclination\": \"Mud\",\n            \"Azimuth\": \"Cost:\",\n            \"DLS\": \"$54,185.80\",\n            \"TVD\": \"Cumulative\"\n        }\n    ]\n}\nINFO:UnifiedExtractor:survey output JSON:\n{\n    \"SURVEY\": [\n        {\n            \"MD\": \"COST\",\n            \"Inclination\": \"DATA\",\n            \"Azimuth\": \"COST\",\n            \"DLS\": \"DATA\",\n            \"TVD\": \"[BLANK]\"\n        },\n        {\n            \"MD\": \"Drilling\",\n            \"Inclination\": \"AFE\",\n            \"Azimuth\": \"Amount:\",\n            \"DLS\": \"Daily\",\n            \"TVD\": \"Drilling\"\n        },\n        {\n            \"MD\": \"Daily\",\n            \"Inclination\": \"Mud\",\n            \"Azimuth\": \"Cost:\",\n            \"DLS\": \"$54,185.80\",\n            \"TVD\": \"Cumulative\"\n        }\n    ]\n}\nINFO: Running process: casing\nINFO:UnifiedExtractor:Running process: casing\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png with shape (241, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png with shape (241, 2502, 3)\nWARNING: CASING - Line has fewer tokens than expected: ['CASING', 'CASING', '[BLANK]']\nWARNING:UnifiedExtractor:CASING - Line has fewer tokens than expected: ['CASING', 'CASING', '[BLANK]']\nINFO: CASING - Skipping header line: ['Type', 'Size', 'Weight', 'Grade', 'Connection', 'Top', 'MD', 'Bottom', 'MD', 'TOC']\nINFO:UnifiedExtractor:CASING - Skipping header line: ['Type', 'Size', 'Weight', 'Grade', 'Connection', 'Top', 'MD', 'Bottom', 'MD', 'TOC']\nWARNING: CASING - Line has fewer tokens than expected: ['OG']\nWARNING:UnifiedExtractor:CASING - Line has fewer tokens than expected: ['OG']\nINFO: CASING DataFrame shape: (4, 8)\nINFO:UnifiedExtractor:CASING DataFrame shape: (4, 8)\nINFO: casing JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/casing_data.json\nINFO:UnifiedExtractor:casing JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/casing_data.json\nINFO: casing CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/casing_data.csv\nINFO:UnifiedExtractor:casing CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/casing_data.csv\nINFO: casing output JSON:\n{\n    \"CASING\": [\n        {\n            \"Type\": \"Conductor\",\n            \"Size\": \"16.000\",\n            \"Weight\": \"36.94\",\n            \"Grade\": \"A252\",\n            \"Connection\": \"WELDED\",\n            \"Top MD\": \"32.00\",\n            \"Bottom MD\": \"108.00\",\n            \"TOC\": \"16\"\n        },\n        {\n            \"Type\": \"Surface\",\n            \"Size\": \"10.750\",\n            \"Weight\": \"40.5\",\n            \"Grade\": \"J55\",\n            \"Connection\": \"BTC\",\n            \"Top MD\": \"31.17\",\n            \"Bottom MD\": \"2268.00\",\n            \"TOC\": \"30\"\n        },\n        {\n            \"Type\": \"Intermediate\",\n            \"Size\": \"7.625\",\n            \"Weight\": \"29.7\",\n            \"Grade\": \"HCP110\",\n            \"Connection\": \"BTC\",\n            \"Top MD\": \"28.89\",\n            \"Bottom MD\": \"9857.70\",\n            \"TOC\": \"2750\"\n        },\n        {\n            \"Type\": \"[BLANK]\",\n            \"Size\": \"[BLANK]\",\n            \"Weight\": \"[BLANK]\",\n            \"Grade\": \"[BLANK]\",\n            \"Connection\": \"[BLANK]\",\n            \"Top MD\": \"[BLANK]\",\n            \"Bottom MD\": \"[BLANK]\",\n            \"TOC\": \"[BLANK]\"\n        }\n    ]\n}\nINFO:UnifiedExtractor:casing output JSON:\n{\n    \"CASING\": [\n        {\n            \"Type\": \"Conductor\",\n            \"Size\": \"16.000\",\n            \"Weight\": \"36.94\",\n            \"Grade\": \"A252\",\n            \"Connection\": \"WELDED\",\n            \"Top MD\": \"32.00\",\n            \"Bottom MD\": \"108.00\",\n            \"TOC\": \"16\"\n        },\n        {\n            \"Type\": \"Surface\",\n            \"Size\": \"10.750\",\n            \"Weight\": \"40.5\",\n            \"Grade\": \"J55\",\n            \"Connection\": \"BTC\",\n            \"Top MD\": \"31.17\",\n            \"Bottom MD\": \"2268.00\",\n            \"TOC\": \"30\"\n        },\n        {\n            \"Type\": \"Intermediate\",\n            \"Size\": \"7.625\",\n            \"Weight\": \"29.7\",\n            \"Grade\": \"HCP110\",\n            \"Connection\": \"BTC\",\n            \"Top MD\": \"28.89\",\n            \"Bottom MD\": \"9857.70\",\n            \"TOC\": \"2750\"\n        },\n        {\n            \"Type\": \"[BLANK]\",\n            \"Size\": \"[BLANK]\",\n            \"Weight\": \"[BLANK]\",\n            \"Grade\": \"[BLANK]\",\n            \"Connection\": \"[BLANK]\",\n            \"Top MD\": \"[BLANK]\",\n            \"Bottom MD\": \"[BLANK]\",\n            \"TOC\": \"[BLANK]\"\n        }\n    ]\n}\nINFO: Running process: consumables\nINFO:UnifiedExtractor:Running process: consumables\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png with shape (209, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png with shape (209, 2502, 3)\nINFO: CONSUMABLES DataFrame shape: (3, 5)\nINFO:UnifiedExtractor:CONSUMABLES DataFrame shape: (3, 5)\nINFO: consumables JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/consumables_data.json\nINFO:UnifiedExtractor:consumables JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/consumables_data.json\nINFO: consumables CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/consumables_data.csv\nINFO:UnifiedExtractor:consumables CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/consumables_data.csv\nINFO: consumables output JSON:\n{\n    \"CONSUMABLES\": [\n        {\n            \"Consumable\": \"Fuel\",\n            \"Daily Received (gal)\": \"[BLANK]\",\n            \"Daily Used (gal)\": \"1,386\",\n            \"Cumulative Used (gal)\": \"20,626\",\n            \"Daily on Hand (gal)\": \"5,735\"\n        },\n        {\n            \"Consumable\": \"CNG (DGE)\",\n            \"Daily Received (gal)\": \"1,652\",\n            \"Daily Used (gal)\": \"1,652\",\n            \"Cumulative Used (gal)\": \"6,535\",\n            \"Daily on Hand (gal)\": \"[BLANK]\"\n        },\n        {\n            \"Consumable\": \"Mud Fuel\",\n            \"Daily Received (gal)\": \"8,367\",\n            \"Daily Used (gal)\": \"[BLANK]\",\n            \"Cumulative Used (gal)\": \"24,150\",\n            \"Daily on Hand (gal)\": \"11,643\"\n        }\n    ]\n}\nINFO:UnifiedExtractor:consumables output JSON:\n{\n    \"CONSUMABLES\": [\n        {\n            \"Consumable\": \"Fuel\",\n            \"Daily Received (gal)\": \"[BLANK]\",\n            \"Daily Used (gal)\": \"1,386\",\n            \"Cumulative Used (gal)\": \"20,626\",\n            \"Daily on Hand (gal)\": \"5,735\"\n        },\n        {\n            \"Consumable\": \"CNG (DGE)\",\n            \"Daily Received (gal)\": \"1,652\",\n            \"Daily Used (gal)\": \"1,652\",\n            \"Cumulative Used (gal)\": \"6,535\",\n            \"Daily on Hand (gal)\": \"[BLANK]\"\n        },\n        {\n            \"Consumable\": \"Mud Fuel\",\n            \"Daily Received (gal)\": \"8,367\",\n            \"Daily Used (gal)\": \"[BLANK]\",\n            \"Cumulative Used (gal)\": \"24,150\",\n            \"Daily on Hand (gal)\": \"11,643\"\n        }\n    ]\n}\nINFO: Running process: mud\nINFO:UnifiedExtractor:Running process: mud\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png with shape (173, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png with shape (173, 2502, 3)\nINFO: Row 0 text: MUD MUD [BLANK]\nINFO:UnifiedExtractor:Row 0 text: MUD MUD [BLANK]\nINFO: Row 1 text: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO:UnifiedExtractor:Row 1 text: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO: Row 2 text: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO:UnifiedExtractor:Row 2 text: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO: Row 3 text: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO:UnifiedExtractor:Row 3 text: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO: Row 4 text: 4 5 1023 13 481 [BLANK]\nINFO:UnifiedExtractor:Row 4 text: 4 5 1023 13 481 [BLANK]\nINFO: Header1: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO:UnifiedExtractor:Header1: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO: Value1: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO:UnifiedExtractor:Value1: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO: Header2: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO:UnifiedExtractor:Header2: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO: Value2: 4 5 1023 13 481 [BLANK]\nINFO:UnifiedExtractor:Value2: 4 5 1023 13 481 [BLANK]\nINFO: Tokens from data row 1: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00']\nINFO:UnifiedExtractor:Tokens from data row 1: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00']\nINFO: Tokens from data row 2: ['4', '5', '1023', '13', '481', '[BLANK]']\nINFO:UnifiedExtractor:Tokens from data row 2: ['4', '5', '1023', '13', '481', '[BLANK]']\nINFO: Combined tokens: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO:UnifiedExtractor:Combined tokens: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO: Expected token count: 24, tokens extracted: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO:UnifiedExtractor:Expected token count: 24, tokens extracted: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO: Mapped dictionary: {'Type': 'OBM', 'Weight In': '11.5', 'Weight Out': '11.5', 'pH': '[BLANK]', 'CAKE': '3', 'GELS (10s/10m/30m)': {'10s': '8', '10m': '25', '30m': '27'}, 'Oil/Water': '88/12', 'FV': '60.0', 'ES': '753', 'PV': '16', 'YP': '8', 'CL': '31,000', 'Ca': '326,667', 'LGS': '4.47', 'WL': '[BLANK]', 'HTHP Loss': '5.00', '3 RPM': '4', '6 RPM': '5', 'Mud Pits and Hole Volume': '1023', '24 Hr Loss': '13', 'Total Loss': '481', 'Comments': '[BLANK]'}\nINFO:UnifiedExtractor:Mapped dictionary: {'Type': 'OBM', 'Weight In': '11.5', 'Weight Out': '11.5', 'pH': '[BLANK]', 'CAKE': '3', 'GELS (10s/10m/30m)': {'10s': '8', '10m': '25', '30m': '27'}, 'Oil/Water': '88/12', 'FV': '60.0', 'ES': '753', 'PV': '16', 'YP': '8', 'CL': '31,000', 'Ca': '326,667', 'LGS': '4.47', 'WL': '[BLANK]', 'HTHP Loss': '5.00', '3 RPM': '4', '6 RPM': '5', 'Mud Pits and Hole Volume': '1023', '24 Hr Loss': '13', 'Total Loss': '481', 'Comments': '[BLANK]'}\nINFO: mud JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/mud_data.json\nINFO:UnifiedExtractor:mud JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/mud_data.json\nINFO: mud CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/mud_data.csv\nINFO:UnifiedExtractor:mud CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/mud_data.csv\nINFO: mud output JSON:\n{\n    \"MUD\": {\n        \"Type\": \"OBM\",\n        \"Weight In\": \"11.5\",\n        \"Weight Out\": \"11.5\",\n        \"pH\": \"[BLANK]\",\n        \"CAKE\": \"3\",\n        \"GELS (10s/10m/30m)\": {\n            \"10s\": \"8\",\n            \"10m\": \"25\",\n            \"30m\": \"27\"\n        },\n        \"Oil/Water\": \"88/12\",\n        \"FV\": \"60.0\",\n        \"ES\": \"753\",\n        \"PV\": \"16\",\n        \"YP\": \"8\",\n        \"CL\": \"31,000\",\n        \"Ca\": \"326,667\",\n        \"LGS\": \"4.47\",\n        \"WL\": \"[BLANK]\",\n        \"HTHP Loss\": \"5.00\",\n        \"3 RPM\": \"4\",\n        \"6 RPM\": \"5\",\n        \"Mud Pits and Hole Volume\": \"1023\",\n        \"24 Hr Loss\": \"13\",\n        \"Total Loss\": \"481\",\n        \"Comments\": \"[BLANK]\"\n    }\n}\nINFO:UnifiedExtractor:mud output JSON:\n{\n    \"MUD\": {\n        \"Type\": \"OBM\",\n        \"Weight In\": \"11.5\",\n        \"Weight Out\": \"11.5\",\n        \"pH\": \"[BLANK]\",\n        \"CAKE\": \"3\",\n        \"GELS (10s/10m/30m)\": {\n            \"10s\": \"8\",\n            \"10m\": \"25\",\n            \"30m\": \"27\"\n        },\n        \"Oil/Water\": \"88/12\",\n        \"FV\": \"60.0\",\n        \"ES\": \"753\",\n        \"PV\": \"16\",\n        \"YP\": \"8\",\n        \"CL\": \"31,000\",\n        \"Ca\": \"326,667\",\n        \"LGS\": \"4.47\",\n        \"WL\": \"[BLANK]\",\n        \"HTHP Loss\": \"5.00\",\n        \"3 RPM\": \"4\",\n        \"6 RPM\": \"5\",\n        \"Mud Pits and Hole Volume\": \"1023\",\n        \"24 Hr Loss\": \"13\",\n        \"Total Loss\": \"481\",\n        \"Comments\": \"[BLANK]\"\n    }\n}\nINFO: Running process: bha\nINFO:UnifiedExtractor:Running process: bha\nERROR: Error in process 'bha': [Errno 2] No such file or directory: '/Workspace/Repos/divya.dhaipullay@zeussolutionsinc.com/automate_ddr/dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_11.png'\nERROR:UnifiedExtractor:Error in process 'bha': [Errno 2] No such file or directory: '/Workspace/Repos/divya.dhaipullay@zeussolutionsinc.com/automate_ddr/dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_11.png'\nINFO: Running process: pumps\nINFO:UnifiedExtractor:Running process: pumps\nERROR: Error in process 'pumps': name 'read_pil_image' is not defined\nERROR:UnifiedExtractor:Error in process 'pumps': name 'read_pil_image' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_process(name, pipe, debug=False):\n",
    "    \"\"\"Runs an individual process and logs/saves its textual outputs.\"\"\"\n",
    "    logger.info(f\"Running process: {name}\")\n",
    "    try:\n",
    "        # Execute the process function and get JSON and DataFrame outputs.\n",
    "        output_json, output_df = pipe[\"func\"](None)\n",
    "        \n",
    "        # Define output folder and ensure sequential processing\n",
    "        output_folder = dbfs_to_local_path(\"dbfs:/mnt/mini-proj-dd/final_results\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Save JSON output\n",
    "        json_path = os.path.join(output_folder, pipe[\"json\"])\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(output_json, f, indent=4)\n",
    "        logger.info(f\"{name} JSON data saved to {json_path}\")\n",
    "        \n",
    "        # Save CSV output\n",
    "        csv_path = os.path.join(output_folder, pipe[\"csv\"])\n",
    "        output_df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"{name} CSV data saved to {csv_path}\")\n",
    "        \n",
    "        # Log textual output\n",
    "        logger.info(f\"{name} output JSON:\\n{json.dumps(output_json, indent=4)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process '{name}': {e}\")\n",
    "\n",
    "def process_pumps_data(dummy_arg=None):\n",
    "    pumps_img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_12.png\"\n",
    "    return process_pumps(pumps_img_path, debug=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main Function (Merged with processs Dictionary)\n",
    "# -----------------------------------------------------------------------------\n",
    "def main():\n",
    "    debug = False  # Set True for detailed logging\n",
    "    processs = {\n",
    "        \"cost_data\": {\n",
    "            \"func\": process_cost_data,\n",
    "            \"csv\": \"cost_data.csv\",\n",
    "            \"json\": \"cost_data.json\"\n",
    "        },\n",
    "        \"well_job\": {\n",
    "            \"func\": lambda d: process_well_job_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\", d),\n",
    "            \"csv\": \"well_job_data.csv\",\n",
    "            \"json\": \"well_job_data.json\"\n",
    "        },\n",
    "        \"obs_int\": {\n",
    "            \"func\": lambda d: process_obs_int(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\", d),\n",
    "            \"csv\": \"obs_int_data.csv\",\n",
    "            \"json\": \"obs_int_data.json\"\n",
    "        },\n",
    "        \"bop\": {\n",
    "            \"func\": lambda d: process_bop(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\", d),\n",
    "            \"csv\": \"bop_data.csv\",\n",
    "            \"json\": \"bop_data.json\"\n",
    "        },\n",
    "        \"dir_info\": {\n",
    "            \"func\": lambda d: process_dir_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\", d),\n",
    "            \"csv\": \"dir_info_data.csv\",\n",
    "            \"json\": \"dir_info_data.json\"\n",
    "        },\n",
    "        \"survey\": {\n",
    "            \"func\": process_survey_data,\n",
    "            \"csv\": \"survey_data.csv\",\n",
    "            \"json\": \"survey_data.json\"\n",
    "        },\n",
    "        \"casing\": {\n",
    "            \"func\": process_casing_data,\n",
    "            \"csv\": \"casing_data.csv\",\n",
    "            \"json\": \"casing_data.json\"\n",
    "        },\n",
    "        \"consumables\": {\n",
    "            \"func\": process_consumables_data,\n",
    "            \"csv\": \"consumables_data.csv\",\n",
    "            \"json\": \"consumables_data.json\"\n",
    "        },\n",
    "        \"mud\": {\n",
    "            \"func\": process_mud_data,\n",
    "            \"csv\": \"mud_data.csv\",\n",
    "            \"json\": \"mud_data.json\"\n",
    "        },\n",
    "        \"bha\": {\n",
    "            \"func\": process_bha_data,\n",
    "            \"csv\": \"bha_data.csv\",\n",
    "            \"json\": \"bha_data.json\"\n",
    "        },\n",
    "        \"pumps\": {\n",
    "            \"func\": process_pumps_data,\n",
    "            \"csv\": \"pumps_data.csv\",\n",
    "            \"json\": \"pumps_data.json\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Process processs sequentially in the order defined in the dictionary.\n",
    "    for name, pipe in processs.items():\n",
    "        run_process(name, pipe, debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba78492e-ce7c-4f7b-bcd9-70f117c27e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------------\n",
    "# 5) Personnel Extraction process\n",
    "# --------------------------------------------------------\n",
    "def detect_text_regions_personnel(thresh_img, debug=False):\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois = []\n",
    "    debug_img = cv2.cvtColor(thresh_img, cv2.COLOR_GRAY2BGR)\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "            cv2.rectangle(debug_img, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    rois.sort(key=lambda b: (b[1], b[0]))\n",
    "    logger.debug(f\"Detected {len(rois)} text regions for personnel.\")\n",
    "    if debug:\n",
    "        show_image(\"Detected Personnel Text Regions\", debug_img, size=(12, 12))\n",
    "    return rois\n",
    "\n",
    "def perform_ocr_on_rois_personnel(img, rois, debug=False):\n",
    "    results = []\n",
    "    for (x, y, w, h) in rois:\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        text = pytesseract.image_to_string(roi, config='--psm 6').strip()\n",
    "        if not text:\n",
    "            text = \"[BLANK]\"\n",
    "        results.append((x, y, w, h, text))\n",
    "        logger.debug(f\"ROI bbox=({x}, {y}, {w}, {h}), text: '{text}'\")\n",
    "    return results\n",
    "\n",
    "def group_rois_by_row(roi_results, threshold=20):\n",
    "    roi_with_center = [(x, y, w, h, text, y + h/2) for (x, y, w, h, text) in roi_results]\n",
    "    roi_with_center.sort(key=lambda r: r[5])\n",
    "    groups = []\n",
    "    current_group = []\n",
    "    current_center = None\n",
    "    for roi in roi_with_center:\n",
    "        x, y, w, h, text, y_center = roi\n",
    "        if current_center is None:\n",
    "            current_center = y_center\n",
    "            current_group.append((x, y, w, h, text))\n",
    "        elif abs(y_center - current_center) < threshold:\n",
    "            current_group.append((x, y, w, h, text))\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [(x, y, w, h, text)]\n",
    "            current_center = y_center\n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "    logger.debug(f\"Grouped ROIs into {len(groups)} rows.\")\n",
    "    return groups\n",
    "\n",
    "def preprocess_personnel_data_from_rows(groups):\n",
    "    personnel_data = []\n",
    "    header_lines = {\n",
    "        \"personnel\", \n",
    "        \"company contractor no. personnel daily hours cumulative hours\",\n",
    "        \"ssn\"\n",
    "    }\n",
    "    for group in groups:\n",
    "        group.sort(key=lambda r: r[0])\n",
    "        row_text = \" \".join([r[4] for r in group]).strip()\n",
    "        logger.debug(f\"Processing row: '{row_text}'\")\n",
    "        if row_text.lower() in header_lines:\n",
    "            logger.debug(\"Skipping header row.\")\n",
    "            continue\n",
    "        tokens = row_text.split()\n",
    "        numeric_tokens = re.findall(r'\\d+(?:\\.\\d+)?', row_text)\n",
    "        logger.debug(f\"Row tokens: {tokens}\")\n",
    "        logger.debug(f\"Numeric tokens found: {numeric_tokens}\")\n",
    "        if tokens[0].lower().startswith(\"totals\"):\n",
    "            if len(numeric_tokens) >= 2:\n",
    "                try:\n",
    "                    daily_hours = int(float(numeric_tokens[0]))\n",
    "                    cumulative_hours = numeric_tokens[1]\n",
    "                except ValueError as e:\n",
    "                    logger.error(f\"Error parsing Totals row: {row_text} => {e}\")\n",
    "                    continue\n",
    "                row_dict = {\n",
    "                    \"Company\": \"\",\n",
    "                    \"Contractor\": \"\",\n",
    "                    \"No. Personnel\": \"Totals\",\n",
    "                    \"Daily Hours\": daily_hours,\n",
    "                    \"Cumulative Hours\": cumulative_hours\n",
    "                }\n",
    "                logger.info(f\"Totals row parsed: {row_dict}\")\n",
    "                personnel_data.append(row_dict)\n",
    "            else:\n",
    "                logger.warning(f\"Totals row without sufficient numbers: {row_text}\")\n",
    "            continue\n",
    "        if len(numeric_tokens) >= 3:\n",
    "            try:\n",
    "                no_personnel = int(float(numeric_tokens[-3]))\n",
    "                daily_hours = int(float(numeric_tokens[-2]))\n",
    "                cumulative_hours = int(float(numeric_tokens[-1]))\n",
    "                logger.debug(f\"Extracted: no_personnel={no_personnel}, daily_hours={daily_hours}, cumulative_hours={cumulative_hours}\")\n",
    "            except ValueError as e:\n",
    "                logger.error(f\"Error converting numbers in row: {row_text} => {e}\")\n",
    "                continue\n",
    "            pattern = (r'\\s*' + re.escape(numeric_tokens[-3]) +\n",
    "                       r'\\s+' + re.escape(numeric_tokens[-2]) +\n",
    "                       r'\\s+' + re.escape(numeric_tokens[-1]) + r'\\s*$')\n",
    "            text_only = re.sub(pattern, '', row_text).strip()\n",
    "        elif len(numeric_tokens) == 1:\n",
    "            try:\n",
    "                cumulative_hours = int(float(numeric_tokens[0]))\n",
    "                logger.debug(f\"Single numeric token, cumulative_hours: {cumulative_hours}\")\n",
    "            except ValueError as e:\n",
    "                logger.error(f\"Error converting single number in row: {row_text} => {e}\")\n",
    "                continue\n",
    "            no_personnel = None\n",
    "            daily_hours = None\n",
    "            pattern = r'\\s*' + re.escape(numeric_tokens[0]) + r'\\s*$'\n",
    "            text_only = re.sub(pattern, '', row_text).strip()\n",
    "        else:\n",
    "            logger.warning(f\"Row has unexpected number of numeric tokens: {row_text}\")\n",
    "            continue\n",
    "\n",
    "        if \"service company\" in text_only.lower():\n",
    "            parts = re.split(r'(?i)service company', text_only, maxsplit=1)\n",
    "            company = parts[0].strip()\n",
    "            contractor = \"Service Company\"\n",
    "        else:\n",
    "            company = text_only\n",
    "            contractor = \"Service Company\"\n",
    "        \n",
    "        row_dict = {\n",
    "            \"Company\": company,\n",
    "            \"Contractor\": contractor,\n",
    "            \"No. Personnel\": no_personnel,\n",
    "            \"Daily Hours\": daily_hours,\n",
    "            \"Cumulative Hours\": cumulative_hours\n",
    "        }\n",
    "        logger.info(f\"Parsed row: {row_dict}\")\n",
    "        personnel_data.append(row_dict)\n",
    "    return {\"PERSONNEL\": personnel_data}\n",
    "\n",
    "def process_personnel(personnel_img_path, debug=False):\n",
    "    \"\"\"Processes the personnel section: read image, detect ROIs, OCR, group and parse rows.\"\"\"\n",
    "    img = read_cropped_section_image(personnel_img_path)\n",
    "    if debug:\n",
    "        show_image(\"Original Personnel Image\", img, size=(10,10))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh_img = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                       cv2.THRESH_BINARY, 11, 2)\n",
    "    if debug:\n",
    "        show_image(\"Thresholded Personnel Image\", cv2.cvtColor(thresh_img, cv2.COLOR_GRAY2BGR), size=(8,8))\n",
    "    rois = detect_text_regions_personnel(thresh_img, debug=debug)\n",
    "    roi_results = perform_ocr_on_rois_personnel(img, rois, debug=debug)\n",
    "    grouped_rows = group_rois_by_row(roi_results, threshold=20)\n",
    "    data_dict = preprocess_personnel_data_from_rows(grouped_rows)\n",
    "    df = pd.DataFrame(data_dict[\"PERSONNEL\"]) if data_dict[\"PERSONNEL\"] else pd.DataFrame(\n",
    "        columns=[\"Company\", \"Contractor\", \"No. Personnel\", \"Daily Hours\", \"Cumulative Hours\"])\n",
    "    return data_dict, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ee0ef8-6d1c-4dab-a173-909b8fb166d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Running process: cost_data\nINFO:UnifiedExtractor:Running process: cost_data\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png with shape (105, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png with shape (105, 2502, 3)\nINFO: Cost OCR extraction complete.\nINFO:UnifiedExtractor:Cost OCR extraction complete.\nINFO: COST DataFrame shape: (6, 2)\nINFO:UnifiedExtractor:COST DataFrame shape: (6, 2)\nINFO: cost_data JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/cost_data.json\nINFO:UnifiedExtractor:cost_data JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/cost_data.json\nINFO: cost_data CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/cost_data.csv\nINFO:UnifiedExtractor:cost_data CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/cost_data.csv\nINFO: Running process: well_job\nINFO:UnifiedExtractor:Running process: well_job\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png with shape (309, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png with shape (309, 2502, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- COST_DATA JSON Output ---\n{\n    \"COST DATA\": {\n        \"Drilling AFE Amount\": null,\n        \"Daily Drilling Cost\": \"$167,006.63\",\n        \"Cumulative Drilling Cost\": \"$1,747,745\",\n        \"Cumulative Well Cost\": \"$1,914,752\",\n        \"Daily Mud Cost\": \"$54,185.80\",\n        \"Cumulative Mud Cost\": \"$299,370.66\"\n    }\n}\n\n--- COST_DATA DataFrame ---\n                        Key        Value\n0       Drilling AFE Amount         None\n1       Daily Drilling Cost  $167,006.63\n2  Cumulative Drilling Cost   $1,747,745\n3      Cumulative Well Cost   $1,914,752\n4            Daily Mud Cost   $54,185.80\n5       Cumulative Mud Cost  $299,370.66\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Well/Job OCR extraction complete.\nINFO:UnifiedExtractor:Well/Job OCR extraction complete.\nINFO: WELL/JOB DataFrame shape: (19, 2)\nINFO:UnifiedExtractor:WELL/JOB DataFrame shape: (19, 2)\nINFO: well_job JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/well_job_data.json\nINFO:UnifiedExtractor:well_job JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/well_job_data.json\nINFO: well_job CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/well_job_data.csv\nINFO:UnifiedExtractor:well_job CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/well_job_data.csv\nINFO: Running process: obs_int\nINFO:UnifiedExtractor:Running process: obs_int\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png with shape (241, 942, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png with shape (241, 942, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- WELL_JOB JSON Output ---\n{\n    \"WELL/JOB INFORMATION\": {\n        \"Well Name\": \"Ross Fee 4371-31-7-15 MH\",\n        \"Job Name\": \"Drilling\",\n        \"Supervisor(s)\": \"CHAD MILLER / ED COOLEY\",\n        \"Field\": \"XBE\",\n        \"Sec/Twn/Rng\": \"31, 43N, 71W\",\n        \"Phone\": \"307-315-1908\",\n        \"AFE #\": \"240098\",\n        \"API #\": \"49-005-78911\",\n        \"Email\": \"cyclone39@aec-denver.com\",\n        \"Contractor\": \"\",\n        \"Elevation\": \"4913.5\",\n        \"RKB\": \"27.5\",\n        \"Spud Date\": \"6/4/2024\",\n        \"Days from Spud\": \"7.67\",\n        \"Days on Loc\": \"34\",\n        \"MD/TVD\": \"20537 FT/10719 FT\",\n        \"24 Hr Footage\": \"3068\",\n        \"Present Operations\": \"DRILLING LATERAL @ 20,537'.\",\n        \"Activity Planned\": \"DRILL LATERAL SECTION TO PLANNED TD @ ~21,226', PUMP TD SWEEPS & CHC, SOOH & L/D DRILL PIPE.\"\n    }\n}\n\n--- WELL_JOB DataFrame ---\n                   Key                                              Value\n0            Well Name                           Ross Fee 4371-31-7-15 MH\n1             Job Name                                           Drilling\n2        Supervisor(s)                            CHAD MILLER / ED COOLEY\n3                Field                                                XBE\n4          Sec/Twn/Rng                                       31, 43N, 71W\n5                Phone                                       307-315-1908\n6                AFE #                                             240098\n7                API #                                       49-005-78911\n8                Email                           cyclone39@aec-denver.com\n9           Contractor                                                   \n10           Elevation                                             4913.5\n11                 RKB                                               27.5\n12           Spud Date                                           6/4/2024\n13      Days from Spud                                               7.67\n14         Days on Loc                                                 34\n15              MD/TVD                                  20537 FT/10719 FT\n16       24 Hr Footage                                               3068\n17  Present Operations                        DRILLING LATERAL @ 20,537'.\n18    Activity Planned  DRILL LATERAL SECTION TO PLANNED TD @ ~21,226'...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: DAILY NUMBERS: OBSERVATION & INTERVENTION DataFrame shape: (5, 2)\nINFO:UnifiedExtractor:DAILY NUMBERS: OBSERVATION & INTERVENTION DataFrame shape: (5, 2)\nINFO: obs_int JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/obs_int_data.json\nINFO:UnifiedExtractor:obs_int JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/obs_int_data.json\nINFO: obs_int CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/obs_int_data.csv\nINFO:UnifiedExtractor:obs_int CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/obs_int_data.csv\nINFO: Running process: bop\nINFO:UnifiedExtractor:Running process: bop\nERROR: Error in process 'bop': name 'read_pil_image' is not defined\nERROR:UnifiedExtractor:Error in process 'bop': name 'read_pil_image' is not defined\nINFO: Running process: dir_info\nINFO:UnifiedExtractor:Running process: dir_info\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png with shape (241, 1200, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png with shape (241, 1200, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- OBS_INT JSON Output ---\n{\n    \"DAILY NUMBERS: OBSERVATION & INTERVENTION\": [\n        {\n            \"Type\": \"Stop Cards\",\n            \"Number\": \"14\"\n        },\n        {\n            \"Type\": \"Hazard ID's\",\n            \"Number\": \"2\"\n        },\n        {\n            \"Type\": \"JSA's\",\n            \"Number\": \"5\"\n        },\n        {\n            \"Type\": \"Permit to Work\",\n            \"Number\": \"21\"\n        },\n        {\n            \"Type\": \"Totals\",\n            \"Number\": \"\"\n        }\n    ]\n}\n\n--- OBS_INT DataFrame ---\n             Type Number\n0      Stop Cards     14\n1     Hazard ID's      2\n2           JSA's      5\n3  Permit to Work     21\n4          Totals       \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: DIR INFO DataFrame shape: (5, 3)\nINFO:UnifiedExtractor:DIR INFO DataFrame shape: (5, 3)\nINFO: dir_info JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/dir_info_data.json\nINFO:UnifiedExtractor:dir_info JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/dir_info_data.json\nINFO: dir_info CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/dir_info_data.csv\nINFO:UnifiedExtractor:dir_info CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/dir_info_data.csv\nINFO: Running process: survey\nINFO:UnifiedExtractor:Running process: survey\nERROR: Error in process 'survey': cannot unpack non-iterable NoneType object\nERROR:UnifiedExtractor:Error in process 'survey': cannot unpack non-iterable NoneType object\nINFO: Running process: casing\nINFO:UnifiedExtractor:Running process: casing\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png with shape (241, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png with shape (241, 2502, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- DIR_INFO JSON Output ---\n{\n    \"DIR INFO\": [\n        {\n            \"Category\": \"Circ/Cond Hours\",\n            \"Daily\": \"\",\n            \"Cumulative\": \"6.8\"\n        },\n        {\n            \"Category\": \"Sliding Hours\",\n            \"Daily\": \"5.8\",\n            \"Cumulative\": \"28.4\"\n        },\n        {\n            \"Category\": \"Sliding Footage\",\n            \"Daily\": \"247\",\n            \"Cumulative\": \"1488\"\n        },\n        {\n            \"Category\": \"Rotating Hours\",\n            \"Daily\": \"17.8\",\n            \"Cumulative\": \"75.9\"\n        },\n        {\n            \"Category\": \"Rotating Footage\",\n            \"Daily\": \"2821\",\n            \"Cumulative\": \"18941\"\n        }\n    ]\n}\n\n--- DIR_INFO DataFrame ---\n           Category Daily Cumulative\n0   Circ/Cond Hours              6.8\n1     Sliding Hours   5.8       28.4\n2   Sliding Footage   247       1488\n3    Rotating Hours  17.8       75.9\n4  Rotating Footage  2821      18941\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CASING - Line has fewer tokens than expected: ['CASING', 'CASING', '[BLANK]']\nWARNING:UnifiedExtractor:CASING - Line has fewer tokens than expected: ['CASING', 'CASING', '[BLANK]']\nINFO: CASING - Skipping header line: ['Type', 'Size', 'Weight', 'Grade', 'Connection', 'Top', 'MD', 'Bottom', 'MD', 'TOC']\nINFO:UnifiedExtractor:CASING - Skipping header line: ['Type', 'Size', 'Weight', 'Grade', 'Connection', 'Top', 'MD', 'Bottom', 'MD', 'TOC']\nWARNING: CASING - Line has fewer tokens than expected: ['OG']\nWARNING:UnifiedExtractor:CASING - Line has fewer tokens than expected: ['OG']\nINFO: CASING DataFrame shape: (4, 8)\nINFO:UnifiedExtractor:CASING DataFrame shape: (4, 8)\nINFO: casing JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/casing_data.json\nINFO:UnifiedExtractor:casing JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/casing_data.json\nINFO: casing CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/casing_data.csv\nINFO:UnifiedExtractor:casing CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/casing_data.csv\nINFO: Running process: consumables\nINFO:UnifiedExtractor:Running process: consumables\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png with shape (209, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png with shape (209, 2502, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- CASING JSON Output ---\n{\n    \"CASING\": [\n        {\n            \"Type\": \"Conductor\",\n            \"Size\": \"16.000\",\n            \"Weight\": \"36.94\",\n            \"Grade\": \"A252\",\n            \"Connection\": \"WELDED\",\n            \"Top MD\": \"32.00\",\n            \"Bottom MD\": \"108.00\",\n            \"TOC\": \"16\"\n        },\n        {\n            \"Type\": \"Surface\",\n            \"Size\": \"10.750\",\n            \"Weight\": \"40.5\",\n            \"Grade\": \"J55\",\n            \"Connection\": \"BTC\",\n            \"Top MD\": \"31.17\",\n            \"Bottom MD\": \"2268.00\",\n            \"TOC\": \"30\"\n        },\n        {\n            \"Type\": \"Intermediate\",\n            \"Size\": \"7.625\",\n            \"Weight\": \"29.7\",\n            \"Grade\": \"HCP110\",\n            \"Connection\": \"BTC\",\n            \"Top MD\": \"28.89\",\n            \"Bottom MD\": \"9857.70\",\n            \"TOC\": \"2750\"\n        },\n        {\n            \"Type\": \"[BLANK]\",\n            \"Size\": \"[BLANK]\",\n            \"Weight\": \"[BLANK]\",\n            \"Grade\": \"[BLANK]\",\n            \"Connection\": \"[BLANK]\",\n            \"Top MD\": \"[BLANK]\",\n            \"Bottom MD\": \"[BLANK]\",\n            \"TOC\": \"[BLANK]\"\n        }\n    ]\n}\n\n--- CASING DataFrame ---\n           Type     Size   Weight  ...   Top MD Bottom MD      TOC\n0     Conductor   16.000    36.94  ...    32.00    108.00       16\n1       Surface   10.750     40.5  ...    31.17   2268.00       30\n2  Intermediate    7.625     29.7  ...    28.89   9857.70     2750\n3       [BLANK]  [BLANK]  [BLANK]  ...  [BLANK]   [BLANK]  [BLANK]\n\n[4 rows x 8 columns]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: CONSUMABLES DataFrame shape: (3, 5)\nINFO:UnifiedExtractor:CONSUMABLES DataFrame shape: (3, 5)\nINFO: consumables JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/consumables_data.json\nINFO:UnifiedExtractor:consumables JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/consumables_data.json\nINFO: consumables CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/consumables_data.csv\nINFO:UnifiedExtractor:consumables CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/consumables_data.csv\nINFO: Running process: mud\nINFO:UnifiedExtractor:Running process: mud\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png with shape (173, 2502, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png with shape (173, 2502, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- CONSUMABLES JSON Output ---\n{\n    \"CONSUMABLES\": [\n        {\n            \"Consumable\": \"Fuel\",\n            \"Daily Received (gal)\": \"[BLANK]\",\n            \"Daily Used (gal)\": \"1,386\",\n            \"Cumulative Used (gal)\": \"20,626\",\n            \"Daily on Hand (gal)\": \"5,735\"\n        },\n        {\n            \"Consumable\": \"CNG (DGE)\",\n            \"Daily Received (gal)\": \"1,652\",\n            \"Daily Used (gal)\": \"1,652\",\n            \"Cumulative Used (gal)\": \"6,535\",\n            \"Daily on Hand (gal)\": \"[BLANK]\"\n        },\n        {\n            \"Consumable\": \"Mud Fuel\",\n            \"Daily Received (gal)\": \"8,367\",\n            \"Daily Used (gal)\": \"[BLANK]\",\n            \"Cumulative Used (gal)\": \"24,150\",\n            \"Daily on Hand (gal)\": \"11,643\"\n        }\n    ]\n}\n\n--- CONSUMABLES DataFrame ---\n  Consumable Daily Received (gal)  ... Cumulative Used (gal) Daily on Hand (gal)\n0       Fuel              [BLANK]  ...                20,626               5,735\n1  CNG (DGE)                1,652  ...                 6,535             [BLANK]\n2   Mud Fuel                8,367  ...                24,150              11,643\n\n[3 rows x 5 columns]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Row 0 text: MUD MUD [BLANK]\nINFO:UnifiedExtractor:Row 0 text: MUD MUD [BLANK]\nINFO: Row 1 text: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO:UnifiedExtractor:Row 1 text: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO: Row 2 text: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO:UnifiedExtractor:Row 2 text: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO: Row 3 text: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO:UnifiedExtractor:Row 3 text: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO: Row 4 text: 4 5 1023 13 481 [BLANK]\nINFO:UnifiedExtractor:Row 4 text: 4 5 1023 13 481 [BLANK]\nINFO: Header1: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO:UnifiedExtractor:Header1: Type Weight In Weight Out pH CAKE GELS (10s/10m/30m) Oil/Water FV ES PV YP CL Ca LGS WL HTHP Loss\nINFO: Value1: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO:UnifiedExtractor:Value1: OBM 11.5 11.5 [BLANK] 3 8 25 27 88/12 60.0 753 16 8 31,000 326,667 4.47 [BLANK] 5.00\nINFO: Header2: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO:UnifiedExtractor:Header2: 3 RPM 6 RPM Mud Pits and Hole Volume 24 Hr Loss Total Loss Comments\nINFO: Value2: 4 5 1023 13 481 [BLANK]\nINFO:UnifiedExtractor:Value2: 4 5 1023 13 481 [BLANK]\nINFO: Tokens from data row 1: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00']\nINFO:UnifiedExtractor:Tokens from data row 1: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00']\nINFO: Tokens from data row 2: ['4', '5', '1023', '13', '481', '[BLANK]']\nINFO:UnifiedExtractor:Tokens from data row 2: ['4', '5', '1023', '13', '481', '[BLANK]']\nINFO: Combined tokens: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO:UnifiedExtractor:Combined tokens: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO: Expected token count: 24, tokens extracted: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO:UnifiedExtractor:Expected token count: 24, tokens extracted: ['OBM', '11.5', '11.5', '[BLANK]', '3', '8', '25', '27', '88/12', '60.0', '753', '16', '8', '31,000', '326,667', '4.47', '[BLANK]', '5.00', '4', '5', '1023', '13', '481', '[BLANK]']\nINFO: Mapped dictionary: {'Type': 'OBM', 'Weight In': '11.5', 'Weight Out': '11.5', 'pH': '[BLANK]', 'CAKE': '3', 'GELS (10s/10m/30m)': {'10s': '8', '10m': '25', '30m': '27'}, 'Oil/Water': '88/12', 'FV': '60.0', 'ES': '753', 'PV': '16', 'YP': '8', 'CL': '31,000', 'Ca': '326,667', 'LGS': '4.47', 'WL': '[BLANK]', 'HTHP Loss': '5.00', '3 RPM': '4', '6 RPM': '5', 'Mud Pits and Hole Volume': '1023', '24 Hr Loss': '13', 'Total Loss': '481', 'Comments': '[BLANK]'}\nINFO:UnifiedExtractor:Mapped dictionary: {'Type': 'OBM', 'Weight In': '11.5', 'Weight Out': '11.5', 'pH': '[BLANK]', 'CAKE': '3', 'GELS (10s/10m/30m)': {'10s': '8', '10m': '25', '30m': '27'}, 'Oil/Water': '88/12', 'FV': '60.0', 'ES': '753', 'PV': '16', 'YP': '8', 'CL': '31,000', 'Ca': '326,667', 'LGS': '4.47', 'WL': '[BLANK]', 'HTHP Loss': '5.00', '3 RPM': '4', '6 RPM': '5', 'Mud Pits and Hole Volume': '1023', '24 Hr Loss': '13', 'Total Loss': '481', 'Comments': '[BLANK]'}\nINFO: mud JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/mud_data.json\nINFO:UnifiedExtractor:mud JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/mud_data.json\nINFO: mud CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/mud_data.csv\nINFO:UnifiedExtractor:mud CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/mud_data.csv\nINFO: Running process: bha\nINFO:UnifiedExtractor:Running process: bha\nERROR: Error in process 'bha': 'NoneType' object has no attribute 'read'\nERROR:UnifiedExtractor:Error in process 'bha': 'NoneType' object has no attribute 'read'\nINFO: Running process: pumps\nINFO:UnifiedExtractor:Running process: pumps\nERROR: Error in process 'pumps': name 'read_pil_image' is not defined\nERROR:UnifiedExtractor:Error in process 'pumps': name 'read_pil_image' is not defined\nINFO: Running process: personnel\nINFO:UnifiedExtractor:Running process: personnel\nINFO: Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_9.png with shape (241, 1560, 3)\nINFO:UnifiedExtractor:Image loaded from /dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_9.png with shape (241, 1560, 3)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- MUD JSON Output ---\n{\n    \"MUD\": {\n        \"Type\": \"OBM\",\n        \"Weight In\": \"11.5\",\n        \"Weight Out\": \"11.5\",\n        \"pH\": \"[BLANK]\",\n        \"CAKE\": \"3\",\n        \"GELS (10s/10m/30m)\": {\n            \"10s\": \"8\",\n            \"10m\": \"25\",\n            \"30m\": \"27\"\n        },\n        \"Oil/Water\": \"88/12\",\n        \"FV\": \"60.0\",\n        \"ES\": \"753\",\n        \"PV\": \"16\",\n        \"YP\": \"8\",\n        \"CL\": \"31,000\",\n        \"Ca\": \"326,667\",\n        \"LGS\": \"4.47\",\n        \"WL\": \"[BLANK]\",\n        \"HTHP Loss\": \"5.00\",\n        \"3 RPM\": \"4\",\n        \"6 RPM\": \"5\",\n        \"Mud Pits and Hole Volume\": \"1023\",\n        \"24 Hr Loss\": \"13\",\n        \"Total Loss\": \"481\",\n        \"Comments\": \"[BLANK]\"\n    }\n}\n\n--- MUD DataFrame ---\n                         Key                                   Value\n0                       Type                                     OBM\n1                  Weight In                                    11.5\n2                 Weight Out                                    11.5\n3                         pH                                 [BLANK]\n4                       CAKE                                       3\n5         GELS (10s/10m/30m)  {'10s': '8', '10m': '25', '30m': '27'}\n6                  Oil/Water                                   88/12\n7                         FV                                    60.0\n8                         ES                                     753\n9                         PV                                      16\n10                        YP                                       8\n11                        CL                                  31,000\n12                        Ca                                 326,667\n13                       LGS                                    4.47\n14                        WL                                 [BLANK]\n15                 HTHP Loss                                    5.00\n16                     3 RPM                                       4\n17                     6 RPM                                       5\n18  Mud Pits and Hole Volume                                    1023\n19                24 Hr Loss                                      13\n20                Total Loss                                     481\n21                  Comments                                 [BLANK]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Row has unexpected number of numeric tokens: PERSONNEL PERSONNEL |\nWARNING:UnifiedExtractor:Row has unexpected number of numeric tokens: PERSONNEL PERSONNEL |\nINFO: Parsed row: {'Company': 'WORKRISE', 'Contractor': 'Service Company', 'No. Personnel': 2, 'Daily Hours': 24, 'Cumulative Hours': 2777}\nINFO:UnifiedExtractor:Parsed row: {'Company': 'WORKRISE', 'Contractor': 'Service Company', 'No. Personnel': 2, 'Daily Hours': 24, 'Cumulative Hours': 2777}\nINFO: Parsed row: {'Company': 'Cyclone Drilling Days Crews', 'Contractor': 'Service Company', 'No. Personnel': 7, 'Daily Hours': 84, 'Cumulative Hours': 2777}\nINFO:UnifiedExtractor:Parsed row: {'Company': 'Cyclone Drilling Days Crews', 'Contractor': 'Service Company', 'No. Personnel': 7, 'Daily Hours': 84, 'Cumulative Hours': 2777}\nINFO: Parsed row: {'Company': 'Cyclone Drilling Night Crews', 'Contractor': 'Service Company', 'No. Personnel': 7, 'Daily Hours': 84, 'Cumulative Hours': 2777}\nINFO:UnifiedExtractor:Parsed row: {'Company': 'Cyclone Drilling Night Crews', 'Contractor': 'Service Company', 'No. Personnel': 7, 'Daily Hours': 84, 'Cumulative Hours': 2777}\nINFO: Parsed row: {'Company': 'DCT', 'Contractor': 'Service Company', 'No. Personnel': 2, 'Daily Hours': 24, 'Cumulative Hours': 2777}\nINFO:UnifiedExtractor:Parsed row: {'Company': 'DCT', 'Contractor': 'Service Company', 'No. Personnel': 2, 'Daily Hours': 24, 'Cumulative Hours': 2777}\nINFO: Totals row parsed: {'Company': '', 'Contractor': '', 'No. Personnel': 'Totals', 'Daily Hours': 347, 'Cumulative Hours': '3069.0'}\nINFO:UnifiedExtractor:Totals row parsed: {'Company': '', 'Contractor': '', 'No. Personnel': 'Totals', 'Daily Hours': 347, 'Cumulative Hours': '3069.0'}\nINFO: personnel JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/personnel_data.json\nINFO:UnifiedExtractor:personnel JSON data saved to /dbfs/mnt/mini-proj-dd/final_results/personnel_data.json\nINFO: personnel CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/personnel_data.csv\nINFO:UnifiedExtractor:personnel CSV data saved to /dbfs/mnt/mini-proj-dd/final_results/personnel_data.csv\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- PERSONNEL JSON Output ---\n{\n    \"PERSONNEL\": [\n        {\n            \"Company\": \"WORKRISE\",\n            \"Contractor\": \"Service Company\",\n            \"No. Personnel\": 2,\n            \"Daily Hours\": 24,\n            \"Cumulative Hours\": 2777\n        },\n        {\n            \"Company\": \"Cyclone Drilling Days Crews\",\n            \"Contractor\": \"Service Company\",\n            \"No. Personnel\": 7,\n            \"Daily Hours\": 84,\n            \"Cumulative Hours\": 2777\n        },\n        {\n            \"Company\": \"Cyclone Drilling Night Crews\",\n            \"Contractor\": \"Service Company\",\n            \"No. Personnel\": 7,\n            \"Daily Hours\": 84,\n            \"Cumulative Hours\": 2777\n        },\n        {\n            \"Company\": \"DCT\",\n            \"Contractor\": \"Service Company\",\n            \"No. Personnel\": 2,\n            \"Daily Hours\": 24,\n            \"Cumulative Hours\": 2777\n        },\n        {\n            \"Company\": \"\",\n            \"Contractor\": \"\",\n            \"No. Personnel\": \"Totals\",\n            \"Daily Hours\": 347,\n            \"Cumulative Hours\": \"3069.0\"\n        }\n    ]\n}\n\n--- PERSONNEL DataFrame ---\n                        Company       Contractor  ... Daily Hours  Cumulative Hours\n0                      WORKRISE  Service Company  ...          24              2777\n1   Cyclone Drilling Days Crews  Service Company  ...          84              2777\n2  Cyclone Drilling Night Crews  Service Company  ...          84              2777\n3                           DCT  Service Company  ...          24              2777\n4                                                 ...         347            3069.0\n\n[5 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# 4) BOP Extraction process\n",
    "# --------------------------------------------------------\n",
    "\n",
    "def extract_bop_info(ocr_text):\n",
    "    \"\"\"Extract BOP information using regex.\"\"\"\n",
    "    pattern = {\n",
    "        \"Last BOP Test Date\": r\"Last BOP Test Date\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Last BOP Drill\": r\"Last BOP Drill\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Next BOP Test\": r\"Next BOP Test\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\"\n",
    "    }\n",
    "    result = {}\n",
    "    for key, regex in pattern.items():\n",
    "        match = re.search(regex, ocr_text, re.IGNORECASE)\n",
    "        result[key] = match.group(1) if match else \"\"\n",
    "    return result\n",
    "\n",
    "def process_bop(bop_img_path, debug=False):\n",
    "    \"\"\"Processes BOP section: read image, perform OCR, extract info, return JSON and DataFrame.\"\"\"\n",
    "    pil_img = read_pil_image(bop_img_path)\n",
    "    ocr_text = perform_ocr(pil_img)\n",
    "    bop_data = extract_bop_info(ocr_text)\n",
    "    final_output = {\"BOP\": bop_data}\n",
    "    df = pd.DataFrame(list(bop_data.items()), columns=[\"Key\", \"Value\"])\n",
    "    return final_output, df\n",
    "\n",
    "\n",
    "def process_pumps_data(dummy_arg=None):\n",
    "    pumps_img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_12.png\"\n",
    "    pil_img = read_pil_image(pumps_img_path)\n",
    "    ocr_text = pytesseract.image_to_string(pil_img)\n",
    "    # Dummy parsing logic for pumps; replace with your actual extraction.\n",
    "    pumps = [{\"Number\": \"1\", \"Model\": \"BOMCO\", \"Type\": \"TRIPLEX\", \"Efficiency\": \"95\"}]\n",
    "    circ_rates = []  # Assume drilling/circ rates not available in this dummy.\n",
    "    final_data = {\"Pumps\": pumps, \"DrillingCircRates\": circ_rates}\n",
    "    df = pd.DataFrame(pumps)\n",
    "    return final_data, df\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 7) process Runner Helper Function\n",
    "# --------------------------------------------------------\n",
    "def run_process(name, pipe, debug=False):\n",
    "    logger.info(f\"Running process: {name}\")\n",
    "    try:\n",
    "        output_json, output_df = pipe[\"func\"](None)\n",
    "        output_folder = dbfs_to_local_path(\"dbfs:/mnt/mini-proj-dd/final_results\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        json_path = os.path.join(output_folder, pipe[\"json\"])\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(output_json, f, indent=4)\n",
    "        logger.info(f\"{name} JSON data saved to {json_path}\")\n",
    "        csv_path = os.path.join(output_folder, pipe[\"csv\"])\n",
    "        output_df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"{name} CSV data saved to {csv_path}\")\n",
    "        print(f\"\\n--- {name.upper()} JSON Output ---\")\n",
    "        print(json.dumps(output_json, indent=4))\n",
    "        print(f\"\\n--- {name.upper()} DataFrame ---\")\n",
    "        print(output_df)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process '{name}': {e}\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 8) Main process Function with processs Dictionary\n",
    "# --------------------------------------------------------\n",
    "def main():\n",
    "    debug = False  # Set True for detailed logging and image display.\n",
    "    processs = {\n",
    "        \"cost_data\": {\n",
    "            \"func\": process_cost_data,\n",
    "            \"csv\": \"cost_data.csv\",\n",
    "            \"json\": \"cost_data.json\"\n",
    "        },\n",
    "        \"well_job\": {\n",
    "            \"func\": lambda d: process_well_job_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\", d),\n",
    "            \"csv\": \"well_job_data.csv\",\n",
    "            \"json\": \"well_job_data.json\"\n",
    "        },\n",
    "        \"obs_int\": {\n",
    "            \"func\": lambda d: process_obs_int(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\", d),\n",
    "            \"csv\": \"obs_int_data.csv\",\n",
    "            \"json\": \"obs_int_data.json\"\n",
    "        },\n",
    "        \"bop\": {\n",
    "            \"func\": lambda d: process_bop(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\", debug),\n",
    "            \"csv\": \"bop_data.csv\",\n",
    "            \"json\": \"bop_data.json\"\n",
    "        },\n",
    "        \"dir_info\": {\n",
    "            \"func\": lambda d: process_dir_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\", d),\n",
    "            \"csv\": \"dir_info_data.csv\",\n",
    "            \"json\": \"dir_info_data.json\"\n",
    "        },\n",
    "        \"survey\": {\n",
    "            \"func\": lambda d: process_survey(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_4.png\", debug),\n",
    "            \"csv\": \"survey_data.csv\",\n",
    "            \"json\": \"survey_data.json\"\n",
    "        },\n",
    "        \"casing\": {\n",
    "            \"func\": process_casing_data,\n",
    "            \"csv\": \"casing_data.csv\",\n",
    "            \"json\": \"casing_data.json\"\n",
    "        },\n",
    "        \"consumables\": {\n",
    "            \"func\": process_consumables_data,\n",
    "            \"csv\": \"consumables_data.csv\",\n",
    "            \"json\": \"consumables_data.json\"\n",
    "        },\n",
    "        \"mud\": {\n",
    "            \"func\": process_mud_data,\n",
    "            \"csv\": \"mud_data.csv\",\n",
    "            \"json\": \"mud_data.json\"\n",
    "        },\n",
    "        \"bha\": {\n",
    "            \"func\": extract_bha_data,\n",
    "            \"csv\": \"bha_data.csv\",\n",
    "            \"json\": \"bha_data.json\"\n",
    "        },\n",
    "        \"pumps\": {\n",
    "            \"func\": process_pumps_data,\n",
    "            \"csv\": \"pumps_data.csv\",\n",
    "            \"json\": \"pumps_data.json\"\n",
    "        },\n",
    "        \"personnel\": {\n",
    "            \"func\": lambda d: process_personnel(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_9.png\", debug),\n",
    "            \"csv\": \"personnel_data.csv\",\n",
    "            \"json\": \"personnel_data.json\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, pipe in processs.items():\n",
    "        run_process(name, pipe, debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d6234a0-539a-4a16-a2ff-e31f9652768e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 3) Survey Extraction process\n",
    "# ---------------------------------------------------------------------\n",
    "def build_survey_dict_from_rois(roi_texts, expected_headers):\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    \n",
    "    row_strings = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row.sort(key=lambda cell: cell[0])\n",
    "        line = \" \".join(cell[4] for cell in row)\n",
    "        row_strings.append(line)\n",
    "        logger.info(f\"Grouped Row {i}: {line}\")\n",
    "    \n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for subline in line.split(\"\\n\"):\n",
    "            subline = subline.strip()\n",
    "            if subline:\n",
    "                all_lines.append(subline)\n",
    "    logger.info(f\"All extracted lines: {all_lines}\")\n",
    "    \n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"md\" in lower_tokens and \"inclination\" in lower_tokens:\n",
    "            logger.info(f\"Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    \n",
    "    survey_list = []\n",
    "    for tokens in data_lines:\n",
    "        row_dict = {expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "        survey_list.append(row_dict)\n",
    "    return survey_list\n",
    "\n",
    "def sort_survey_data(survey_list):\n",
    "    def md_value(row):\n",
    "        try:\n",
    "            return float(row[\"MD\"].replace(\",\", \"\"))\n",
    "        except Exception:\n",
    "            return 0\n",
    "    sorted_list = sorted(survey_list, key=md_value, reverse=True)\n",
    "    filtered_list = [row for row in sorted_list if not row[\"MD\"].upper().startswith(\"SURVEY\")]\n",
    "    return filtered_list\n",
    "\n",
    "def process_survey(survey_img_path, debug=False):\n",
    "    expected_headers = [\"MD\", \"Inclination\", \"Azimuth\", \"DLS\", \"TVD\"]\n",
    "    img = safe_read_image(survey_img_path)\n",
    "    if debug:\n",
    "        show_image(\"Original Survey Image\", img, size=(12,12))\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 15, 9\n",
    "    )\n",
    "    if debug:\n",
    "        show_image(\"Adaptive Threshold\", thresh, cmap=\"gray\")\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "    rois.sort(key=lambda b: (b[1], b[0]))\n",
    "    if debug:\n",
    "        debug_img = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n",
    "        for (x, y, w, h) in rois:\n",
    "            cv2.rectangle(debug_img, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "        show_image(\"Detected Text Regions\", debug_img)\n",
    "    roi_texts = []\n",
    "    for (x, y, w, h) in rois:\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        text = pytesseract.image_to_string(roi, config=\"--psm 6\").strip()\n",
    "        if not text:\n",
    "            text = \"[BLANK]\"\n",
    "        roi_texts.append((x, y, w, h, text))\n",
    "        if debug:\n",
    "            logger.info(f\"OCR Box ({x},{y},{w},{h}): {text}\")\n",
    "    survey_list = build_survey_dict_from_rois(roi_texts, expected_headers)\n",
    "    survey_list = sort_survey_data(survey_list)\n",
    "    final_output = {\"SURVEY DATA\": survey_list}\n",
    "    df = pd.DataFrame(survey_list)\n",
    "    return final_output, df\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) BOP Extraction process\n",
    "# ---------------------------------------------------------------------\n",
    "# def perform_ocr_bop(img):\n",
    "#     text = pytesseract.image_to_string(img)\n",
    "#     logger.info(\"BOP OCR extraction complete.\")\n",
    "#     return text\n",
    "\n",
    "def extract_bop_info(ocr_text):\n",
    "    pattern = {\n",
    "        \"Last BOP Test Date\": r\"Last BOP Test Date\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Last BOP Drill\": r\"Last BOP Drill\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Next BOP Test\": r\"Next BOP Test\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\"\n",
    "    }\n",
    "    result = {}\n",
    "    for key, regex in pattern.items():\n",
    "        match = re.search(regex, ocr_text, re.IGNORECASE)\n",
    "        result[key] = match.group(1) if match else \"\"\n",
    "    return result\n",
    "\n",
    "def process_bop(bop_img_path, debug=False):\n",
    "    pil_img = read_pil_image(bop_img_path)\n",
    "    ocr_text = perform_ocr_bop(pil_img)\n",
    "    bop_data = extract_bop_info(ocr_text)\n",
    "    final_output = {\"BOP\": bop_data}\n",
    "    df = pd.DataFrame(list(bop_data.items()), columns=[\"Key\", \"Value\"])\n",
    "    return final_output, df\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) BHA Extraction process\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_bha_data(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    patterns = {\n",
    "        \"Drill Pipe Detail\": r\"Drill Pipe Detail:\\s*([^\\n]+)\",\n",
    "        \"Size\": r\"Size:\\s*([\\d.]+)\\b\",\n",
    "        \"Wt./Ft\": r\"Wt\\./Ft:\\s*([\\d.]+)\\b\",\n",
    "        \"Connection\": r\"Connection:\\s*([\\w\\d-]+)\\b\",\n",
    "        \"ID\": r\"ID:\\s*([\\d.]+)\\b\",\n",
    "        \"Drill Bit\": r\"Drill Bit:\\s*([^\\n;]+)\",\n",
    "        \"Motor\": r\"Motor:\\s*([^\\n;]+)\",\n",
    "        \"MWD Tool\": r\"MWD Tool:\\s*([^\\n;]+)\",\n",
    "        \"Monel Collar\": r\"Monel Collar:\\s*([^\\n;]+)\",\n",
    "        \"X-Over\": r\"X-Over:\\s*([^\\n;]+)\",\n",
    "        \"Sub\": r\"Sub:\\s*([^\\n;]+)\",\n",
    "        \"HWDP\": r\"HWDP:\\s*([^\\n;]+)\",\n",
    "        \"Drill Pipe\": r\"Drill Pipe:\\s*([\\d.]+(?:\\\" DP)?)\",\n",
    "        \"Reamer\": r\"Reamer:\\s*([^\\n;]+)\",\n",
    "        \"Shock Sub\": r\"Shock Sub:\\s*([^\\n;]+)\",\n",
    "        \"Total Length\": r\"Total Length:\\s*(\\d+)\\b\"\n",
    "    }\n",
    "    bha_data = {}\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, ocr_text)\n",
    "        if match:\n",
    "            bha_data[key] = match.group(1).strip()\n",
    "    if \"Drill Pipe Detail\" in bha_data:\n",
    "        detail = bha_data[\"Drill Pipe Detail\"]\n",
    "        for remove_key in [\"Size\", \"Wt./Ft\", \"Connection\", \"ID\"]:\n",
    "            if remove_key in bha_data:\n",
    "                detail = re.sub(rf\"{remove_key}:\\s*{re.escape(bha_data[remove_key])}\", \"\", detail).strip(\",; \")\n",
    "        bha_data[\"Drill Pipe Detail\"] = detail\n",
    "    structured_data = {\n",
    "        \"BHA\": {\n",
    "            \"Drill Pipe Detail\": bha_data.get(\"Drill Pipe Detail\", \"\"),\n",
    "            \"Size\": bha_data.get(\"Size\", \"\"),\n",
    "            \"Wt./Ft\": bha_data.get(\"Wt./Ft\", \"\"),\n",
    "            \"Connection\": bha_data.get(\"Connection\", \"\"),\n",
    "            \"ID\": bha_data.get(\"ID\", \"\"),\n",
    "            \"BHA #4\": {\n",
    "                \"Drill Bit\": bha_data.get(\"Drill Bit\", \"\"),\n",
    "                \"Motor\": bha_data.get(\"Motor\", \"\"),\n",
    "                \"MWD Tool\": bha_data.get(\"MWD Tool\", \"\"),\n",
    "                \"Monel Collar\": bha_data.get(\"Monel Collar\", \"\"),\n",
    "                \"X-Over\": bha_data.get(\"X-Over\", \"\"),\n",
    "                \"Sub\": bha_data.get(\"Sub\", \"\"),\n",
    "                \"HWDP\": bha_data.get(\"HWDP\", \"\"),\n",
    "                \"Drill Pipe\": bha_data.get(\"Drill Pipe\", \"\"),\n",
    "                \"Reamer\": bha_data.get(\"Reamer\", \"\"),\n",
    "                \"Shock Sub\": bha_data.get(\"Shock Sub\", \"\")\n",
    "            },\n",
    "            \"Total Length\": bha_data.get(\"Total Length\", \"\")\n",
    "        }\n",
    "    }\n",
    "    return structured_data\n",
    "\n",
    "def process_bha(bha_img_path, debug=False):\n",
    "    bha_json = extract_bha_data(bha_img_path)\n",
    "    df = pd.json_normalize(bha_json[\"BHA\"])\n",
    "    return {\"BHA\": bha_json[\"BHA\"]}, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def92dc4-6ae2-400d-ac40-b7650d9d5c02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error in process 'cost_data': cannot unpack non-iterable NoneType object\nERROR:__main__:Error in process 'well_job': cannot unpack non-iterable NoneType object\nERROR:__main__:Error in process 'obs_int': cannot unpack non-iterable NoneType object\nERROR:__main__:Error in process 'bop': name 'read_pil_image' is not defined\nERROR:__main__:Error in process 'dir_info': cannot unpack non-iterable NoneType object\nERROR:__main__:Error in process 'casing': cannot unpack non-iterable NoneType object\nERROR:__main__:Error in process 'consumables': cannot unpack non-iterable NoneType object\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- SURVEY JSON Output ---\n{\n    \"SURVEY DATA\": [\n        {\n            \"MD\": \"20,286\",\n            \"Inclination\": \"89.20\",\n            \"Azimuth\": \"179.98\",\n            \"DLS\": \"0.67\",\n            \"TVD\": \"10,716\"\n        },\n        {\n            \"MD\": \"20,191\",\n            \"Inclination\": \"89.23\",\n            \"Azimuth\": \"179.34\",\n            \"DLS\": \"0.51\",\n            \"TVD\": \"10,715\"\n        },\n        {\n            \"MD\": \"20,096\",\n            \"Inclination\": \"89.65\",\n            \"Azimuth\": \"179.59\",\n            \"DLS\": \"0.55\",\n            \"TVD\": \"10,714\"\n        },\n        {\n            \"MD\": \"20,001\",\n            \"Inclination\": \"89.65\",\n            \"Azimuth\": \"180.11\",\n            \"DLS\": \"0.34\",\n            \"TVD\": \"10,714\"\n        },\n        {\n            \"MD\": \"19,906\",\n            \"Inclination\": \"89.76\",\n            \"Azimuth\": \"179.81\",\n            \"DLS\": \"0.15\",\n            \"TVD\": \"10,713\"\n        }\n    ]\n}\n\n--- SURVEY DataFrame ---\n       MD Inclination Azimuth   DLS     TVD\n0  20,286       89.20  179.98  0.67  10,716\n1  20,191       89.23  179.34  0.51  10,715\n2  20,096       89.65  179.59  0.55  10,714\n3  20,001       89.65  180.11  0.34  10,714\n4  19,906       89.76  179.81  0.15  10,713\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Error in process 'bha': extract_bha_data() got an unexpected keyword argument 'debug'\nERROR:__main__:Error in process 'pumps': name 'read_pil_image' is not defined\nERROR:__main__:Error in process 'personnel': cannot unpack non-iterable NoneType object\nERROR:__main__:Error in process 'time_breakdown': name 'show_image' is not defined\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n--- MUD JSON Output ---\n{\n    \"MUD\": {\n        \"Type\": \"OBM\",\n        \"Weight In\": \"11.5\",\n        \"Weight Out\": \"11.5\",\n        \"pH\": \"[BLANK]\",\n        \"CAKE\": \"3\",\n        \"GELS (10s/10m/30m)\": {\n            \"10s\": \"8\",\n            \"10m\": \"25\",\n            \"30m\": \"27\"\n        },\n        \"Oil/Water\": \"88/12\",\n        \"FV\": \"60.0\",\n        \"ES\": \"753\",\n        \"PV\": \"16\",\n        \"YP\": \"8\",\n        \"CL\": \"31,000\",\n        \"Ca\": \"326,667\",\n        \"LGS\": \"4.47\",\n        \"WL\": \"[BLANK]\",\n        \"HTHP Loss\": \"5.00\",\n        \"3 RPM\": \"4\",\n        \"6 RPM\": \"5\",\n        \"Mud Pits and Hole Volume\": \"1023\",\n        \"24 Hr Loss\": \"13\",\n        \"Total Loss\": \"481\",\n        \"Comments\": \"[BLANK]\"\n    }\n}\n\n--- MUD DataFrame ---\n                         Key                                   Value\n0                       Type                                     OBM\n1                  Weight In                                    11.5\n2                 Weight Out                                    11.5\n3                         pH                                 [BLANK]\n4                       CAKE                                       3\n5         GELS (10s/10m/30m)  {'10s': '8', '10m': '25', '30m': '27'}\n6                  Oil/Water                                   88/12\n7                         FV                                    60.0\n8                         ES                                     753\n9                         PV                                      16\n10                        YP                                       8\n11                        CL                                  31,000\n12                        Ca                                 326,667\n13                       LGS                                    4.47\n14                        WL                                 [BLANK]\n15                 HTHP Loss                                    5.00\n16                     3 RPM                                       4\n17                     6 RPM                                       5\n18  Mud Pits and Hole Volume                                    1023\n19                24 Hr Loss                                      13\n20                Total Loss                                     481\n21                  Comments                                 [BLANK]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# 7) Time Breakdown process\n",
    "# ---------------------------------------------------------------------\n",
    "# (The following functions are taken and merged from your provided time breakdown code.)\n",
    "def preprocess_image(img, debug=False):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if debug:\n",
    "        show_image(\"1) Grayscale\", gray, cmap=\"gray\", size=(10,10))\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 15, 9)\n",
    "    if debug:\n",
    "        show_image(\"2) Adaptive Threshold\", thresh, cmap=\"gray\", size=(10,10))\n",
    "    return thresh\n",
    "\n",
    "def detect_text_regions_tb(thresh_img, debug=True):\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois = []\n",
    "    debug_img = cv2.cvtColor(thresh_img, cv2.COLOR_GRAY2BGR)\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "            cv2.rectangle(debug_img, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "    rois.sort(key=lambda b: (b[1], b[0]))\n",
    "    if debug:\n",
    "        show_image(\"3) Detected Text Regions\", debug_img, size=(12,12))\n",
    "    return rois\n",
    "\n",
    "def perform_ocr_on_rois_tb(img, rois, debug=True):\n",
    "    results = []\n",
    "    n = len(rois)\n",
    "    if debug and n > 0:\n",
    "        cols = 5\n",
    "        rows = math.ceil(n / cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "        axes = axes.flatten() if rows > 1 else [axes]\n",
    "    for i, (x, y, w, h) in enumerate(rois):\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        text = pytesseract.image_to_string(roi, config=\"--psm 6\").strip()\n",
    "        if not text:\n",
    "            text = \"[BLANK]\"\n",
    "        results.append((x, y, w, h, text))\n",
    "        if debug and i < len(axes):\n",
    "            roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "            axes[i].imshow(roi_rgb)\n",
    "            axes[i].set_title(f\"ROI {i+1}\\n{text[:30]}...\")\n",
    "            axes[i].axis(\"off\")\n",
    "    if debug and n > 0:\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    return results\n",
    "\n",
    "def group_ocr_rows(roi_results, y_threshold=20):\n",
    "    groups = []\n",
    "    roi_results_sorted = sorted(roi_results, key=lambda r: r[1])\n",
    "    current_group = []\n",
    "    current_y = None\n",
    "    for (x, y, w, h, text) in roi_results_sorted:\n",
    "        if current_y is None:\n",
    "            current_y = y\n",
    "            current_group.append((x, y, w, h, text))\n",
    "        elif abs(y - current_y) <= y_threshold:\n",
    "            current_group.append((x, y, w, h, text))\n",
    "        else:\n",
    "            groups.append(current_group)\n",
    "            current_group = [(x, y, w, h, text)]\n",
    "            current_y = y\n",
    "    if current_group:\n",
    "        groups.append(current_group)\n",
    "    return groups\n",
    "\n",
    "def parse_operations_description(ops_text):\n",
    "    ops_data = {\n",
    "        \"Depth\": {\"From\": \"\", \"To\": \"\"},\n",
    "        \"Performance\": {\"Feet\": \"\", \"FPH\": \"\"},\n",
    "        \"Rotation_Slide\": {\"Rotate\": \"\", \"Slide\": \"\"},\n",
    "        \"Rotation_Time\": {\"Rotate Time\": \"\", \"Slide Time\": \"\"},\n",
    "        \"GPM\": \"\",\n",
    "        \"MTR RPM\": \"\",\n",
    "        \"SPP\": \"\",\n",
    "        \"DIFF\": \"\",\n",
    "        \"WOB\": \"\",\n",
    "        \"ROT RPM\": \"\",\n",
    "        \"ON BTM TRQ\": \"\",\n",
    "        \"OFF BTM TRQ\": \"\",\n",
    "        \"GAS\": {\"Units\": \"\", \"Flare\": \"\"},\n",
    "        \"MW\": {\"In\": \"\", \"Out\": \"\"},\n",
    "        \"Targets\": [],\n",
    "        \"Observations\": []\n",
    "    }\n",
    "    depth_match = re.search(r\"F/\\s*([\\d,']+)\\s*T/\\s*([\\d,']+)\", ops_text, re.IGNORECASE)\n",
    "    if depth_match:\n",
    "        ops_data[\"Depth\"][\"From\"] = depth_match.group(1)\n",
    "        ops_data[\"Depth\"][\"To\"] = depth_match.group(2)\n",
    "    perf_match = re.search(r\"\\(([\\d,']+)\\s*@\\s*(\\d+)\\s*FPH\\)\", ops_text, re.IGNORECASE)\n",
    "    if perf_match:\n",
    "        ops_data[\"Performance\"][\"Feet\"] = perf_match.group(1)\n",
    "        ops_data[\"Performance\"][\"FPH\"] = perf_match.group(2)\n",
    "    rs_match = re.search(r\"ROTATE\\s*([\\d.]+%)\\s*/\\s*SLIDE\\s*([\\d.]+%)\", ops_text, re.IGNORECASE)\n",
    "    if rs_match:\n",
    "        ops_data[\"Rotation_Slide\"][\"Rotate\"] = rs_match.group(1)\n",
    "        ops_data[\"Rotation_Slide\"][\"Slide\"] = rs_match.group(2)\n",
    "    rt_match = re.search(r\"ROTATE\\s*TIME\\s*([\\d.]+%)\\s*/\\s*SLIDE\\s*TIME\\s*([\\d.]+%)\", ops_text, re.IGNORECASE)\n",
    "    if rt_match:\n",
    "        ops_data[\"Rotation_Time\"][\"Rotate Time\"] = rt_match.group(1)\n",
    "        ops_data[\"Rotation_Time\"][\"Slide Time\"] = rt_match.group(2)\n",
    "    numeric_patterns = {\n",
    "        \"GPM\": r\"GPM:\\s*(\\d+)\",\n",
    "        \"MTR RPM\": r\"MTR\\s*RPM:\\s*(\\d+)\",\n",
    "        \"SPP\": r\"SPP:\\s*([\\d,]+(?:-\\d+)?)(?:,|\\s|$)\",\n",
    "        \"DIFF\": r\"DIFF:\\s*([\\d\\-]+)\",\n",
    "        \"WOB\": r\"WOB:\\s*([\\d,]+(?:-\\d+)?)(?:,|\\s|$)\",\n",
    "        \"ROT RPM\": r\"ROT\\s*RPM:\\s*([\\d,]+(?:-\\d+)?)(?:,|\\s|$)\",\n",
    "        \"ON BTM TRQ\": r\"ON\\s*BTM\\s*TRQ[:;]?\\s*([\\d\\-K]+)\",\n",
    "        \"OFF BTM TRQ\": r\"OFF\\s*BTM\\s*TRQ[:;]?\\s*([\\d\\-K]+)\"\n",
    "    }\n",
    "    for key, pattern in numeric_patterns.items():\n",
    "        m = re.search(pattern, ops_text, re.IGNORECASE)\n",
    "        if m:\n",
    "            ops_data[key] = m.group(1)\n",
    "    gas_units = re.search(r\"GAS:\\s*([\\d,]+)\\s*UNITS\", ops_text, re.IGNORECASE)\n",
    "    if gas_units:\n",
    "        ops_data[\"GAS\"][\"Units\"] = gas_units.group(1)\n",
    "    flare = re.search(r\"(NO\\s*FLARE|FLARE\\s*ON|FLARE\\s*\\S+)\", ops_text, re.IGNORECASE)\n",
    "    if flare:\n",
    "        ops_data[\"GAS\"][\"Flare\"] = flare.group(1)\n",
    "    mw_match = re.search(r\"MW\\s*IN\\s*([\\d.+]+)\\s*PPG\\s*/\\s*OUT\\s*([\\d.+]+)\\s*PPG\", ops_text, re.IGNORECASE)\n",
    "    if mw_match:\n",
    "        ops_data[\"MW\"][\"In\"] = mw_match.group(1)\n",
    "        ops_data[\"MW\"][\"Out\"] = mw_match.group(2)\n",
    "    header_match = re.search(r\".*MW\\s*IN\\s*[\\d.+]+\\s*PPG\\s*/\\s*OUT\\s*[\\d.+]+\\s*PPG\\.\", ops_text, re.IGNORECASE)\n",
    "    if header_match:\n",
    "        residual = ops_text[header_match.end():]\n",
    "    else:\n",
    "        residual = ops_text\n",
    "    segments = re.split(r'(?=\\*\\*\\*)', residual)\n",
    "    obs_list = []\n",
    "    for seg in segments:\n",
    "        seg = seg.strip()\n",
    "        if not seg:\n",
    "            continue\n",
    "        if not seg.startswith('***'):\n",
    "            parts = [p.strip() for p in seg.split('.') if p.strip()]\n",
    "            obs_list.extend(parts)\n",
    "        else:\n",
    "            obs_list.append(seg)\n",
    "    obs_list = [o.lstrip('* ').strip() for o in obs_list]\n",
    "    clean_obs = [o for o in obs_list if \"TARGET\" not in o.upper()]\n",
    "    targets = [o for o in obs_list if \"TARGET\" in o.upper()]\n",
    "    targets = [t.lstrip('* ').strip() for t in targets]\n",
    "    ops_data[\"Observations\"] = clean_obs\n",
    "    ops_data[\"Targets\"] = targets\n",
    "    return ops_data\n",
    "\n",
    "def parse_row_text(row_text):\n",
    "    clean_text = \" \".join(row_text.split())\n",
    "    if \"Daily Hrs\" in clean_text:\n",
    "        pattern = r\"Daily Hrs\\s+(\\S+)\\s+Daily NPT Hrs\\s*(\\S*)\\s+Total Job NPT Hours\\s+(\\S+)\"\n",
    "        m = re.search(pattern, clean_text, re.IGNORECASE)\n",
    "        if m:\n",
    "            return {\n",
    "                \"Daily Summary\": {\n",
    "                    \"Daily Hrs\": m.group(1),\n",
    "                    \"Daily NPT Hrs\": m.group(2),\n",
    "                    \"Total Job NPT Hours\": m.group(3)\n",
    "                }\n",
    "            }\n",
    "        else:\n",
    "            logger.warning(f\"Daily summary row detected but could not parse: {clean_text}\")\n",
    "            return None\n",
    "    tokens = clean_text.split()\n",
    "    if not tokens or not re.match(r\"\\d{2}:\\d{2}\", tokens[0]):\n",
    "        logger.info(f\"Skipping header or invalid row: {clean_text}\")\n",
    "        return None\n",
    "    if len(tokens) < 8:\n",
    "        logger.warning(f\"Row does not have enough tokens: {clean_text}\")\n",
    "        return None\n",
    "    from_time = tokens[0]\n",
    "    to_time = tokens[1]\n",
    "    hours = tokens[2]\n",
    "    depth_start = tokens[3]\n",
    "    depth_end = tokens[4]\n",
    "    header_rest = \" \".join(tokens[5:])\n",
    "    m = re.search(r\"^(?P<phase>.+?)\\s+(?P<activity>DR[-]?Drilling)\\s+(?P<ops>.*)$\", header_rest, re.IGNORECASE)\n",
    "    if m:\n",
    "        phase = m.group(\"phase\")\n",
    "        activity = m.group(\"activity\")\n",
    "        ops = m.group(\"ops\")\n",
    "    else:\n",
    "        phase = tokens[5]\n",
    "        activity = tokens[6] if len(tokens) > 6 else \"\"\n",
    "        ops = \" \".join(tokens[7:]) if len(tokens) > 7 else \"\"\n",
    "    return {\n",
    "        \"From\": from_time,\n",
    "        \"To\": to_time,\n",
    "        \"Hours\": hours,\n",
    "        \"Depth Start\": depth_start,\n",
    "        \"Depth End\": depth_end,\n",
    "        \"Phase\": phase,\n",
    "        \"Activity\": activity,\n",
    "        \"Operations Description\": parse_operations_description(ops)\n",
    "    }\n",
    "\n",
    "def parse_all_rows_from_text(full_text):\n",
    "    if re.search(r\"\\d{2}:\\d{2}\\s+\\d{2}:\\d{2}\", full_text):\n",
    "        row_chunks = re.split(r\"(?=\\d{2}:\\d{2}\\s+\\d{2}:\\d{2})\", full_text)\n",
    "        rows = []\n",
    "        for chunk in row_chunks:\n",
    "            chunk = chunk.strip()\n",
    "            if not chunk:\n",
    "                continue\n",
    "            row = parse_row_text(chunk)\n",
    "            if row:\n",
    "                rows.append(row)\n",
    "        return rows\n",
    "    else:\n",
    "        fallback_row = {\n",
    "            \"From\": \"\",\n",
    "            \"To\": \"\",\n",
    "            \"Hours\": \"\",\n",
    "            \"Depth Start\": \"\",\n",
    "            \"Depth End\": \"\",\n",
    "            \"Phase\": \"\",\n",
    "            \"Activity\": \"\",\n",
    "            \"Operations Description\": parse_operations_description(full_text)\n",
    "        }\n",
    "        return [fallback_row]\n",
    "\n",
    "def parse_all_rows_from_ocr_groups(roi_results):\n",
    "    rows = []\n",
    "    groups = group_ocr_rows(roi_results, y_threshold=20)\n",
    "    for group in groups:\n",
    "        group_sorted = sorted(group, key=lambda r: r[0])\n",
    "        row_text = \" \".join([text for (x, y, w, h, text) in group_sorted])\n",
    "        if any(kw in row_text.upper() for kw in [\"TIME PERIOD\", \"FROM TO\", \"DEPTH PHASE\", \"OPERATIONS DESCRIPTION\"]):\n",
    "            continue\n",
    "        parsed_row = parse_row_text(row_text)\n",
    "        if parsed_row:\n",
    "            rows.append(parsed_row)\n",
    "    return rows\n",
    "\n",
    "def main_time_breakdown_process():\n",
    "    img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_14.png\"\n",
    "    try:\n",
    "        img = safe_read_image(img_path)\n",
    "        logger.info(\"Time Breakdown image loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        return\n",
    "    thresh_img = preprocess_image(img, debug=True)\n",
    "    rois = detect_text_regions_tb(thresh_img, debug=True)\n",
    "    roi_ocr_results = perform_ocr_on_rois_tb(img, rois, debug=True)\n",
    "    time_breakdown_list = parse_all_rows_from_ocr_groups(roi_ocr_results)\n",
    "    if not time_breakdown_list:\n",
    "        logger.warning(\"No rows detected from ROI grouping. Falling back to full-image OCR.\")\n",
    "        full_text = pytesseract.image_to_string(thresh_img, config=\"--psm 6\")\n",
    "        time_breakdown_list = parse_all_rows_from_text(full_text)\n",
    "    if not time_breakdown_list:\n",
    "        logger.error(\"No rows were detected. Please check the OCR output and header format.\")\n",
    "        return\n",
    "    final_output = {\"TIME BREAKDOWN\": time_breakdown_list}\n",
    "    logger.info(\"===== FINAL TIME BREAKDOWN DATA =====\")\n",
    "    logger.info(json.dumps(final_output, indent=4))\n",
    "    df = pd.json_normalize(final_output[\"TIME BREAKDOWN\"])\n",
    "    print(\"----- Extracted Time Breakdown DataFrame -----\")\n",
    "    print(df)\n",
    "    output_folder = \"dbfs:/mnt/mini-proj-dd/final_time_breakdown_results\"\n",
    "    local_folder = output_folder.replace(\"dbfs:\", \"/dbfs\")\n",
    "    os.makedirs(local_folder, exist_ok=True)\n",
    "    out_json = os.path.join(local_folder, \"time_breakdown_data.json\")\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(final_output, f, indent=4)\n",
    "    logger.info(f\"Time Breakdown JSON saved to {out_json}\")\n",
    "    out_csv = os.path.join(local_folder, \"time_breakdown_data.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    logger.info(f\"Time Breakdown CSV saved to {out_csv}\")\n",
    "    return final_output, df\n",
    "\n",
    "def merge_time_breakdown_data(main_data, continuation_data):\n",
    "    return main_data + continuation_data\n",
    "\n",
    "def process_time_breakdown_image(img_path, debug=False):\n",
    "    img = safe_read_image(img_path)\n",
    "    thresh_img = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions_tb(thresh_img, debug=debug)\n",
    "    roi_ocr_results = perform_ocr_on_rois_tb(img, rois, debug=debug)\n",
    "    rows = parse_all_rows_from_ocr_groups(roi_ocr_results)\n",
    "    if not rows:\n",
    "        full_text = pytesseract.image_to_string(thresh_img, config=\"--psm 6\")\n",
    "        rows = parse_all_rows_from_text(full_text)\n",
    "    return rows\n",
    "\n",
    "def process_pumps_data(dummy_arg=None):\n",
    "    pumps_img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_12.png\"\n",
    "    pil_img = read_pil_image(pumps_img_path)\n",
    "    ocr_text = pytesseract.image_to_string(pil_img)\n",
    "    pumps = [{\"Number\": \"1\", \"Model\": \"BOMCO\", \"Type\": \"TRIPLEX\", \"Efficiency\": \"95\"}]\n",
    "    circ_rates = []\n",
    "    final_data = {\"Pumps\": pumps, \"DrillingCircRates\": circ_rates}\n",
    "    df = pd.DataFrame(pumps)\n",
    "    return final_data, df\n",
    "\n",
    "\n",
    "def extract_bha_data_process(dummy_arg=None):\n",
    "    bha_img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_11.png\"\n",
    "    return process_bha(bha_img_path, debug=False)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 9) process Runner Helper Function\n",
    "# ---------------------------------------------------------------------\n",
    "def run_process(name, pipe, debug=False):\n",
    "    logger.info(f\"Running process: {name}\")\n",
    "    try:\n",
    "        output_json, output_df = pipe[\"func\"](None)\n",
    "        output_folder = dbfs_to_local_path(\"dbfs:/mnt/mini-proj-dd/final_results\")\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        json_path = os.path.join(output_folder, pipe[\"json\"])\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(output_json, f, indent=4)\n",
    "        logger.info(f\"{name} JSON data saved to {json_path}\")\n",
    "        csv_path = os.path.join(output_folder, pipe[\"csv\"])\n",
    "        output_df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"{name} CSV data saved to {csv_path}\")\n",
    "        print(f\"\\n--- {name.upper()} JSON Output ---\")\n",
    "        print(json.dumps(output_json, indent=4))\n",
    "        print(f\"\\n--- {name.upper()} DataFrame ---\")\n",
    "        print(output_df)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in process '{name}': {e}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 10) Main process Function with processs Dictionary\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    debug = False  # Set True for detailed logging and image display\n",
    "    processs = {\n",
    "        \"cost_data\": {\n",
    "            \"func\": process_cost_data,\n",
    "            \"csv\": \"cost_data.csv\",\n",
    "            \"json\": \"cost_data.json\"\n",
    "        },\n",
    "        \"well_job\": {\n",
    "            \"func\": lambda d: process_well_job_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\", d),\n",
    "            \"csv\": \"well_job_data.csv\",\n",
    "            \"json\": \"well_job_data.json\"\n",
    "        },\n",
    "        \"obs_int\": {\n",
    "            \"func\": lambda d: process_obs_int(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\", d),\n",
    "            \"csv\": \"obs_int_data.csv\",\n",
    "            \"json\": \"obs_int_data.json\"\n",
    "        },\n",
    "        \"bop\": {\n",
    "            \"func\": lambda d: process_bop(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\", debug),\n",
    "            \"csv\": \"bop_data.csv\",\n",
    "            \"json\": \"bop_data.json\"\n",
    "        },\n",
    "        \"dir_info\": {\n",
    "            \"func\": lambda d: process_dir_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\", d),\n",
    "            \"csv\": \"dir_info_data.csv\",\n",
    "            \"json\": \"dir_info_data.json\"\n",
    "        },\n",
    "        \"survey\": {\n",
    "            \"func\": lambda d: process_survey(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_4.png\", debug),\n",
    "            \"csv\": \"survey_data.csv\",\n",
    "            \"json\": \"survey_data.json\"\n",
    "        },\n",
    "        \"casing\": {\n",
    "            \"func\": process_casing_data,\n",
    "            \"csv\": \"casing_data.csv\",\n",
    "            \"json\": \"casing_data.json\"\n",
    "        },\n",
    "        \"consumables\": {\n",
    "            \"func\": process_consumables_data,\n",
    "            \"csv\": \"consumables_data.csv\",\n",
    "            \"json\": \"consumables_data.json\"\n",
    "        },\n",
    "        \"mud\": {\n",
    "            \"func\": process_mud_data,\n",
    "            \"csv\": \"mud_data.csv\",\n",
    "            \"json\": \"mud_data.json\"\n",
    "        },\n",
    "        \"bha\": {\n",
    "            \"func\": process_bha_data,\n",
    "            \"csv\": \"bha_data.csv\",\n",
    "            \"json\": \"bha_data.json\"\n",
    "        },\n",
    "        \"pumps\": {\n",
    "            \"func\": process_pumps_data,\n",
    "            \"csv\": \"pumps_data.csv\",\n",
    "            \"json\": \"pumps_data.json\"\n",
    "        },\n",
    "        \"personnel\": {\n",
    "            \"func\": lambda d: process_personnel(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_9.png\", debug),\n",
    "            \"csv\": \"personnel_data.csv\",\n",
    "            \"json\": \"personnel_data.json\"\n",
    "        },\n",
    "        \"time_breakdown\": {\n",
    "            \"func\": lambda d: main_time_breakdown_process() if main_time_breakdown_process() else ({\"TIME BREAKDOWN\": []}, pd.DataFrame()),\n",
    "            \"csv\": \"time_breakdown_data.csv\",\n",
    "            \"json\": \"time_breakdown_data.json\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, pipe in processs.items():\n",
    "        run_process(name, pipe, debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be878dee-f030-4137-b805-24ef86720b16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:DAILY DRILLING REPORT processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:WELL/JOB INFORMATION processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:MUD processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:SURVEY DATA processing failed: name 'show_image' is not defined\nERROR:__main__:DIR INFO processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:DRILL BITS processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:CASING processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:BOP processing failed: name 'read_pil_image' is not defined\nERROR:__main__:PERSONNEL processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:DAILY NUMBERS: OBSERVATION & INTERVENTION processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:BHA processing failed: extract_bha_data_process() takes from 0 to 1 positional arguments but 2 were given\nERROR:__main__:PUMPS processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:COST DATA processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:TIME BREAKDOWN processing failed: main_time_breakdown_process() takes 0 positional arguments but 1 was given\nERROR:__main__:CONSUMABLES processing failed: cannot unpack non-iterable NoneType object\n"
     ]
    }
   ],
   "source": [
    "import os# ---------------------------------------------------------------------\n",
    "# Preliminary Check: Ensure Required Functions are Defined\n",
    "# ---------------------------------------------------------------------\n",
    "required_functions = [\n",
    "    \"process_daily_drilling_report\",\n",
    "    \"process_well_job_info\",\n",
    "    \"process_mud\",\n",
    "    \"process_survey\",\n",
    "    \"process_dir_info\",\n",
    "    \"process_drill_bits\",\n",
    "    \"process_casing_data\",\n",
    "    \"process_bop\",\n",
    "    \"process_personnel\",\n",
    "    \"process_obs_int\",\n",
    "    \"extract_bha_data_process\",\n",
    "    \"process_pumps\",\n",
    "    \"process_cost_data\",\n",
    "    \"main_time_breakdown_process\",\n",
    "    \"process_consumables_data\"\n",
    "]\n",
    "\n",
    "for func_name in required_functions:\n",
    "    if func_name not in globals():\n",
    "        raise ImportError(f\"{func_name} is not defined. Please define or import it before running this script.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logging and Utility Setup\n",
    "# ---------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def dbfs_to_local_path(dbfs_path):\n",
    "    \"\"\"\n",
    "    Convert a DBFS path to a local filesystem path.\n",
    "    Avoids double conversion if already converted.\n",
    "    \"\"\"\n",
    "    if dbfs_path.startswith(\"dbfs:\"):\n",
    "        return dbfs_path.replace(\"dbfs:\", \"/mnt\")\n",
    "    return dbfs_path\n",
    "\n",
    "def save_output(section_name, data_json, df, output_folder):\n",
    "    \"\"\"\n",
    "    Save the JSON and CSV outputs for a given process section.\n",
    "    Filenames are generated based on the section name.\n",
    "    \"\"\"\n",
    "    file_base = section_name.lower().replace(\" \", \"_\").replace(\":\", \"\").replace(\"&\", \"and\")\n",
    "    json_path = os.path.join(output_folder, f\"{file_base}_data.json\")\n",
    "    csv_path = os.path.join(output_folder, f\"{file_base}_data.csv\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data_json, f, indent=4)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    logger.info(f\"{section_name} data saved to {json_path} and {csv_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main process Execution\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    debug = True  # Set to False for less verbose logging\n",
    "\n",
    "    # Define DBFS image paths for sections that require an image.\n",
    "    image_paths = {\n",
    "        \"DAILY DRILLING REPORT\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_1.png\",\n",
    "        \"WELL/JOB INFORMATION\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\",\n",
    "        \"MUD\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png\",\n",
    "        \"SURVEY DATA\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_4.png\",\n",
    "        \"DIR INFO\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\",\n",
    "        \"DRILL BITS\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_6.png\",\n",
    "        \"BOP\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\",\n",
    "        \"PERSONNEL\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_9.png\",\n",
    "        \"DAILY NUMBERS: OBSERVATION & INTERVENTION\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\",\n",
    "        \"BHA\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_11.png\",\n",
    "        \"PUMPS\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_12.png\"\n",
    "    }\n",
    "\n",
    "    # Define the output folder and create it if it doesn't exist.\n",
    "    output_folder = dbfs_to_local_path(\"dbfs:/mnt/mini-proj-dd/final_results\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Define the processs in the specified order.\n",
    "    # For sections that require an image, the corresponding DBFS image path is provided.\n",
    "    # For sections that do not require an image, pass None.\n",
    "    processs = [\n",
    "        (\"DAILY DRILLING REPORT\", process_daily_drilling_report, image_paths.get(\"DAILY DRILLING REPORT\")),\n",
    "        (\"WELL/JOB INFORMATION\", process_well_job_info, image_paths.get(\"WELL/JOB INFORMATION\")),\n",
    "        (\"MUD\", process_mud, image_paths.get(\"MUD\")),\n",
    "        (\"SURVEY DATA\", process_survey, image_paths.get(\"SURVEY DATA\")),\n",
    "        (\"DIR INFO\", process_dir_info, image_paths.get(\"DIR INFO\")),\n",
    "        (\"DRILL BITS\", process_drill_bits, image_paths.get(\"DRILL BITS\")),\n",
    "        (\"CASING\", process_casing_data, None),\n",
    "        (\"BOP\", process_bop, image_paths.get(\"BOP\")),\n",
    "        (\"PERSONNEL\", process_personnel, image_paths.get(\"PERSONNEL\")),\n",
    "        (\"DAILY NUMBERS: OBSERVATION & INTERVENTION\", process_obs_int, image_paths.get(\"DAILY NUMBERS: OBSERVATION & INTERVENTION\")),\n",
    "        (\"BHA\", extract_bha_data_process, image_paths.get(\"BHA\")),\n",
    "        (\"PUMPS\", process_pumps, image_paths.get(\"PUMPS\")),\n",
    "        (\"COST DATA\", process_cost_data, None),\n",
    "        (\"TIME BREAKDOWN\", main_time_breakdown_process, None),\n",
    "        (\"CONSUMABLES\", process_consumables_data, None)\n",
    "    ]\n",
    "\n",
    "    # Process each process sequentially.\n",
    "    for section_name, func, image_path in processs:\n",
    "        try:\n",
    "            logger.info(f\"Processing section: {section_name}\")\n",
    "            # If an image path is provided, call the function with the image path and debug flag.\n",
    "            if image_path:\n",
    "                data_json, df = func(image_path, debug)\n",
    "            else:\n",
    "                data_json, df = func(debug)\n",
    "            logger.info(f\"{section_name} output:\\n{json.dumps(data_json, indent=4)}\")\n",
    "            save_output(section_name, data_json, df, output_folder)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{section_name} processing failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34300ccb-0fa6-409f-b664-999e1996a4e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:DAILY DRILLING REPORT processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:WELL/JOB INFORMATION processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:MUD processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:SURVEY DATA processing failed: name 'show_image' is not defined\nERROR:__main__:DIR INFO processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:DRILL BITS processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:CASING processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:BOP processing failed: name 'read_pil_image' is not defined\nERROR:__main__:PERSONNEL processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:DAILY NUMBERS: OBSERVATION & INTERVENTION processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:BHA processing failed: extract_bha_data_process() takes from 0 to 1 positional arguments but 2 were given\nERROR:__main__:PUMPS processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:COST DATA processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:TIME BREAKDOWN processing failed: main_time_breakdown_process() takes 0 positional arguments but 1 was given\nERROR:__main__:CONSUMABLES processing failed: cannot unpack non-iterable NoneType object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logging and Utility Setup\n",
    "# ---------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def dbfs_to_local_path(dbfs_path):\n",
    "    \"\"\"\n",
    "    Convert a DBFS path to a local filesystem path.\n",
    "    Avoids double conversion if already converted.\n",
    "    \"\"\"\n",
    "    if dbfs_path.startswith(\"dbfs:\"):\n",
    "        return dbfs_path.replace(\"dbfs:\", \"/mnt\")\n",
    "    return dbfs_path\n",
    "\n",
    "def save_output(section_name, data_json, df, output_folder):\n",
    "    \"\"\"\n",
    "    Save the JSON and CSV outputs for a given process section.\n",
    "    Filenames are generated based on the section name.\n",
    "    \"\"\"\n",
    "    # Remove or replace characters to form a valid file base name\n",
    "    file_base = section_name.lower().replace(\" \", \"_\").replace(\":\", \"\").replace(\"&\", \"and\")\n",
    "    json_path = os.path.join(output_folder, f\"{file_base}_data.json\")\n",
    "    csv_path = os.path.join(output_folder, f\"{file_base}_data.csv\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data_json, f, indent=4)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    logger.info(f\"{section_name} data saved to {json_path} and {csv_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main process Execution\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    debug = True  # Set to False if less verbose logging is preferred\n",
    "\n",
    "    # Define DBFS image paths for sections that require an image.\n",
    "    # Adjust these paths to match your environment.\n",
    "    image_paths = {\n",
    "        \"DAILY DRILLING REPORT\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_1.png\",\n",
    "        \"WELL/JOB INFORMATION\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\",\n",
    "        \"MUD\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png\",\n",
    "        \"SURVEY DATA\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_4.png\",\n",
    "        \"DIR INFO\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\",\n",
    "        \"DRILL BITS\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_6.png\",\n",
    "        \"BOP\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\",\n",
    "        \"PERSONNEL\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_9.png\",\n",
    "        \"DAILY NUMBERS: OBSERVATION & INTERVENTION\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\",\n",
    "        \"BHA\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_11.png\",\n",
    "        \"PUMPS\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_12.png\"\n",
    "    }\n",
    "\n",
    "    # Define the output folder and create it if it doesn't exist.\n",
    "    output_folder = dbfs_to_local_path(\"dbfs:/mnt/mini-proj-dd/final_results\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Define the processs in the specified order.\n",
    "    # For sections that require an image, the corresponding DBFS image path is provided.\n",
    "    # For sections that do not require an image, pass None.\n",
    "    processs = [\n",
    "        (\"DAILY DRILLING REPORT\",       process_daily_drilling_report,       image_paths.get(\"DAILY DRILLING REPORT\")),\n",
    "        (\"WELL/JOB INFORMATION\",         process_well_job_info,               image_paths.get(\"WELL/JOB INFORMATION\")),\n",
    "        (\"MUD\",                        process_mud,                         image_paths.get(\"MUD\")),\n",
    "        (\"SURVEY DATA\",                process_survey,                      image_paths.get(\"SURVEY DATA\")),\n",
    "        (\"DIR INFO\",                   process_dir_info,                    image_paths.get(\"DIR INFO\")),\n",
    "        (\"DRILL BITS\",                 process_drill_bits,                  image_paths.get(\"DRILL BITS\")),\n",
    "        (\"CASING\",                     process_casing_data,                None),\n",
    "        (\"BOP\",                        process_bop,                         image_paths.get(\"BOP\")),\n",
    "        (\"PERSONNEL\",                  process_personnel,                   image_paths.get(\"PERSONNEL\")),\n",
    "        (\"DAILY NUMBERS: OBSERVATION & INTERVENTION\", process_obs_int,         image_paths.get(\"DAILY NUMBERS: OBSERVATION & INTERVENTION\")),\n",
    "        (\"BHA\",                        extract_bha_data_process,           image_paths.get(\"BHA\")),\n",
    "        (\"PUMPS\",                      process_pumps,                       image_paths.get(\"PUMPS\")),\n",
    "        (\"COST DATA\",                  process_cost_data,                  None),\n",
    "        (\"TIME BREAKDOWN\",             main_time_breakdown_process,        None),\n",
    "        (\"CONSUMABLES\",                process_consumables_data,           None)\n",
    "    ]\n",
    "\n",
    "    # Process each process sequentially.\n",
    "    for section_name, func, image_path in processs:\n",
    "        try:\n",
    "            logger.info(f\"Processing section: {section_name}\")\n",
    "            # Call the function with the image path if provided; otherwise, call with debug flag only.\n",
    "            if image_path:\n",
    "                data_json, df = func(image_path, debug)\n",
    "            else:\n",
    "                data_json, df = func(debug)\n",
    "            logger.info(f\"{section_name} output:\\n{json.dumps(data_json, indent=4)}\")\n",
    "            save_output(section_name, data_json, df, output_folder)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{section_name} processing failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cfc7d3f-af3b-4fe6-aebc-29d23d115ddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:DAILY DRILLING REPORT processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:WELL/JOB INFORMATION processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:MUD processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:SURVEY DATA processing failed: name 'show_image' is not defined\nERROR:__main__:DIR INFO processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:DRILL BITS processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:CASING processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:BOP processing failed: name 'read_pil_image' is not defined\nERROR:__main__:PERSONNEL processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:DAILY NUMBERS: OBSERVATION & INTERVENTION processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:BHA processing failed: extract_bha_data_process() takes from 0 to 1 positional arguments but 2 were given\nERROR:__main__:PUMPS processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:COST DATA processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:TIME BREAKDOWN processing failed: main_time_breakdown_process() takes 0 positional arguments but 1 was given\nERROR:__main__:CONSUMABLES processing failed: cannot unpack non-iterable NoneType object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Ensure that the following functions are defined in your environment:\n",
    "# process_daily_drilling_report, process_well_job_info, process_mud,\n",
    "# process_survey, process_dir_info, process_drill_bits, process_casing_data,\n",
    "# process_bop, process_personnel, process_obs_int, extract_bha_data_process,\n",
    "# process_pumps, process_cost_data, main_time_breakdown_process,\n",
    "# process_consumables_data\n",
    "#\n",
    "# For example, you might have:\n",
    "# from your_module import process_daily_drilling_report, process_well_job_info, ... \n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logging and Utility Setup\n",
    "# ---------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def dbfs_to_local_path(dbfs_path):\n",
    "    \"\"\"\n",
    "    Convert a DBFS path to a local filesystem path.\n",
    "    Avoids double conversion if already converted.\n",
    "    \"\"\"\n",
    "    if dbfs_path.startswith(\"dbfs:\"):\n",
    "        return dbfs_path.replace(\"dbfs:\", \"/mnt\")\n",
    "    return dbfs_path\n",
    "\n",
    "def save_output(section_name, data_json, df, output_folder):\n",
    "    \"\"\"\n",
    "    Save the JSON and CSV outputs for a given process section.\n",
    "    Filenames are generated based on the section name.\n",
    "    \"\"\"\n",
    "    file_base = section_name.lower().replace(\" \", \"_\").replace(\":\", \"\").replace(\"&\", \"and\")\n",
    "    json_path = os.path.join(output_folder, f\"{file_base}_data.json\")\n",
    "    csv_path = os.path.join(output_folder, f\"{file_base}_data.csv\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data_json, f, indent=4)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    logger.info(f\"{section_name} data saved to {json_path} and {csv_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main process Execution\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    debug = True  # Set to False for less verbose logging\n",
    "\n",
    "    # Define DBFS image paths for sections that require an image.\n",
    "    image_paths = {\n",
    "        \"DAILY DRILLING REPORT\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_1.png\",\n",
    "        \"WELL/JOB INFORMATION\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\",\n",
    "        \"MUD\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png\",\n",
    "        \"SURVEY DATA\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_4.png\",\n",
    "        \"DIR INFO\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\",\n",
    "        \"DRILL BITS\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_6.png\",\n",
    "        \"BOP\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\",\n",
    "        \"PERSONNEL\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_9.png\",\n",
    "        \"DAILY NUMBERS: OBSERVATION & INTERVENTION\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\",\n",
    "        \"BHA\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_11.png\",\n",
    "        \"PUMPS\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_12.png\"\n",
    "    }\n",
    "\n",
    "    # Define the output folder and create it if it doesn't exist.\n",
    "    output_folder = dbfs_to_local_path(\"dbfs:/mnt/mini-proj-dd/final_results\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Define the processs in the specified order.\n",
    "    # For sections that require an image, the corresponding DBFS image path is provided.\n",
    "    # For sections that do not require an image, pass None.\n",
    "    processs = [\n",
    "        (\"DAILY DRILLING REPORT\", process_daily_drilling_report, image_paths.get(\"DAILY DRILLING REPORT\")),\n",
    "        (\"WELL/JOB INFORMATION\", process_well_job_info, image_paths.get(\"WELL/JOB INFORMATION\")),\n",
    "        (\"MUD\", process_mud, image_paths.get(\"MUD\")),\n",
    "        (\"SURVEY DATA\", process_survey, image_paths.get(\"SURVEY DATA\")),\n",
    "        (\"DIR INFO\", process_dir_info, image_paths.get(\"DIR INFO\")),\n",
    "        (\"DRILL BITS\", process_drill_bits, image_paths.get(\"DRILL BITS\")),\n",
    "        (\"CASING\", process_casing_data, None),\n",
    "        (\"BOP\", process_bop, image_paths.get(\"BOP\")),\n",
    "        (\"PERSONNEL\", process_personnel, image_paths.get(\"PERSONNEL\")),\n",
    "        (\"DAILY NUMBERS: OBSERVATION & INTERVENTION\", process_obs_int, image_paths.get(\"DAILY NUMBERS: OBSERVATION & INTERVENTION\")),\n",
    "        (\"BHA\", extract_bha_data_process, image_paths.get(\"BHA\")),\n",
    "        (\"PUMPS\", process_pumps, image_paths.get(\"PUMPS\")),\n",
    "        (\"COST DATA\", process_cost_data, None),\n",
    "        (\"TIME BREAKDOWN\", main_time_breakdown_process, None),\n",
    "        (\"CONSUMABLES\", process_consumables_data, None)\n",
    "    ]\n",
    "\n",
    "    # Process each process sequentially.\n",
    "    for section_name, func, image_path in processs:\n",
    "        try:\n",
    "            logger.info(f\"Processing section: {section_name}\")\n",
    "            # If an image path is provided, call the function with the image path and debug flag.\n",
    "            if image_path:\n",
    "                data_json, df = func(image_path, debug)\n",
    "            else:\n",
    "                data_json, df = func(debug)\n",
    "            logger.info(f\"{section_name} output:\\n{json.dumps(data_json, indent=4)}\")\n",
    "            save_output(section_name, data_json, df, output_folder)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{section_name} processing failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6835d265-cd25-4988-995f-ddb3bd2f8271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:DAILY DRILLING REPORT processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:WELL/JOB INFORMATION processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:MUD processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:SURVEY DATA processing failed: name 'show_image' is not defined\nERROR:__main__:DIR INFO processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:DRILL BITS processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:CASING processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:BOP processing failed: name 'read_pil_image' is not defined\nERROR:__main__:PERSONNEL processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:DAILY NUMBERS: OBSERVATION & INTERVENTION processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:BHA processing failed: extract_bha_data_process() takes from 0 to 1 positional arguments but 2 were given\nERROR:__main__:PUMPS processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:COST DATA processing failed: cannot unpack non-iterable NoneType object\nERROR:__main__:TIME BREAKDOWN processing failed: main_time_breakdown_process() takes 0 positional arguments but 1 was given\nERROR:__main__:CONSUMABLES processing failed: cannot unpack non-iterable NoneType object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logging and Utility Setup\n",
    "# ---------------------------------------------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def dbfs_to_local_path(dbfs_path):\n",
    "    \"\"\"\n",
    "    Convert a DBFS path to a local filesystem path.\n",
    "    Avoid double conversion if already converted.\n",
    "    \"\"\"\n",
    "    if dbfs_path.startswith(\"dbfs:\"):\n",
    "        return dbfs_path.replace(\"dbfs:\", \"/mnt\")\n",
    "    return dbfs_path\n",
    "\n",
    "def save_output(section_name, data_json, df, output_folder):\n",
    "    \"\"\"\n",
    "    Save the JSON and CSV output for a given process section.\n",
    "    \"\"\"\n",
    "    json_path = os.path.join(output_folder, f\"{section_name.lower().replace(' ', '_')}_data.json\")\n",
    "    csv_path  = os.path.join(output_folder, f\"{section_name.lower().replace(' ', '_')}_data.csv\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(data_json, f, indent=4)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    logger.info(f\"{section_name} data saved to {json_path} and {csv_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main process Execution\n",
    "# ---------------------------------------------------------------------\n",
    "def main():\n",
    "    debug = True  # Set to False for less verbose logging\n",
    "    \n",
    "    # Define image paths for sections that require an image.\n",
    "    # Adjust these paths as needed.\n",
    "    paths = {\n",
    "        \"DAILY DRILLING REPORT\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_DDR.png\",\n",
    "        \"WELL/JOB INFORMATION\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\",\n",
    "        \"MUD\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png\",\n",
    "        \"SURVEY DATA\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_4.png\",\n",
    "        \"DIR INFO\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\",\n",
    "        \"DRILL BITS\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_6.png\",\n",
    "        \"BOP\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\",\n",
    "        \"PERSONNEL\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_9.png\",\n",
    "        \"DAILY NUMBERS: OBSERVATION & INTERVENTION\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\",\n",
    "        \"BHA\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_11.png\",\n",
    "        \"PUMPS\": \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_12.png\"\n",
    "    }\n",
    "    \n",
    "    # Define the output folder and create it if it doesn't exist.\n",
    "    output_folder = dbfs_to_local_path(\"dbfs:/mnt/mini-proj-dd/final_results\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # List of processs in the specified order.\n",
    "    # For sections that do not require an image, pass None.\n",
    "    processs = [\n",
    "        (\"DAILY DRILLING REPORT\",       process_daily_drilling_report,       paths.get(\"DAILY DRILLING REPORT\")),\n",
    "        (\"WELL/JOB INFORMATION\",         process_well_job_info,               paths.get(\"WELL/JOB INFORMATION\")),\n",
    "        (\"MUD\",                        process_mud,                         paths.get(\"MUD\")),\n",
    "        (\"SURVEY DATA\",                process_survey,                      paths.get(\"SURVEY DATA\")),\n",
    "        (\"DIR INFO\",                   process_dir_info,                    paths.get(\"DIR INFO\")),\n",
    "        (\"DRILL BITS\",                 process_drill_bits,                  paths.get(\"DRILL BITS\")),\n",
    "        (\"CASING\",                     process_casing_data,                None),\n",
    "        (\"BOP\",                        process_bop,                         paths.get(\"BOP\")),\n",
    "        (\"PERSONNEL\",                  process_personnel,                   paths.get(\"PERSONNEL\")),\n",
    "        (\"DAILY NUMBERS: OBSERVATION & INTERVENTION\", process_obs_int,         paths.get(\"DAILY NUMBERS: OBSERVATION & INTERVENTION\")),\n",
    "        (\"BHA\",                        extract_bha_data_process,           paths.get(\"BHA\")),\n",
    "        (\"PUMPS\",                      process_pumps,                       paths.get(\"PUMPS\")),\n",
    "        (\"COST DATA\",                  process_cost_data,                  None),\n",
    "        (\"TIME BREAKDOWN\",             main_time_breakdown_process,        None),\n",
    "        (\"CONSUMABLES\",                process_consumables_data,           None)\n",
    "    ]\n",
    "    \n",
    "    # Process each process sequentially.\n",
    "    for section_name, func, image_path in processs:\n",
    "        try:\n",
    "            # Call the function with the image path if provided, else pass None.\n",
    "            if image_path:\n",
    "                data_json, df = func(image_path, debug)\n",
    "            else:\n",
    "                data_json, df = func(debug)\n",
    "            logger.info(f\"===== {section_name.upper()} =====\")\n",
    "            logger.info(json.dumps(data_json, indent=4))\n",
    "            save_output(section_name, data_json, df, output_folder)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"{section_name} processing failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904bfb1e-8ab9-4b8a-bc4b-320c5142abcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# [\n",
    "#   \"DAILY DRILLING REPORT\",\n",
    "#   \"WELL/JOB INFORMATION\",\n",
    "#   \"MUD\",\n",
    "#   \"SURVEY DATA\",\n",
    "#   \"DIR INFO\",\n",
    "#   \"DRILL BITS\",\n",
    "#   \"CASING\",\n",
    "#   \"BOP\",\n",
    "#   \"PERSONNEL\",\n",
    "#   \"DAILY NUMBERS: OBSERVATION & INTERVENTION\",\n",
    "#   \"BHA\",\n",
    "#   \"PUMPS\",\n",
    "#   \"COST DATA\",\n",
    "#   \"TIME BREAKDOWN\",\n",
    "#   \"CONSUMABLES\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84254171-cbd2-4012-b5a8-e9179b9c0b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7760541951788028,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "preprocess_sections_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
