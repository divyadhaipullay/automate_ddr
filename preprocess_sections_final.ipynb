{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "221c90b2-5321-43a6-b6d4-bb111080a359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "496d7298-d546-4673-aef5-bfcf51863f30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json, os\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6431f3cd-dba3-4c27-bb3a-99b336eba6c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Global Logger & Output Directories\n",
    "# -----------------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"OCRProduction\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "OUTPUT_FOLDER = \"/dbfs/mnt/mini-proj-dd/final_results\"\n",
    "CSV_FOLDER = os.path.join(OUTPUT_FOLDER, \"csv\")\n",
    "JSON_FOLDER = os.path.join(OUTPUT_FOLDER, \"json\")\n",
    "os.makedirs(CSV_FOLDER, exist_ok=True)\n",
    "os.makedirs(JSON_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33043d19-605f-4834-b763-a1a940bed780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Global Logger & Output Directories\n",
    "# -----------------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"OCRProduction\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "OUTPUT_FOLDER = \"/dbfs/mnt/mini-proj-dd/final_results\"\n",
    "CSV_FOLDER = os.path.join(OUTPUT_FOLDER, \"csv\")\n",
    "JSON_FOLDER = os.path.join(OUTPUT_FOLDER, \"json\")\n",
    "os.makedirs(CSV_FOLDER, exist_ok=True)\n",
    "os.makedirs(JSON_FOLDER, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# COMMON UTILITY FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def safe_read_image(img_path):\n",
    "    local_path = img_path if not img_path.startswith(\"dbfs:\") else img_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {local_path}\")\n",
    "    logger.info(f\"Image loaded from {local_path} with shape {img.shape}\")\n",
    "    return img\n",
    "\n",
    "def preprocess_image(img, debug=False):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 15, 9)\n",
    "    if debug:\n",
    "        logger.info(\"Preprocessing completed (grayscale and threshold applied).\")\n",
    "    return thresh\n",
    "\n",
    "def detect_text_regions(thresh_img, debug=False):\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "    rois.sort(key=lambda b: (b[1], b[0]))\n",
    "    if debug:\n",
    "        logger.info(f\"Detected {len(rois)} text regions.\")\n",
    "    return rois\n",
    "\n",
    "def perform_ocr_on_rois(img, rois, debug=False):\n",
    "    results = []\n",
    "    for (x, y, w, h) in rois:\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        text = pytesseract.image_to_string(roi, config=\"--psm 6\").strip() or \"[BLANK]\"\n",
    "        results.append((x, y, w, h, text))\n",
    "        if debug:\n",
    "            logger.info(f\"OCR Box ({x},{y},{w},{h}): {text}\")\n",
    "    return results\n",
    "\n",
    "def perform_ocr(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return pytesseract.image_to_string(gray, config=\"--psm 6\")\n",
    "\n",
    "def extract_key_value_from_text(text, expected_keys):\n",
    "    combined = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "    combined = re.sub(r'\\s+', ' ', combined)\n",
    "    result = {}\n",
    "    for i, key in enumerate(expected_keys):\n",
    "        if i < len(expected_keys) - 1:\n",
    "            next_key = expected_keys[i+1]\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*?)(?=\\s*' + re.escape(next_key) + r'\\s*:|$)'\n",
    "        else:\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*)'\n",
    "        match = re.search(pattern, combined, re.IGNORECASE)\n",
    "        result[key] = match.group(1).strip() if match and match.group(1).strip() else None\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PARSING HELPERS\n",
    "# -----------------------------------------------------------------------------\n",
    "def group_ocr_results(roi_texts, row_tolerance=10):\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for row in rows:\n",
    "        row.sort(key=lambda c: c[0])\n",
    "        line = \" \".join(cell[4] for cell in row)\n",
    "        row_strings.append(line)\n",
    "    return row_strings\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PIPELINE FUNCTIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def pipeline_cost_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"Cost OCR extraction complete.\")\n",
    "    expected_keys = [\n",
    "        \"Drilling AFE Amount\", \"Daily Drilling Cost\", \"Cumulative Drilling Cost\",\n",
    "        \"Cumulative Well Cost\", \"Daily Mud Cost\", \"Cumulative Mud Cost\"\n",
    "    ]\n",
    "    extracted = extract_key_value_from_text(ocr_text, expected_keys)\n",
    "    df = pd.DataFrame(list(extracted.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"COST DataFrame shape: {df.shape}\")\n",
    "    return {\"COST DATA\": extracted}, df\n",
    "\n",
    "def process_well_job_info(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"Well/Job OCR extraction complete.\")\n",
    "    expected_keys = [\n",
    "        \"Well Name\", \"Job Name\", \"Supervisor(s)\", \"Field\", \"Sec/Twn/Rng\", \"Phone\",\n",
    "        \"AFE #\", \"API #\", \"Email\", \"Contractor\", \"Elevation\", \"RKB\",\n",
    "        \"Spud Date\", \"Days from Spud\", \"Days on Loc\", \"MD/TVD\", \"24 Hr Footage\",\n",
    "        \"Present Operations\", \"Activity Planned\"\n",
    "    ]\n",
    "    combined = \" \".join(line.strip() for line in ocr_text.splitlines() if line.strip())\n",
    "    combined = re.sub(r'\\s+', ' ', combined)\n",
    "    result = {}\n",
    "    for i, key in enumerate(expected_keys):\n",
    "        if i < len(expected_keys) - 1:\n",
    "            next_key = expected_keys[i+1]\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*?)(?=\\s*' + re.escape(next_key) + r'\\s*:|$)'\n",
    "        else:\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*)'\n",
    "        match = re.search(pattern, combined, re.IGNORECASE)\n",
    "        result[key] = match.group(1).strip() if match else \"\"\n",
    "    df = pd.DataFrame(list(result.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"WELL/JOB DataFrame shape: {df.shape}\")\n",
    "    return {\"WELL/JOB INFORMATION\": result}, df\n",
    "\n",
    "def process_obs_int(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    header_str = \"daily numbers: observation & intervention\"\n",
    "    all_texts = [t[4] for t in roi_texts]\n",
    "    types_list, numbers_list = [], []\n",
    "    for txt in all_texts:\n",
    "        clean = txt.strip()\n",
    "        if clean.lower() in [header_str, \"number\", \"[blank]\"]:\n",
    "            continue\n",
    "        try:\n",
    "            float(clean)\n",
    "            numbers_list.append(\"\" if clean.lower() == \"[blank]\" else clean)\n",
    "            continue\n",
    "        except ValueError:\n",
    "            pass\n",
    "        if \"\\n\" in clean:\n",
    "            for line in clean.splitlines():\n",
    "                line = line.strip()\n",
    "                if line and line.lower() != \"[blank]\":\n",
    "                    types_list.append(line)\n",
    "        else:\n",
    "            types_list.append(clean)\n",
    "    expected_count = 5\n",
    "    while len(numbers_list) < expected_count:\n",
    "        numbers_list.append(\"\")\n",
    "    types_list = types_list[:expected_count]\n",
    "    numbers_list = numbers_list[:expected_count]\n",
    "    structured = [{\"Type\": types_list[i], \"Number\": numbers_list[i]} for i in range(expected_count)]\n",
    "    df = pd.DataFrame(structured)\n",
    "    logger.info(f\"OBS_INT DataFrame shape: {df.shape}\")\n",
    "    return {\"DAILY NUMBERS: OBSERVATION & INTERVENTION\": structured}, df\n",
    "\n",
    "def process_bop(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"BOP OCR extraction complete.\")\n",
    "    patterns = {\n",
    "        \"Last BOP Test Date\": r\"Last BOP Test Date\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Last BOP Drill\": r\"Last BOP Drill\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Next BOP Test\": r\"Next BOP Test\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\"\n",
    "    }\n",
    "    result = {}\n",
    "    for key, regex in patterns.items():\n",
    "        match = re.search(regex, ocr_text, re.IGNORECASE)\n",
    "        result[key] = match.group(1) if match else \"\"\n",
    "    df = pd.DataFrame(list(result.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"BOP DataFrame shape: {df.shape}\")\n",
    "    return {\"BOP\": result}, df\n",
    "\n",
    "def build_dir_info_dict_from_rois(roi_texts, debug=False):\n",
    "    all_texts = [t[4] for t in roi_texts]\n",
    "    daily_cum_idx = next((i for i, txt in enumerate(all_texts)\n",
    "                           if \"daily\" in txt.lower() and \"cumulative\" in txt.lower()), None)\n",
    "    if daily_cum_idx is None:\n",
    "        logger.warning(\"Could not find 'Daily Cumulative' bounding box.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    cat_idx = daily_cum_idx + 1\n",
    "    if cat_idx >= len(all_texts):\n",
    "        logger.warning(\"No bounding box after 'Daily Cumulative'.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    categories_box = all_texts[cat_idx]\n",
    "    lines = [ln.strip() for ln in categories_box.split(\"\\n\") if ln.strip()]\n",
    "    if len(lines) < 5:\n",
    "        logger.warning(f\"Expected 5 category lines, got {len(lines)}: {lines}\")\n",
    "    def safe_get(idx):\n",
    "        return all_texts[idx] if 0 <= idx < len(all_texts) else \"\"\n",
    "    structured = []\n",
    "    for i in range(4):\n",
    "        cat_name = lines[i] if i < len(lines) else f\"Unknown Category {i+1}\"\n",
    "        daily_box = safe_get(cat_idx + 1 + (i * 2))\n",
    "        cum_box = safe_get(cat_idx + 2 + (i * 2))\n",
    "        structured.append({\n",
    "            \"Category\": cat_name,\n",
    "            \"Daily\": \"\" if daily_box == \"[BLANK]\" else daily_box,\n",
    "            \"Cumulative\": \"\" if cum_box == \"[BLANK]\" else cum_box\n",
    "        })\n",
    "    last_box = safe_get(cat_idx + 9)\n",
    "    last_cat = lines[4] if len(lines) >= 5 else \"Rotating Footage\"\n",
    "    remainder = last_box.replace(last_cat, \"\").strip()\n",
    "    tokens = remainder.split()\n",
    "    daily_val = tokens[0] if len(tokens) >= 2 else \"\"\n",
    "    cum_val = tokens[1] if len(tokens) >= 2 else \"\"\n",
    "    structured.append({\n",
    "        \"Category\": last_cat,\n",
    "        \"Daily\": \"\" if daily_val == \"[BLANK]\" else daily_val,\n",
    "        \"Cumulative\": \"\" if cum_val == \"[BLANK]\" else cum_val\n",
    "    })\n",
    "    df = pd.DataFrame(structured)\n",
    "    logger.info(f\"DIR INFO DataFrame shape: {df.shape}\")\n",
    "    return {\"DIR INFO\": structured}, df\n",
    "\n",
    "def process_dir_info(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    return build_dir_info_dict_from_rois(roi_texts, debug=debug)\n",
    "\n",
    "def build_survey_dict_from_rois(roi_texts, expected_headers):\n",
    "    row_strings = group_ocr_results(roi_texts)\n",
    "    logger.info(f\"SURVEY - Grouped Rows: {row_strings}\")\n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for subline in line.split(\"\\n\"):\n",
    "            subline = subline.strip()\n",
    "            if subline:\n",
    "                all_lines.append(subline)\n",
    "    logger.info(f\"SURVEY - All extracted lines: {all_lines}\")\n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"md\" in lower_tokens and \"inclination\" in lower_tokens:\n",
    "            logger.info(f\"SURVEY - Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"SURVEY - Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    logger.info(f\"SURVEY - Data lines to parse: {data_lines}\")\n",
    "    survey_list = [{expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "                   for tokens in data_lines]\n",
    "    return survey_list\n",
    "\n",
    "def sort_survey_data(survey_list):\n",
    "    def md_value(row):\n",
    "        try:\n",
    "            return float(row[\"MD\"].replace(\",\", \"\"))\n",
    "        except Exception:\n",
    "            return 0\n",
    "    return sorted(survey_list, key=md_value, reverse=True)\n",
    "\n",
    "def pipeline_survey_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"MD\", \"Inclination\", \"Azimuth\", \"DLS\", \"TVD\"]\n",
    "    survey_list = build_survey_dict_from_rois(roi_texts, expected_headers)\n",
    "    survey_list = sort_survey_data(survey_list)\n",
    "    df = pd.DataFrame(survey_list)\n",
    "    logger.info(f\"SURVEY DataFrame shape: {df.shape}\")\n",
    "    return {\"SURVEY\": survey_list}, df\n",
    "\n",
    "def build_casing_dict_from_rois(roi_texts, expected_headers, debug=False):\n",
    "    row_strings = group_ocr_results(roi_texts)\n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for sub in line.split(\"\\n\"):\n",
    "            sub = sub.strip()\n",
    "            if sub:\n",
    "                all_lines.append(sub)\n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"type\" in lower_tokens and \"size\" in lower_tokens:\n",
    "            logger.info(f\"CASING - Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"CASING - Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    casing_list = [{expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "                   for tokens in data_lines]\n",
    "    return casing_list\n",
    "\n",
    "def pipeline_casing_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"Type\", \"Size\", \"Weight\", \"Grade\", \"Connection\", \"Top MD\", \"Bottom MD\", \"TOC\"]\n",
    "    casing_list = build_casing_dict_from_rois(roi_texts, expected_headers, debug=debug)\n",
    "    df = pd.DataFrame(casing_list)\n",
    "    logger.info(f\"CASING DataFrame shape: {df.shape}\")\n",
    "    return {\"CASING\": casing_list}, df\n",
    "\n",
    "def build_consumables_dict_from_rois(roi_texts, debug=False):\n",
    "    row_strings = group_ocr_results(roi_texts)\n",
    "    data_rows = []\n",
    "    for line in row_strings:\n",
    "        lower_line = line.lower()\n",
    "        if (\"consumable\" in lower_line and \"received\" in lower_line) or \"nun\" in lower_line:\n",
    "            continue\n",
    "        if len(line.split()) < 5:\n",
    "            continue\n",
    "        data_rows.append(line)\n",
    "    consumables_list = []\n",
    "    for line in data_rows:\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "        if len(tokens) > 5:\n",
    "            first = \" \".join(tokens[:-4])\n",
    "            tokens = [first] + tokens[-4:]\n",
    "        if len(tokens) != 5:\n",
    "            logger.warning(f\"CONSUMABLES - Skipping row (unexpected token count): {tokens}\")\n",
    "            continue\n",
    "        consumables_list.append({\n",
    "            \"Consumable\": tokens[0],\n",
    "            \"Daily Received (gal)\": tokens[1],\n",
    "            \"Daily Used (gal)\": tokens[2],\n",
    "            \"Cumulative Used (gal)\": tokens[3],\n",
    "            \"Daily on Hand (gal)\": tokens[4]\n",
    "        })\n",
    "    return consumables_list\n",
    "\n",
    "def pipeline_consumables_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    consumables_list = build_consumables_dict_from_rois(roi_texts, debug=debug)\n",
    "    df = pd.DataFrame(consumables_list)\n",
    "    logger.info(f\"CONSUMABLES DataFrame shape: {df.shape}\")\n",
    "    return {\"CONSUMABLES\": consumables_list}, df\n",
    "\n",
    "def extract_bha_data(image_path, debug=False):\n",
    "    image = Image.open(image_path)\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    patterns = {\n",
    "        \"Drill Pipe Detail\": r\"Drill Pipe Detail:\\s*([^\\n]+)\",\n",
    "        \"Size\": r\"Size:\\s*([\\d.]+)\\b\",\n",
    "        \"Wt./Ft\": r\"Wt\\./Ft:\\s*([\\d.]+)\\b\",\n",
    "        \"Connection\": r\"Connection:\\s*([\\w\\d-]+)\\b\",\n",
    "        \"ID\": r\"ID:\\s*([\\d.]+)\\b\",\n",
    "        \"Drill Bit\": r\"Drill Bit:\\s*([^\\n;]+)\",\n",
    "        \"Motor\": r\"Motor:\\s*([^\\n;]+)\",\n",
    "        \"MWD Tool\": r\"MWD Tool:\\s*([^\\n;]+)\",\n",
    "        \"Monel Collar\": r\"Monel Collar:\\s*([^\\n;]+)\",\n",
    "        \"X-Over\": r\"X-Over:\\s*([^\\n;]+)\",\n",
    "        \"Sub\": r\"Sub:\\s*([^\\n;]+)\",\n",
    "        \"HWDP\": r\"HWDP:\\s*([^\\n;]+)\",\n",
    "        \"Drill Pipe\": r\"Drill Pipe:\\s*([\\d.]+(?:\\\" DP)?)\",\n",
    "        \"Reamer\": r\"Reamer:\\s*([^\\n;]+)\",\n",
    "        \"Shock Sub\": r\"Shock Sub:\\s*([^\\n;]+)\",\n",
    "        \"Total Length\": r\"Total Length:\\s*(\\d+)\\b\"\n",
    "    }\n",
    "    bha_data = {}\n",
    "    for key, pat in patterns.items():\n",
    "        match = re.search(pat, ocr_text)\n",
    "        if match:\n",
    "            bha_data[key] = match.group(1).strip()\n",
    "    if \"Drill Pipe Detail\" in bha_data:\n",
    "        detail = bha_data[\"Drill Pipe Detail\"]\n",
    "        for remove_key in [\"Size\", \"Wt./Ft\", \"Connection\", \"ID\"]:\n",
    "            if remove_key in bha_data:\n",
    "                detail = re.sub(rf\"{remove_key}:\\s*{re.escape(bha_data[remove_key])}\", \"\", detail).strip(\",; \")\n",
    "        bha_data[\"Drill Pipe Detail\"] = detail\n",
    "    structured_data = {\n",
    "        \"BHA\": {\n",
    "            \"Drill Pipe Detail\": bha_data.get(\"Drill Pipe Detail\", \"\"),\n",
    "            \"Size\": bha_data.get(\"Size\", \"\"),\n",
    "            \"Wt./Ft\": bha_data.get(\"Wt./Ft\", \"\"),\n",
    "            \"Connection\": bha_data.get(\"Connection\", \"\"),\n",
    "            \"ID\": bha_data.get(\"ID\", \"\"),\n",
    "            \"BHA #4\": {\n",
    "                \"Drill Bit\": bha_data.get(\"Drill Bit\", \"\"),\n",
    "                \"Motor\": bha_data.get(\"Motor\", \"\"),\n",
    "                \"MWD Tool\": bha_data.get(\"MWD Tool\", \"\"),\n",
    "                \"Monel Collar\": bha_data.get(\"Monel Collar\", \"\"),\n",
    "                \"X-Over\": bha_data.get(\"X-Over\", \"\"),\n",
    "                \"Sub\": bha_data.get(\"Sub\", \"\"),\n",
    "                \"HWDP\": bha_data.get(\"HWDP\", \"\"),\n",
    "                \"Drill Pipe\": bha_data.get(\"Drill Pipe\", \"\"),\n",
    "                \"Reamer\": bha_data.get(\"Reamer\", \"\"),\n",
    "                \"Shock Sub\": bha_data.get(\"Shock Sub\", \"\")\n",
    "            },\n",
    "            \"Total Length\": bha_data.get(\"Total Length\", \"\")\n",
    "        }\n",
    "    }\n",
    "    if debug:\n",
    "        logger.info(\"Extracted BHA data:\")\n",
    "        logger.info(json.dumps(structured_data, indent=4))\n",
    "    return structured_data\n",
    "\n",
    "def pipeline_bha_data(debug=False):\n",
    "    image_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_?BHA.png\"  # Adjust as needed\n",
    "    bha_json = extract_bha_data(image_path, debug=debug)\n",
    "    img = safe_read_image(image_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    pump_data = parse_pumps_table(ocr_text)\n",
    "    circ_data = parse_drilling_circ_rates(ocr_text)\n",
    "    pumps_df = pd.DataFrame(pump_data)\n",
    "    circ_df = pd.DataFrame(circ_data)\n",
    "    bha_df = pd.DataFrame([bha_json])\n",
    "    logger.info(f\"BHA DataFrame shape: {bha_df.shape}\")\n",
    "    combined = {\"BHA\": bha_json, \"Pumps\": pump_data, \"DrillingCircRates\": circ_data}\n",
    "    return combined, pumps_df, circ_df, bha_df\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Additional Parsing Functions for Pumps and Drilling/Circ Rates\n",
    "# -----------------------------------------------------------------------------\n",
    "def parse_pumps_table(ocr_text):\n",
    "    pump_pattern = re.compile(\n",
    "        r\"^(\\d+)?\\s*(BOMCO)\\s+(TRIPLEX)\\s+(\\d+)?\\s*(\\d+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s*$\",\n",
    "        re.IGNORECASE)\n",
    "    pumps = []\n",
    "    for line in ocr_text.splitlines():\n",
    "        line = line.strip()\n",
    "        match = pump_pattern.match(line)\n",
    "        if match:\n",
    "            (number, model, pump_type, hhp, efficiency, stroke, liner,\n",
    "             p_rating, p_limit, spm_rating, spm_limit) = match.groups()\n",
    "            pumps.append({\n",
    "                \"Number\": number if number else \"\",\n",
    "                \"Model\": model,\n",
    "                \"Type\": pump_type,\n",
    "                \"HHP\": hhp if hhp else \"\",\n",
    "                \"Efficiency\": efficiency,\n",
    "                \"Stroke(in)\": stroke,\n",
    "                \"Liner(in)\": liner,\n",
    "                \"P-Rating(psi)\": p_rating,\n",
    "                \"P-Limit(psi)\": p_limit,\n",
    "                \"SPM Rating\": spm_rating,\n",
    "                \"SPM Limit\": spm_limit\n",
    "            })\n",
    "    return pumps\n",
    "\n",
    "def parse_drilling_circ_rates(ocr_text):\n",
    "    circ_pattern = re.compile(\n",
    "        r\"Drilling/Circ\\s+Rate\\s+(\\d+)\\s+(\\d+)\\s+PSI\\s*@\\s*(\\d+)\\s*SPM\\s*([\\d.]+)\\s+Gal/Stoke\\s+([\\d.]+)\\s+GPM\\s+([\\d.]+)\\s+BPM\\s+([\\d.]+)\\s+DC\\s+([\\d.]+)\\s+DP\",\n",
    "        re.IGNORECASE)\n",
    "    circ_rates = []\n",
    "    for line in ocr_text.splitlines():\n",
    "        line = line.strip()\n",
    "        match = circ_pattern.search(line)\n",
    "        if match:\n",
    "            rate_id, pressure, spm, gal_stroke, gpm, bpm, dc, dp = match.groups()\n",
    "            circ_rates.append({\n",
    "                \"RateID\": rate_id,\n",
    "                \"Pressure(PSI)\": pressure,\n",
    "                \"SPM\": spm,\n",
    "                \"Gal/Stoke\": gal_stroke,\n",
    "                \"GPM\": gpm,\n",
    "                \"BPM\": bpm,\n",
    "                \"DC\": dc,\n",
    "                \"DP\": dp\n",
    "            })\n",
    "    return circ_rates\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MAIN RUNNER: STANDARDIZED PIPELINE EXECUTION\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_pipeline(name, pipeline_info, debug=False):\n",
    "    try:\n",
    "        logger.info(f\"Processing pipeline: {name}\")\n",
    "        output, df = pipeline_info[\"func\"](debug)\n",
    "        logger.info(f\"{name} DataFrame shape: {df.shape} (Rows: {df.shape[0]}, Columns: {df.shape[1]})\")\n",
    "        json_path = os.path.join(JSON_FOLDER, pipeline_info[\"json\"])\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(output, f, indent=4)\n",
    "        csv_path = os.path.join(CSV_FOLDER, pipeline_info[\"csv\"])\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"{name} saved: JSON({json_path}), CSV({csv_path})\")\n",
    "        print(f\"--- {name.upper()} DataFrame ---\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(df.head(10))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in pipeline {name}: {e}\")\n",
    "\n",
    "def main():\n",
    "    debug = False  # Set True for detailed logging\n",
    "    # Define all pipelines in sequence\n",
    "    pipelines = {\n",
    "        \"cost_data\": {\n",
    "            \"func\": pipeline_cost_data,\n",
    "            \"csv\": \"cost_data.csv\",\n",
    "            \"json\": \"cost_data.json\"\n",
    "        },\n",
    "        \"well_job\": {\n",
    "            \"func\": lambda d: process_well_job_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\", d),\n",
    "            \"csv\": \"well_job_data.csv\",\n",
    "            \"json\": \"well_job_data.json\"\n",
    "        },\n",
    "        \"obs_int\": {\n",
    "            \"func\": lambda d: process_obs_int(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\", d),\n",
    "            \"csv\": \"obs_int_data.csv\",\n",
    "            \"json\": \"obs_int_data.json\"\n",
    "        },\n",
    "        \"bop\": {\n",
    "            \"func\": lambda d: process_bop(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\", d),\n",
    "            \"csv\": \"bop_data.csv\",\n",
    "            \"json\": \"bop_data.json\"\n",
    "        },\n",
    "        \"dir_info\": {\n",
    "            \"func\": lambda d: process_dir_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\", d),\n",
    "            \"csv\": \"dir_info_data.csv\",\n",
    "            \"json\": \"dir_info_data.json\"\n",
    "        },\n",
    "        \"survey\": {\n",
    "            \"func\": pipeline_survey_data,\n",
    "            \"csv\": \"survey_data.csv\",\n",
    "            \"json\": \"survey_data.json\"\n",
    "        },\n",
    "        \"casing\": {\n",
    "            \"func\": pipeline_casing_data,\n",
    "            \"csv\": \"casing_data.csv\",\n",
    "            \"json\": \"casing_data.json\"\n",
    "        },\n",
    "        \"consumables\": {\n",
    "            \"func\": pipeline_consumables_data,\n",
    "            \"csv\": \"consumables_data.csv\",\n",
    "            \"json\": \"consumables_data.json\"\n",
    "        },\n",
    "        \"mud\": {\n",
    "            \"func\": pipeline_mud_data,\n",
    "            \"csv\": \"mud_data.csv\",\n",
    "            \"json\": \"mud_data.json\"\n",
    "        },\n",
    "        \"bha\": {\n",
    "            \"func\": pipeline_bha_data,\n",
    "            \"csv\": \"bha_data.csv\",\n",
    "            \"json\": \"bha_data.json\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, pipe in pipelines.items():\n",
    "        run_pipeline(name, pipe, debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13389e7-770d-44a2-b489-d03b4160e713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import cv2, os\n",
    "import pytesseract\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Global Logger & Output Directories\n",
    "# -----------------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"OCRProduction\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "OUTPUT_FOLDER = \"/dbfs/mnt/mini-proj-dd/final_results\"\n",
    "CSV_FOLDER = os.path.join(OUTPUT_FOLDER, \"csv\")\n",
    "JSON_FOLDER = os.path.join(OUTPUT_FOLDER, \"json\")\n",
    "os.makedirs(CSV_FOLDER, exist_ok=True)\n",
    "os.makedirs(JSON_FOLDER, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Common Utility Functions (No image display)\n",
    "# -----------------------------------------------------------------------------\n",
    "def safe_read_image(img_path):\n",
    "    \"\"\"Read an image from a local or DBFS path.\"\"\"\n",
    "    local_path = img_path if not img_path.startswith(\"dbfs:\") else img_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {local_path}\")\n",
    "    logger.info(f\"Image loaded from {local_path} with shape {img.shape}\")\n",
    "    return img\n",
    "\n",
    "def preprocess_image(img, debug=False):\n",
    "    \"\"\"Convert image to grayscale and apply adaptive thresholding.\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                   cv2.THRESH_BINARY, 15, 9)\n",
    "    if debug:\n",
    "        logger.info(\"Preprocessing completed (grayscale and threshold applied).\")\n",
    "    return thresh\n",
    "\n",
    "def detect_text_regions(thresh_img, debug=False):\n",
    "    \"\"\"Detect text regions (bounding boxes) from the thresholded image.\"\"\"\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "    rois.sort(key=lambda b: (b[1], b[0]))\n",
    "    if debug:\n",
    "        logger.info(f\"Detected {len(rois)} text regions.\")\n",
    "    return rois\n",
    "\n",
    "def perform_ocr_on_rois(img, rois, debug=False):\n",
    "    \"\"\"Perform OCR on each detected region and return list of (x,y,w,h,text).\"\"\"\n",
    "    results = []\n",
    "    for i, (x, y, w, h) in enumerate(rois):\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        text = pytesseract.image_to_string(roi, config=\"--psm 6\").strip() or \"[BLANK]\"\n",
    "        results.append((x, y, w, h, text))\n",
    "        if debug:\n",
    "            logger.info(f\"OCR Box {i}: {text}\")\n",
    "    return results\n",
    "\n",
    "def perform_ocr(img):\n",
    "    \"\"\"Perform OCR on the entire image.\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return pytesseract.image_to_string(gray, config=\"--psm 6\")\n",
    "\n",
    "def extract_key_value_from_text(text, expected_keys):\n",
    "    \"\"\"Extract key-value pairs from OCR text given expected keys.\"\"\"\n",
    "    combined = \" \".join(line.strip() for line in text.splitlines() if line.strip())\n",
    "    combined = re.sub(r'\\s+', ' ', combined)\n",
    "    result = {}\n",
    "    for i, key in enumerate(expected_keys):\n",
    "        if i < len(expected_keys) - 1:\n",
    "            next_key = expected_keys[i+1]\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*?)(?=\\s*' + re.escape(next_key) + r'\\s*:|$)'\n",
    "        else:\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*)'\n",
    "        match = re.search(pattern, combined, re.IGNORECASE)\n",
    "        result[key] = match.group(1).strip() if match and match.group(1).strip() else None\n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# [PARSE FUNCTIONS]\n",
    "# These functions (e.g. build_bit_info_dict_from_rois, process_well_job_info, etc.)\n",
    "# are your specialized parsers. They have been left mostly intact.\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_bit_info_dict_from_rois(roi_texts, debug=False):\n",
    "    row_tolerance = 10\n",
    "    grouped_rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            grouped_rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        grouped_rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for i, row in enumerate(grouped_rows):\n",
    "        row.sort(key=lambda cell: cell[0])\n",
    "        line = \" \".join(cell[4] for cell in row)\n",
    "        row_strings.append(line.replace(\"\\n\", \" \").strip())\n",
    "        if debug:\n",
    "            logger.info(f\"BIT - Grouped Row {i}: {row_strings[-1]}\")\n",
    "    if len(row_strings) < 3:\n",
    "        logger.warning(\"Not enough rows found for BIT layout.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    data_lines = row_strings[3:]\n",
    "    final_columns = [\n",
    "        \"Bit #\", \"Size\", \"Make\", \"Model\", \"Serial #\",\n",
    "        \"Nozzle-(Number x Size)\", \"Nozzle-TFA\",\n",
    "        \"Depth-In\", \"Depth-Out\", \"Depth-Feet\", \"Depth-ROP\",\n",
    "        \"Hours-Total\", \"Hours-On Btm\",\n",
    "        \"Dull Grade-I\", \"Dull Grade-O1\", \"Dull Grade-D\", \"Dull Grade-L\",\n",
    "        \"Dull Grade-B\", \"Dull Grade-G\", \"Dull Grade-O2\", \"Dull Grade-RP\"\n",
    "    ]\n",
    "    structured_data = []\n",
    "    for line in data_lines:\n",
    "        tokens = line.split()\n",
    "        if len(tokens) < 21:\n",
    "            tokens += [\"\"] * (21 - len(tokens))\n",
    "        elif len(tokens) > 21:\n",
    "            tokens = tokens[:21]\n",
    "        row_dict = {final_columns[i]: tokens[i] for i in range(21)}\n",
    "        structured_data.append(row_dict)\n",
    "        if debug:\n",
    "            logger.info(f\"BIT - Parsed row: {row_dict}\")\n",
    "    df = pd.DataFrame(structured_data)\n",
    "    logger.info(f\"BIT DataFrame shape: {df.shape}\")\n",
    "    return {\"BIT DETAILS\": df.to_dict(orient='records')}, df\n",
    "\n",
    "def process_well_job_info(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    if debug:\n",
    "        logger.info(\"Well/Job OCR extraction complete.\")\n",
    "        logger.info(f\"OCR Text: {ocr_text}\")\n",
    "    expected_keys = [\n",
    "        \"Well Name\", \"Job Name\", \"Supervisor(s)\", \"Field\", \"Sec/Twn/Rng\", \"Phone\",\n",
    "        \"AFE #\", \"API #\", \"Email\", \"Contractor\", \"Elevation\", \"RKB\",\n",
    "        \"Spud Date\", \"Days from Spud\", \"Days on Loc\", \"MD/TVD\", \"24 Hr Footage\",\n",
    "        \"Present Operations\", \"Activity Planned\"\n",
    "    ]\n",
    "    combined = \" \".join(line.strip() for line in ocr_text.splitlines() if line.strip())\n",
    "    combined = re.sub(r'\\s+', ' ', combined)\n",
    "    result = {}\n",
    "    for i, key in enumerate(expected_keys):\n",
    "        if i < len(expected_keys) - 1:\n",
    "            next_key = expected_keys[i+1]\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*?)(?=\\s*' + re.escape(next_key) + r'\\s*:|$)'\n",
    "        else:\n",
    "            pattern = re.escape(key) + r'\\s*:\\s*(.*)'\n",
    "        match = re.search(pattern, combined, re.IGNORECASE)\n",
    "        result[key] = match.group(1).strip() if match else \"\"\n",
    "    df = pd.DataFrame(list(result.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"WELL/JOB DataFrame shape: {df.shape}\")\n",
    "    return {\"WELL/JOB INFORMATION\": result}, df\n",
    "\n",
    "def process_obs_int(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    header_str = \"daily numbers: observation & intervention\"\n",
    "    all_texts = [t[4] for t in roi_texts]\n",
    "    types_list, numbers_list = [], []\n",
    "    for txt in all_texts:\n",
    "        clean = txt.strip()\n",
    "        if clean.lower() in [header_str, \"number\", \"[blank]\"]:\n",
    "            continue\n",
    "        try:\n",
    "            float(clean)\n",
    "            numbers_list.append(\"\" if clean.lower() == \"[blank]\" else clean)\n",
    "            continue\n",
    "        except ValueError:\n",
    "            pass\n",
    "        if \"\\n\" in clean:\n",
    "            for line in clean.splitlines():\n",
    "                line = line.strip()\n",
    "                if line and line.lower() != \"[blank]\":\n",
    "                    types_list.append(line)\n",
    "        else:\n",
    "            types_list.append(clean)\n",
    "    expected_count = 5\n",
    "    while len(numbers_list) < expected_count:\n",
    "        numbers_list.append(\"\")\n",
    "    types_list = types_list[:expected_count]\n",
    "    numbers_list = numbers_list[:expected_count]\n",
    "    structured = [{\"Type\": types_list[i], \"Number\": numbers_list[i]} for i in range(expected_count)]\n",
    "    df = pd.DataFrame(structured)\n",
    "    logger.info(f\"OBS_INT DataFrame shape: {df.shape}\")\n",
    "    return {\"DAILY NUMBERS: OBSERVATION & INTERVENTION\": structured}, df\n",
    "\n",
    "def pipeline_cost_data(debug=False):\n",
    "    \"\"\"Extract Cost Data from a specified image.\"\"\"\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png\"\n",
    "    try:\n",
    "        img = safe_read_image(section_path)\n",
    "        logger.info(\"Cost image loaded.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(e)\n",
    "        return {}, pd.DataFrame()\n",
    "    ocr_text = perform_ocr(img)\n",
    "    logger.info(\"Cost OCR extraction complete.\")\n",
    "    expected_keys = [\n",
    "        \"Drilling AFE Amount\", \"Daily Drilling Cost\", \"Cumulative Drilling Cost\",\n",
    "        \"Cumulative Well Cost\", \"Daily Mud Cost\", \"Cumulative Mud Cost\"\n",
    "    ]\n",
    "    extracted = extract_key_value_from_text(ocr_text, expected_keys)\n",
    "    final_dict = {\"COST DATA\": extracted}\n",
    "    logger.info(json.dumps(final_dict, indent=4))\n",
    "    df = pd.DataFrame(list(extracted.items()), columns=[\"Key\", \"Value\"])\n",
    "    return final_dict, df\n",
    "\n",
    "def process_bop(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    if debug:\n",
    "        logger.info(\"BOP OCR extraction complete.\")\n",
    "        logger.info(f\"OCR Text: {ocr_text}\")\n",
    "    patterns = {\n",
    "        \"Last BOP Test Date\": r\"Last BOP Test Date\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Last BOP Drill\": r\"Last BOP Drill\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\",\n",
    "        \"Next BOP Test\": r\"Next BOP Test\\s*:\\s*(\\d{1,2}/\\d{1,2}/\\d{2,4})\"\n",
    "    }\n",
    "    result = {}\n",
    "    for key, regex in patterns.items():\n",
    "        match = re.search(regex, ocr_text, re.IGNORECASE)\n",
    "        result[key] = match.group(1) if match else \"\"\n",
    "    df = pd.DataFrame(list(result.items()), columns=[\"Key\", \"Value\"])\n",
    "    logger.info(f\"BOP DataFrame shape: {df.shape}\")\n",
    "    return {\"BOP\": result}, df\n",
    "\n",
    "def build_dir_info_dict_from_rois(roi_texts, debug=False):\n",
    "    all_texts = [t[4] for t in roi_texts]\n",
    "    daily_cum_idx = next((i for i, txt in enumerate(all_texts)\n",
    "                           if \"daily\" in txt.lower() and \"cumulative\" in txt.lower()), None)\n",
    "    if daily_cum_idx is None:\n",
    "        logger.warning(\"Could not find 'Daily Cumulative' bounding box.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    cat_idx = daily_cum_idx + 1\n",
    "    if cat_idx >= len(all_texts):\n",
    "        logger.warning(\"No bounding box after 'Daily Cumulative'.\")\n",
    "        return {}, pd.DataFrame()\n",
    "    categories_box = all_texts[cat_idx]\n",
    "    lines = [ln.strip() for ln in categories_box.split(\"\\n\") if ln.strip()]\n",
    "    if len(lines) < 5:\n",
    "        logger.warning(f\"Expected 5 category lines, got {len(lines)}: {lines}\")\n",
    "    def safe_get(idx):\n",
    "        return all_texts[idx] if 0 <= idx < len(all_texts) else \"\"\n",
    "    structured = []\n",
    "    for i in range(4):\n",
    "        cat_name = lines[i] if i < len(lines) else f\"Unknown Category {i+1}\"\n",
    "        daily_box = safe_get(cat_idx + 1 + (i * 2))\n",
    "        cum_box = safe_get(cat_idx + 2 + (i * 2))\n",
    "        structured.append({\n",
    "            \"Category\": cat_name,\n",
    "            \"Daily\": \"\" if daily_box == \"[BLANK]\" else daily_box,\n",
    "            \"Cumulative\": \"\" if cum_box == \"[BLANK]\" else cum_box\n",
    "        })\n",
    "    last_box = safe_get(cat_idx + 9)\n",
    "    last_cat = lines[4] if len(lines) >= 5 else \"Rotating Footage\"\n",
    "    remainder = last_box.replace(last_cat, \"\").strip()\n",
    "    tokens = remainder.split()\n",
    "    daily_val = tokens[0] if len(tokens) >= 2 else \"\"\n",
    "    cum_val = tokens[1] if len(tokens) >= 2 else \"\"\n",
    "    structured.append({\n",
    "        \"Category\": last_cat,\n",
    "        \"Daily\": \"\" if daily_val == \"[BLANK]\" else daily_val,\n",
    "        \"Cumulative\": \"\" if cum_val == \"[BLANK]\" else cum_val\n",
    "    })\n",
    "    df = pd.DataFrame(structured)\n",
    "    logger.info(f\"DIR INFO DataFrame shape: {df.shape}\")\n",
    "    return {\"DIR INFO\": structured}, df\n",
    "\n",
    "def process_dir_info(section_path, debug=False):\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    return build_dir_info_dict_from_rois(roi_texts, debug=debug)\n",
    "\n",
    "def build_survey_dict_from_rois(roi_texts, expected_headers):\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row.sort(key=lambda c: c[0])\n",
    "        line = \" \".join(cell[4] for cell in row)\n",
    "        row_strings.append(line)\n",
    "        logger.info(f\"SURVEY - Grouped Row {i}: {line}\")\n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for subline in line.split(\"\\n\"):\n",
    "            subline = subline.strip()\n",
    "            if subline:\n",
    "                all_lines.append(subline)\n",
    "    logger.info(f\"SURVEY - All extracted lines: {all_lines}\")\n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"md\" in lower_tokens and \"inclination\" in lower_tokens:\n",
    "            logger.info(f\"SURVEY - Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"SURVEY - Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    logger.info(f\"SURVEY - Data lines to parse: {data_lines}\")\n",
    "    survey_list = []\n",
    "    for tokens in data_lines:\n",
    "        row_dict = {expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "        survey_list.append(row_dict)\n",
    "    return survey_list\n",
    "\n",
    "def sort_survey_data(survey_list):\n",
    "    def md_value(row):\n",
    "        try:\n",
    "            return float(row[\"MD\"].replace(\",\", \"\"))\n",
    "        except Exception:\n",
    "            return 0\n",
    "    return sorted(survey_list, key=md_value, reverse=True)\n",
    "\n",
    "def pipeline_survey_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_13.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"MD\", \"Inclination\", \"Azimuth\", \"DLS\", \"TVD\"]\n",
    "    survey_list = build_survey_dict_from_rois(roi_texts, expected_headers)\n",
    "    survey_list = sort_survey_data(survey_list)\n",
    "    df = pd.DataFrame(survey_list)\n",
    "    logger.info(f\"SURVEY DataFrame shape: {df.shape}\")\n",
    "    return {\"SURVEY\": survey_list}, df\n",
    "\n",
    "def build_casing_dict_from_rois(roi_texts, expected_headers, debug=False):\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row.sort(key=lambda c: c[0])\n",
    "        line = \" \".join(cell[4] for cell in row).strip()\n",
    "        row_strings.append(line)\n",
    "        if debug:\n",
    "            logger.info(f\"CASING - Grouped Row {i}: {line}\")\n",
    "    all_lines = []\n",
    "    for line in row_strings:\n",
    "        for sub in line.split(\"\\n\"):\n",
    "            sub = sub.strip()\n",
    "            if sub:\n",
    "                all_lines.append(sub)\n",
    "    data_lines = []\n",
    "    for line in all_lines:\n",
    "        tokens = re.split(r'\\s{2,}', line)\n",
    "        if len(tokens) == 1:\n",
    "            tokens = line.split()\n",
    "        lower_tokens = [t.lower() for t in tokens]\n",
    "        if \"type\" in lower_tokens and \"size\" in lower_tokens:\n",
    "            logger.info(f\"CASING - Skipping header line: {tokens}\")\n",
    "            continue\n",
    "        if len(tokens) < len(expected_headers):\n",
    "            logger.warning(f\"CASING - Line has fewer tokens than expected: {tokens}\")\n",
    "            continue\n",
    "        tokens = tokens[:len(expected_headers)]\n",
    "        data_lines.append(tokens)\n",
    "    casing_list = []\n",
    "    for tokens in data_lines:\n",
    "        row_dict = {expected_headers[i]: tokens[i] for i in range(len(expected_headers))}\n",
    "        casing_list.append(row_dict)\n",
    "    return casing_list\n",
    "\n",
    "def pipeline_casing_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_7.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"Type\", \"Size\", \"Weight\", \"Grade\", \"Connection\", \"Top MD\", \"Bottom MD\", \"TOC\"]\n",
    "    casing_list = build_casing_dict_from_rois(roi_texts, expected_headers, debug=debug)\n",
    "    df = pd.DataFrame(casing_list)\n",
    "    logger.info(f\"CASING DataFrame shape: {df.shape}\")\n",
    "    return {\"CASING\": casing_list}, df\n",
    "\n",
    "def build_consumables_dict_from_rois(roi_texts, debug=False):\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    grouped_rows = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row.sort(key=lambda cell: cell[0])\n",
    "        line = \" \".join(cell[4] for cell in row).strip()\n",
    "        grouped_rows.append(line)\n",
    "        if debug:\n",
    "            logger.info(f\"CONSUMABLES - Grouped Row {i}: {line}\")\n",
    "    data_rows = []\n",
    "    for line in grouped_rows:\n",
    "        lower_line = line.lower()\n",
    "        if (\"consumable\" in lower_line and \"received\" in lower_line) or \"nun\" in lower_line:\n",
    "            continue\n",
    "        if len(line.split()) < 5:\n",
    "            continue\n",
    "        data_rows.append(line)\n",
    "    consumables_list = []\n",
    "    for line in data_rows:\n",
    "        tokens = re.split(r'\\s+', line)\n",
    "        if len(tokens) > 5:\n",
    "            first = \" \".join(tokens[:-4])\n",
    "            tokens = [first] + tokens[-4:]\n",
    "        if len(tokens) != 5:\n",
    "            logger.warning(f\"CONSUMABLES - Skipping row (unexpected token count): {tokens}\")\n",
    "            continue\n",
    "        row_dict = {\n",
    "            \"Consumable\": tokens[0],\n",
    "            \"Daily Received (gal)\": tokens[1],\n",
    "            \"Daily Used (gal)\": tokens[2],\n",
    "            \"Cumulative Used (gal)\": tokens[3],\n",
    "            \"Daily on Hand (gal)\": tokens[4]\n",
    "        }\n",
    "        consumables_list.append(row_dict)\n",
    "    return consumables_list\n",
    "\n",
    "def pipeline_consumables_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_2.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    consumables_list = build_consumables_dict_from_rois(roi_texts, debug=debug)\n",
    "    df = pd.DataFrame(consumables_list)\n",
    "    logger.info(f\"CONSUMABLES DataFrame shape: {df.shape}\")\n",
    "    return {\"CONSUMABLES\": consumables_list}, df\n",
    "\n",
    "def extract_bha_data(image_path, debug=False):\n",
    "    image = Image.open(image_path)\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    patterns = {\n",
    "        \"Drill Pipe Detail\": r\"Drill Pipe Detail:\\s*([^\\n]+)\",\n",
    "        \"Size\": r\"Size:\\s*([\\d.]+)\\b\",\n",
    "        \"Wt./Ft\": r\"Wt\\./Ft:\\s*([\\d.]+)\\b\",\n",
    "        \"Connection\": r\"Connection:\\s*([\\w\\d-]+)\\b\",\n",
    "        \"ID\": r\"ID:\\s*([\\d.]+)\\b\",\n",
    "        \"Drill Bit\": r\"Drill Bit:\\s*([^\\n;]+)\",\n",
    "        \"Motor\": r\"Motor:\\s*([^\\n;]+)\",\n",
    "        \"MWD Tool\": r\"MWD Tool:\\s*([^\\n;]+)\",\n",
    "        \"Monel Collar\": r\"Monel Collar:\\s*([^\\n;]+)\",\n",
    "        \"X-Over\": r\"X-Over:\\s*([^\\n;]+)\",\n",
    "        \"Sub\": r\"Sub:\\s*([^\\n;]+)\",\n",
    "        \"HWDP\": r\"HWDP:\\s*([^\\n;]+)\",\n",
    "        \"Drill Pipe\": r\"Drill Pipe:\\s*([\\d.]+(?:\\\" DP)?)\",\n",
    "        \"Reamer\": r\"Reamer:\\s*([^\\n;]+)\",\n",
    "        \"Shock Sub\": r\"Shock Sub:\\s*([^\\n;]+)\",\n",
    "        \"Total Length\": r\"Total Length:\\s*(\\d+)\\b\"\n",
    "    }\n",
    "    bha_data = {}\n",
    "    for key, pat in patterns.items():\n",
    "        match = re.search(pat, ocr_text)\n",
    "        if match:\n",
    "            bha_data[key] = match.group(1).strip()\n",
    "    if \"Drill Pipe Detail\" in bha_data:\n",
    "        detail = bha_data[\"Drill Pipe Detail\"]\n",
    "        for remove_key in [\"Size\", \"Wt./Ft\", \"Connection\", \"ID\"]:\n",
    "            if remove_key in bha_data:\n",
    "                detail = re.sub(rf\"{remove_key}:\\s*{re.escape(bha_data[remove_key])}\", \"\", detail).strip(\",; \")\n",
    "        bha_data[\"Drill Pipe Detail\"] = detail\n",
    "    structured_data = {\n",
    "        \"BHA\": {\n",
    "            \"Drill Pipe Detail\": bha_data.get(\"Drill Pipe Detail\", \"\"),\n",
    "            \"Size\": bha_data.get(\"Size\", \"\"),\n",
    "            \"Wt./Ft\": bha_data.get(\"Wt./Ft\", \"\"),\n",
    "            \"Connection\": bha_data.get(\"Connection\", \"\"),\n",
    "            \"ID\": bha_data.get(\"ID\", \"\"),\n",
    "            \"BHA #4\": {\n",
    "                \"Drill Bit\": bha_data.get(\"Drill Bit\", \"\"),\n",
    "                \"Motor\": bha_data.get(\"Motor\", \"\"),\n",
    "                \"MWD Tool\": bha_data.get(\"MWD Tool\", \"\"),\n",
    "                \"Monel Collar\": bha_data.get(\"Monel Collar\", \"\"),\n",
    "                \"X-Over\": bha_data.get(\"X-Over\", \"\"),\n",
    "                \"Sub\": bha_data.get(\"Sub\", \"\"),\n",
    "                \"HWDP\": bha_data.get(\"HWDP\", \"\"),\n",
    "                \"Drill Pipe\": bha_data.get(\"Drill Pipe\", \"\"),\n",
    "                \"Reamer\": bha_data.get(\"Reamer\", \"\"),\n",
    "                \"Shock Sub\": bha_data.get(\"Shock Sub\", \"\")\n",
    "            },\n",
    "            \"Total Length\": bha_data.get(\"Total Length\", \"\")\n",
    "        }\n",
    "    }\n",
    "    if debug:\n",
    "        logger.info(\"Extracted BHA data:\")\n",
    "        logger.info(json.dumps(structured_data, indent=4))\n",
    "    return structured_data\n",
    "\n",
    "def pipeline_bha_data(debug=False):\n",
    "    image_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_2_section_?BHA.png\"  # Adjust as needed\n",
    "    bha_json = extract_bha_data(image_path, debug=debug)\n",
    "    img = safe_read_image(image_path)\n",
    "    ocr_text = perform_ocr(img)\n",
    "    pump_pattern = re.compile(\n",
    "        r\"^(\\d+)?\\s*(BOMCO)\\s+(TRIPLEX)\\s+(\\d+)?\\s*(\\d+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s+(\\d+)\\s*$\",\n",
    "        re.IGNORECASE)\n",
    "    pumps = []\n",
    "    for line in ocr_text.splitlines():\n",
    "        line = line.strip()\n",
    "        match = pump_pattern.match(line)\n",
    "        if match:\n",
    "            (number, model, pump_type, hhp, efficiency, stroke,\n",
    "             liner, p_rating, p_limit, spm_rating, spm_limit) = match.groups()\n",
    "            pumps.append({\n",
    "                \"Number\": number if number else \"\",\n",
    "                \"Model\": model,\n",
    "                \"Type\": pump_type,\n",
    "                \"HHP\": hhp if hhp else \"\",\n",
    "                \"Efficiency\": efficiency,\n",
    "                \"Stroke(in)\": stroke,\n",
    "                \"Liner(in)\": liner,\n",
    "                \"P-Rating(psi)\": p_rating,\n",
    "                \"P-Limit(psi)\": p_limit,\n",
    "                \"SPM Rating\": spm_rating,\n",
    "                \"SPM Limit\": spm_limit\n",
    "            })\n",
    "    circ_pattern = re.compile(\n",
    "        r\"Drilling/Circ\\s+Rate\\s+(\\d+)\\s+(\\d+)\\s+PSI\\s*@\\s*(\\d+)\\s*SPM\\s*([\\d.]+)\\s+Gal/Stoke\\s+([\\d.]+)\\s+GPM\\s+([\\d.]+)\\s+BPM\\s+([\\d.]+)\\s+DC\\s+([\\d.]+)\\s+DP\",\n",
    "        re.IGNORECASE)\n",
    "    circ_rates = []\n",
    "    for line in ocr_text.splitlines():\n",
    "        line = line.strip()\n",
    "        match = circ_pattern.search(line)\n",
    "        if match:\n",
    "            rate_id, pressure, spm, gal_stroke, gpm, bpm, dc, dp = match.groups()\n",
    "            circ_rates.append({\n",
    "                \"RateID\": rate_id,\n",
    "                \"Pressure(PSI)\": pressure,\n",
    "                \"SPM\": spm,\n",
    "                \"Gal/Stoke\": gal_stroke,\n",
    "                \"GPM\": gpm,\n",
    "                \"BPM\": bpm,\n",
    "                \"DC\": dc,\n",
    "                \"DP\": dp\n",
    "            })\n",
    "    pumps_df = pd.DataFrame(pumps)\n",
    "    circ_df = pd.DataFrame(circ_rates)\n",
    "    bha_df = pd.DataFrame([bha_json])\n",
    "    logger.info(f\"BHA DataFrame shape: {bha_df.shape}\")\n",
    "    combined = {\"BHA\": bha_json, \"Pumps\": pumps, \"DrillingCircRates\": circ_rates}\n",
    "    return combined, pumps_df, circ_df, bha_df\n",
    "\n",
    "def build_mud_dict_from_rois(roi_texts, expected_headers, debug=False):\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    row_strings = []\n",
    "    for i, row in enumerate(rows):\n",
    "        row.sort(key=lambda c: c[0])\n",
    "        line_text = \" \".join(cell[4] for cell in row)\n",
    "        row_strings.append(line_text)\n",
    "        if debug:\n",
    "            logger.info(f\"MUD - Row {i} text: {line_text}\")\n",
    "    tokens1 = row_strings[1].split() if len(row_strings) > 1 else []\n",
    "    tokens2 = row_strings[2].split() if len(row_strings) > 2 else []\n",
    "    combined_tokens = tokens1 + tokens2\n",
    "    def parse_value_row_tokens(expected_headers, tokens):\n",
    "        expected_token_count = (len(expected_headers) - 1) + 3\n",
    "        if len(tokens) < expected_token_count:\n",
    "            tokens += [\"[BLANK]\"] * (expected_token_count - len(tokens))\n",
    "        elif len(tokens) > expected_token_count:\n",
    "            tokens = tokens[:expected_token_count]\n",
    "        result = {}\n",
    "        idx = 0\n",
    "        for header in expected_headers:\n",
    "            if header == \"GELS (10s/10m/30m)\":\n",
    "                gels_tokens = tokens[idx:idx+3]\n",
    "                result[header] = {\"10s\": gels_tokens[0], \"10m\": gels_tokens[1], \"30m\": gels_tokens[2]}\n",
    "                idx += 3\n",
    "            else:\n",
    "                result[header] = tokens[idx]\n",
    "                idx += 1\n",
    "        return result\n",
    "    mapped = parse_value_row_tokens(expected_headers, combined_tokens)\n",
    "    return mapped\n",
    "\n",
    "def pipeline_mud_data(debug=False):\n",
    "    section_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_3.png\"\n",
    "    img = safe_read_image(section_path)\n",
    "    thresh = preprocess_image(img, debug=debug)\n",
    "    rois = detect_text_regions(thresh, debug=debug)\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=debug)\n",
    "    expected_headers = [\"Type\", \"Weight In\", \"Weight Out\", \"pH\", \"CAKE\",\n",
    "                        \"GELS (10s/10m/30m)\", \"Oil/Water\", \"FV\", \"ES\", \"PV\",\n",
    "                        \"YP\", \"CL\", \"Ca\", \"LGS\", \"WL\", \"HTHP Loss\", \"3 RPM\",\n",
    "                        \"6 RPM\", \"Mud Pits and Hole Volume\", \"24 Hr Loss\",\n",
    "                        \"Total Loss\", \"Comments\"]\n",
    "    mud_dict = build_mud_dict_from_rois(roi_texts, expected_headers, debug=debug)\n",
    "    df = pd.DataFrame([mud_dict])\n",
    "    logger.info(f\"MUD DataFrame shape: {df.shape}\")\n",
    "    return {\"MUD\": mud_dict}, df\n",
    "import pytesseract\n",
    "import re\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "def extract_bha_data(image_path):\n",
    "    # Load image and perform OCR\n",
    "    image = Image.open(image_path)\n",
    "    ocr_text = pytesseract.image_to_string(image)\n",
    "    \n",
    "    # Define regex patterns to extract key values without repetition\n",
    "    patterns = {\n",
    "        \"Drill Pipe Detail\": r\"Drill Pipe Detail:\\s*([^\\n]+)\",  # Extracts full text but **won't duplicate fields**\n",
    "        \"Size\": r\"Size:\\s*([\\d.]+)\\b\",\n",
    "        \"Wt./Ft\": r\"Wt\\./Ft:\\s*([\\d.]+)\\b\",\n",
    "        \"Connection\": r\"Connection:\\s*([\\w\\d-]+)\\b\",\n",
    "        \"ID\": r\"ID:\\s*([\\d.]+)\\b\",\n",
    "        \"Drill Bit\": r\"Drill Bit:\\s*([^\\n;]+)\",\n",
    "        \"Motor\": r\"Motor:\\s*([^\\n;]+)\",\n",
    "        \"MWD Tool\": r\"MWD Tool:\\s*([^\\n;]+)\",\n",
    "        \"Monel Collar\": r\"Monel Collar:\\s*([^\\n;]+)\",\n",
    "        \"X-Over\": r\"X-Over:\\s*([^\\n;]+)\",\n",
    "        \"Sub\": r\"Sub:\\s*([^\\n;]+)\",\n",
    "        \"HWDP\": r\"HWDP:\\s*([^\\n;]+)\",\n",
    "        \"Drill Pipe\": r\"Drill Pipe:\\s*([\\d.]+(?:\\\" DP)?)\",  \n",
    "        \"Reamer\": r\"Reamer:\\s*([^\\n;]+)\",\n",
    "        \"Shock Sub\": r\"Shock Sub:\\s*([^\\n;]+)\",\n",
    "        \"Total Length\": r\"Total Length:\\s*(\\d+)\\b\"\n",
    "    }\n",
    "    \n",
    "    # Extract data\n",
    "    bha_data = {}\n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, ocr_text)\n",
    "        if match:\n",
    "            bha_data[key] = match.group(1).strip()\n",
    "    \n",
    "    # **Fix duplication issue:** Remove Size, Wt./Ft, Connection, ID from `\"Drill Pipe Detail\"`\n",
    "    if \"Drill Pipe Detail\" in bha_data:\n",
    "        detail = bha_data[\"Drill Pipe Detail\"]\n",
    "        for remove_key in [\"Size\", \"Wt./Ft\", \"Connection\", \"ID\"]:\n",
    "            if remove_key in bha_data:\n",
    "                detail = re.sub(rf\"{remove_key}:\\s*{re.escape(bha_data[remove_key])}\", \"\", detail).strip(\",; \")\n",
    "        bha_data[\"Drill Pipe Detail\"] = detail  # Store cleaned version\n",
    "\n",
    "    # **Final structured JSON without repetition**\n",
    "    structured_data = {\n",
    "        \"BHA\": {\n",
    "            \"Drill Pipe Detail\": bha_data.get(\"Drill Pipe Detail\", \"\"),\n",
    "            \"Size\": bha_data.get(\"Size\", \"\"),\n",
    "            \"Wt./Ft\": bha_data.get(\"Wt./Ft\", \"\"),\n",
    "            \"Connection\": bha_data.get(\"Connection\", \"\"),\n",
    "            \"ID\": bha_data.get(\"ID\", \"\"),\n",
    "            \"BHA #4\": {\n",
    "                \"Drill Bit\": bha_data.get(\"Drill Bit\", \"\"),\n",
    "                \"Motor\": bha_data.get(\"Motor\", \"\"),\n",
    "                \"MWD Tool\": bha_data.get(\"MWD Tool\", \"\"),\n",
    "                \"Monel Collar\": bha_data.get(\"Monel Collar\", \"\"),\n",
    "                \"X-Over\": bha_data.get(\"X-Over\", \"\"),\n",
    "                \"Sub\": bha_data.get(\"Sub\", \"\"),\n",
    "                \"HWDP\": bha_data.get(\"HWDP\", \"\"),\n",
    "                \"Drill Pipe\": bha_data.get(\"Drill Pipe\", \"\"),\n",
    "                \"Reamer\": bha_data.get(\"Reamer\", \"\"),\n",
    "                \"Shock Sub\": bha_data.get(\"Shock Sub\", \"\")\n",
    "            },\n",
    "            \"Total Length\": bha_data.get(\"Total Length\", \"\")  #  Now correctly placed at the end\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return structured_data\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4) Parse Pumps Table\n",
    "# ------------------------------------------------------------------\n",
    "def parse_pumps_table(ocr_text):\n",
    "    \"\"\"\n",
    "    Parses the pumps table from the OCR text.\n",
    "    Expected lines look like:\n",
    "      Number Model Type   HHP  Efficiency  Stroke(in)  Liner(in)  P-Rating(psi)  P-Limit(psi)  SPM Rating  SPM Limit\n",
    "      1      BOMCO TRIPLEX 1600 95       12.000       4.75       7500           7100          120         110\n",
    "      2      BOMCO TRIPLEX 1600 95       12.000       4.75       7500           7100          120         110\n",
    "      (possibly missing fields in some rows)\n",
    "    \"\"\"\n",
    "\n",
    "    # Well search for lines that look like:\n",
    "    #   <Number> BOMCO TRIPLEX <HHP> <Eff> <Stroke> <Liner> <P-Rating> <P-Limit> <SPM Rating> <SPM Limit>\n",
    "    #   or possibly missing the Number or HHP.\n",
    "    # We'll capture them with a regex that checks for 8-11 columns.\n",
    "    # You can refine further as needed.\n",
    "    pump_pattern = re.compile(\n",
    "        r\"^(\\d+)?\\s*\"               # Number (optional)\n",
    "        r\"(BOMCO)\\s+(TRIPLEX)\\s+\"    # Model, Type\n",
    "        r\"(\\d+)?\\s*\"                 # HHP (optional)\n",
    "        r\"(\\d+)\\s+\"                  # Efficiency\n",
    "        r\"([\\d.]+)\\s+\"               # Stroke(in)\n",
    "        r\"([\\d.]+)\\s+\"               # Liner(in)\n",
    "        r\"(\\d+)\\s+\"                  # P-Rating(psi)\n",
    "        r\"(\\d+)\\s+\"                  # P-Limit(psi)\n",
    "        r\"(\\d+)\\s+\"                  # SPM Rating\n",
    "        r\"(\\d+)\\s*$\",                # SPM Limit\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    lines = ocr_text.splitlines()\n",
    "    pumps = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        match = pump_pattern.match(line)\n",
    "        if match:\n",
    "            # Extract fields\n",
    "            number, model, pump_type, hhp, efficiency, stroke, liner, p_rating, p_limit, spm_rating, spm_limit = match.groups()\n",
    "\n",
    "            # Store as dictionary\n",
    "            pumps.append({\n",
    "                \"Number\": number if number else \"\",\n",
    "                \"Model\": model,\n",
    "                \"Type\": pump_type,\n",
    "                \"HHP\": hhp if hhp else \"\",\n",
    "                \"Efficiency\": efficiency,\n",
    "                \"Stroke(in)\": stroke,\n",
    "                \"Liner(in)\": liner,\n",
    "                \"P-Rating(psi)\": p_rating,\n",
    "                \"P-Limit(psi)\": p_limit,\n",
    "                \"SPM Rating\": spm_rating,\n",
    "                \"SPM Limit\": spm_limit\n",
    "            })\n",
    "\n",
    "    return pumps\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5) Parse Drilling/Circ Rates\n",
    "# ------------------------------------------------------------------\n",
    "def parse_drilling_circ_rates(ocr_text):\n",
    "    \"\"\"\n",
    "    Parses lines like:\n",
    "      Drilling/Circ Rate 1 4325 PSI @ 134 SPM 2.63 Gal/Stoke 351.76 GPM 8.38 BPM 468.11 DC 340.61 DP\n",
    "      Drilling/Circ Rate 2 4475 PSI @ 134 SPM 2.63 Gal/Stoke 351.76 GPM 8.38 BPM 468.11 DC 340.61 DP\n",
    "    We'll store them in a structured list of dicts.\n",
    "    \"\"\"\n",
    "\n",
    "    # We'll define a pattern capturing Rate #, Pressure, SPM, Gal/Stoke, GPM, BPM, DC, DP, etc.\n",
    "    # Example line:\n",
    "    #   Drilling/Circ Rate 1 4325 PSI @ 134 SPM 2.63 Gal/Stoke 351.76 GPM 8.38 BPM 468.11 DC 340.61 DP\n",
    "    circ_pattern = re.compile(\n",
    "        r\"Drilling/Circ\\s+Rate\\s+(\\d+)\\s+(\\d+)\\s+PSI\\s*@\\s*(\\d+)\\s*SPM\\s*([\\d.]+)\\s+Gal/Stoke\\s+([\\d.]+)\\s+GPM\\s+([\\d.]+)\\s+BPM\\s+([\\d.]+)\\s+DC\\s+([\\d.]+)\\s+DP\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    lines = ocr_text.splitlines()\n",
    "    circ_rates = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        match = circ_pattern.search(line)\n",
    "        if match:\n",
    "            rate_id, pressure, spm, gal_stroke, gpm, bpm, dc, dp = match.groups()\n",
    "            circ_rates.append({\n",
    "                \"RateID\": rate_id,\n",
    "                \"Pressure(PSI)\": pressure,\n",
    "                \"SPM\": spm,\n",
    "                \"Gal/Stoke\": gal_stroke,\n",
    "                \"GPM\": gpm,\n",
    "                \"BPM\": bpm,\n",
    "                \"DC\": dc,\n",
    "                \"DP\": dp\n",
    "            })\n",
    "\n",
    "    return circ_rates\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main Function: Run All Pipelines in Sequence & Save Outputs\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_pipeline(name, pipeline_info, debug=False):\n",
    "    try:\n",
    "        logger.info(f\"Processing pipeline: {name}\")\n",
    "        output_json, df = pipeline_info[\"func\"](debug)\n",
    "        logger.info(f\"{name} DataFrame shape: {df.shape} (Rows: {df.shape[0]}, Columns: {df.shape[1]})\")\n",
    "        json_path = os.path.join(JSON_FOLDER, pipeline_info[\"json\"])\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(output_json, f, indent=4)\n",
    "        csv_path = os.path.join(CSV_FOLDER, pipeline_info[\"csv\"])\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        logger.info(f\"{name} saved: JSON({json_path}), CSV({csv_path})\")\n",
    "        print(f\"--- {name.upper()} DataFrame ---\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(df.head(10))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in pipeline {name}: {e}\")\n",
    "\n",
    "def main():\n",
    "    debug = False  # Set True for detailed logging\n",
    "    pipelines = {\n",
    "        \"cost_data\": {\n",
    "            \"func\": pipeline_cost_data,\n",
    "            \"csv\": \"cost_data.csv\",\n",
    "            \"json\": \"cost_data.json\"\n",
    "        },\n",
    "        \"well_job\": {\n",
    "            \"func\": lambda d: process_well_job_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_2.png\", d),\n",
    "            \"csv\": \"well_job_data.csv\",\n",
    "            \"json\": \"well_job_data.json\"\n",
    "        },\n",
    "        \"obs_int\": {\n",
    "            \"func\": lambda d: process_obs_int(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_10.png\", d),\n",
    "            \"csv\": \"obs_int_data.csv\",\n",
    "            \"json\": \"obs_int_data.json\"\n",
    "        },\n",
    "        \"bop\": {\n",
    "            \"func\": lambda d: process_bop(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_8.png\", d),\n",
    "            \"csv\": \"bop_data.csv\",\n",
    "            \"json\": \"bop_data.json\"\n",
    "        },\n",
    "        \"dir_info\": {\n",
    "            \"func\": lambda d: process_dir_info(\"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_5.png\", d),\n",
    "            \"csv\": \"dir_info_data.csv\",\n",
    "            \"json\": \"dir_info_data.json\"\n",
    "        },\n",
    "        \"survey\": {\n",
    "            \"func\": pipeline_survey_data,\n",
    "            \"csv\": \"survey_data.csv\",\n",
    "            \"json\": \"survey_data.json\"\n",
    "        },\n",
    "        \"casing\": {\n",
    "            \"func\": pipeline_casing_data,\n",
    "            \"csv\": \"casing_data.csv\",\n",
    "            \"json\": \"casing_data.json\"\n",
    "        },\n",
    "        \"consumables\": {\n",
    "            \"func\": pipeline_consumables_data,\n",
    "            \"csv\": \"consumables_data.csv\",\n",
    "            \"json\": \"consumables_data.json\"\n",
    "        },\n",
    "        \"mud\": {\n",
    "            \"func\": pipeline_mud_data,\n",
    "            \"csv\": \"mud_data.csv\",\n",
    "            \"json\": \"mud_data.json\"\n",
    "        },\n",
    "        \"bha\": {\n",
    "            \"func\": extract_bha_data,\n",
    "            \"csv\": \"bha_data.csv\",\n",
    "            \"json\": \"bha_data.json\"\n",
    "        },\n",
    "        \"pumps\": {\n",
    "            \"func\": parse_pumps_table + parse_drilling_circ_rates,\n",
    "            \"csv\": \"pumps_data.csv\",\n",
    "            \"json\": \"pumps_data.json\"\n",
    "        }\n",
    "    }\n",
    "    for name, pipe in pipelines.items():\n",
    "        run_pipeline(name, pipe, debug)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0e32e64-5933-4252-8c70-32e4644c668c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7760541951788028,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "preprocess_sections_final",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
