{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1930a03-bb23-4d42-8ba5-a587d4415f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3925ae-60f8-47d4-a2ab-8cbcb3dd5b8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logger Setup\n",
    "# ---------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"bit_infoExtractor\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: show_image\n",
    "# ---------------------------------------------------------------------\n",
    "def show_image(title, img, cmap=None, size=(10,10)):\n",
    "    plt.figure(figsize=size)\n",
    "    if cmap:\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    else:\n",
    "        if len(img.shape) == 3:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: annotate_ocr_results\n",
    "# ---------------------------------------------------------------------\n",
    "def annotate_ocr_results(img, roi_texts):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes and OCR text on the image for debugging.\n",
    "    \"\"\"\n",
    "    annotated = img.copy()\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        cv2.rectangle(annotated, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "        cv2.putText(annotated, text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    show_image(\"Annotated OCR Results\", annotated)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: safe_read_image\n",
    "# ---------------------------------------------------------------------\n",
    "def safe_read_image(img_path):\n",
    "    \"\"\"\n",
    "    Reads an image from a local or DBFS path.\n",
    "    \"\"\"\n",
    "    local_path = img_path.replace(\"dbfs:\", \"/dbfs\") if img_path.startswith(\"dbfs:\") else img_path\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {local_path}\")\n",
    "    return img\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: preprocess_image\n",
    "# ---------------------------------------------------------------------\n",
    "def preprocess_image(img, debug=False):\n",
    "    \"\"\"\n",
    "    Converts image to grayscale and applies adaptive thresholding.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if debug:\n",
    "        show_image(\"1) Grayscale\", gray, cmap=\"gray\")\n",
    "    \n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 15, 9\n",
    "    )\n",
    "    if debug:\n",
    "        show_image(\"2) Thresholded\", thresh, cmap=\"gray\")\n",
    "    return thresh\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: detect_text_regions\n",
    "# ---------------------------------------------------------------------\n",
    "def detect_text_regions(thresh_img, debug=False):\n",
    "    \"\"\"\n",
    "    Detects text regions (bounding boxes) from the thresholded image.\n",
    "    Only keeps bounding boxes larger than (width > 30, height > 15) and\n",
    "    sorts them top-to-bottom, then left-to-right.\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    rois = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "    \n",
    "    rois.sort(key=lambda b: (b[1], b[0]))\n",
    "    \n",
    "    if debug:\n",
    "        debug_img = cv2.cvtColor(thresh_img, cv2.COLOR_GRAY2BGR)\n",
    "        for (x, y, w, h) in rois:\n",
    "            cv2.rectangle(debug_img, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "        show_image(\"3) Detected Regions\", debug_img)\n",
    "    \n",
    "    return rois\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: perform_ocr_on_rois\n",
    "# ---------------------------------------------------------------------\n",
    "def perform_ocr_on_rois(img, rois, debug=False):\n",
    "    \"\"\"\n",
    "    Performs OCR on each detected text region. For each region, padding is added,\n",
    "    the image is lightly dilated and upscaled, then Tesseract is applied using\n",
    "    a whitelist (with a double-escaped backslash).\n",
    "    Returns a list of tuples: (x, y, w, h, text).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, (x, y, w, h) in enumerate(rois):\n",
    "        # Add padding for better context\n",
    "        pad = 5\n",
    "        x1 = max(x - pad, 0)\n",
    "        y1 = max(y - pad, 0)\n",
    "        x2 = min(x + w + pad, img.shape[1])\n",
    "        y2 = min(y + h + pad, img.shape[0])\n",
    "        roi = img[y1:y2, x1:x2]\n",
    "        \n",
    "        gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "        kernel = np.ones((1,1), np.uint8)\n",
    "        dilated = cv2.dilate(gray_roi, kernel, iterations=1)\n",
    "        thresh_roi = cv2.adaptiveThreshold(\n",
    "            dilated, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "            cv2.THRESH_BINARY, 15, 9\n",
    "        )\n",
    "        scaled_roi = cv2.resize(thresh_roi, None, fx=2, fy=2, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        config_str = (\n",
    "            \"--psm 6 \"\n",
    "            \"-c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "            \"abcdefghijklmnopqrstuvwxyz0123456789.,:;-_/\\\\\\\\\"\n",
    "        )\n",
    "        text = pytesseract.image_to_string(scaled_roi, config=config_str).strip()\n",
    "        \n",
    "        # If nothing is detected, check if the region is very narrow (likely an \"I\")\n",
    "        if not text:\n",
    "            if (w / h) < 0.3:\n",
    "                text = \"I\"\n",
    "            else:\n",
    "                text = \"[BLANK]\"\n",
    "        \n",
    "        results.append((x, y, w, h, text))\n",
    "        if debug:\n",
    "            logger.info(f\"OCR Box {i} => {text}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: build_bit_info_dict_from_rois (Dynamic Segmentation)\n",
    "# ---------------------------------------------------------------------\n",
    "def build_bit_info_dict_from_rois(roi_texts, debug=False):\n",
    "    \"\"\"\n",
    "    Dynamically segments OCR tokens into columns based on their x-coordinate.\n",
    "    The approach:\n",
    "      1. Group OCR results into rows based on y-coordinates.\n",
    "      2. For each row, retain each tokenâ€™s left x-coordinate and text.\n",
    "      3. For data rows (assumed to be rows 3+), determine the global min and max x,\n",
    "         then compute 22 equally spaced boundaries (for 21 columns).\n",
    "      4. For every row (header or data), assign tokens to columns based on these boundaries.\n",
    "      5. Map the 21 tokens per row to the final column names.\n",
    "    \"\"\"\n",
    "    # Group tokens into rows by y coordinate\n",
    "    row_tolerance = 10\n",
    "    rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, text))\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [(x, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        rows.append(current_row)\n",
    "    \n",
    "    # Sort tokens in each row by their x coordinate\n",
    "    for i in range(len(rows)):\n",
    "        rows[i].sort(key=lambda tup: tup[0])\n",
    "    \n",
    "    if debug:\n",
    "        for i, row in enumerate(rows):\n",
    "            logger.info(f\"Row {i}: {[text for (x, text) in row]}\")\n",
    "    \n",
    "    # Assume rows 0-2 are headers and rows 3+ are data rows.\n",
    "    header_rows = rows[:3]\n",
    "    data_rows = rows[3:]\n",
    "    \n",
    "    if not data_rows:\n",
    "        logger.warning(\"No data rows available for dynamic segmentation.\")\n",
    "        return [], pd.DataFrame()\n",
    "    \n",
    "    # Gather x coordinates from all tokens in the data rows to compute column boundaries\n",
    "    all_x = [x for row in data_rows for (x, _) in row]\n",
    "    min_x = min(all_x)\n",
    "    max_x = max(all_x)\n",
    "    # Compute 22 boundaries for 21 columns\n",
    "    boundaries = np.linspace(min_x, max_x, 22)\n",
    "    \n",
    "    def assign_tokens_to_columns(row_tokens):\n",
    "        cols = [\"\"] * 21\n",
    "        for (x, text) in row_tokens:\n",
    "            # Find the column index for the token based on its x coordinate\n",
    "            for i in range(21):\n",
    "                if boundaries[i] <= x < boundaries[i+1]:\n",
    "                    if cols[i]:\n",
    "                        cols[i] += \" \" + text\n",
    "                    else:\n",
    "                        cols[i] = text\n",
    "                    break\n",
    "        return cols\n",
    "    \n",
    "    # Process all rows using the computed boundaries\n",
    "    processed_rows = [assign_tokens_to_columns(row) for row in rows]\n",
    "    if debug:\n",
    "        for i, cols in enumerate(processed_rows):\n",
    "            logger.info(f\"Processed Row {i}: {cols}\")\n",
    "    \n",
    "    # Define final column names (21 columns expected)\n",
    "    final_columns = [\n",
    "        \"Bit #\", \"Size\", \"Make\", \"Model\", \"Serial #\",         # 5\n",
    "        \"Nozzle-(Number x Size)\", \"Nozzle-TFA\",                # 2\n",
    "        \"Depth-In\", \"Depth-Out\", \"Depth-Feet\", \"Depth-ROP\",    # 4\n",
    "        \"Hours-Total\", \"Hours-On Btm\",                         # 2\n",
    "        \"Dull Grade-I\", \"Dull Grade-O1\", \"Dull Grade-D\", \"Dull Grade-L\",\n",
    "        \"Dull Grade-B\", \"Dull Grade-G\", \"Dull Grade-O2\", \"Dull Grade-RP\"  # 8\n",
    "    ]\n",
    "    \n",
    "    # Build structured data only from the data rows (row index 3 onward)\n",
    "    structured_data = []\n",
    "    for cols in processed_rows[3:]:\n",
    "        if len(cols) < 21:\n",
    "            cols += [\"\"] * (21 - len(cols))\n",
    "        elif len(cols) > 21:\n",
    "            cols = cols[:21]\n",
    "        row_dict = {final_columns[i]: cols[i] for i in range(21)}\n",
    "        structured_data.append(row_dict)\n",
    "        if debug:\n",
    "            logger.info(f\"Parsed row => {row_dict}\")\n",
    "    \n",
    "    df = pd.DataFrame(structured_data)\n",
    "    if debug:\n",
    "        logger.info(\"DataFrame Preview:\")\n",
    "        logger.info(df.head())\n",
    "    \n",
    "    df.to_csv(\"bit_info_data.csv\", index=False)\n",
    "    logger.info(\"Data saved successfully as CSV.\")\n",
    "    \n",
    "    structured_data_json = df.to_dict(orient='records')\n",
    "    with open(\"bit_info_data.json\", \"w\") as json_file:\n",
    "        json.dump(structured_data_json, json_file, indent=4)\n",
    "    logger.info(\"Data saved successfully in JSON format.\")\n",
    "    \n",
    "    return structured_data_json, df\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main Pipeline\n",
    "# ---------------------------------------------------------------------\n",
    "def main_bit_info_pipeline():\n",
    "    \"\"\"\n",
    "    Main pipeline for extracting the BIT DETAILS table.\n",
    "    \"\"\"\n",
    "    # Replace with your actual image path\n",
    "    bit_info_img_path = \"/dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_6.png\"\n",
    "    \n",
    "    try:\n",
    "        img = safe_read_image(bit_info_img_path)\n",
    "        logger.info(\"Image loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        return\n",
    "    \n",
    "    # 1) Preprocess the image\n",
    "    thresh_img = preprocess_image(img, debug=True)\n",
    "    \n",
    "    # 2) Detect text regions\n",
    "    rois = detect_text_regions(thresh_img, debug=True)\n",
    "    \n",
    "    # 3) Perform OCR on each region\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=True)\n",
    "    \n",
    "    # 4) Annotate the OCR results on the image\n",
    "    annotate_ocr_results(img, roi_texts)\n",
    "    \n",
    "    # 5) Dynamically build structured data from OCR results\n",
    "    bit_info_list, df = build_bit_info_dict_from_rois(roi_texts, debug=True)\n",
    "    \n",
    "    # 6) Log and print final JSON and DataFrame\n",
    "    final_output = {\"BIT DETAILS\": bit_info_list}\n",
    "    logger.info(json.dumps(final_output, indent=4))\n",
    "    print(df)\n",
    "    \n",
    "    # 7) Save final results\n",
    "    output_folder = \"/dbfs/mnt/mini-proj-dd/final_bit_info_results\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(os.path.join(output_folder, \"bit_info_data.json\"), \"w\") as f:\n",
    "        json.dump(final_output, f, indent=4)\n",
    "    df.to_csv(os.path.join(output_folder, \"bit_info_data.csv\"), index=False)\n",
    "    logger.info(\"Data saved successfully in output folder.\")\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    main_bit_info_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2eb186-716d-448a-aef3-7a00bc637926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2dbbfb8-1412-4d09-af54-0cd00a96122f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70596c5e-7ad4-46a3-a5be-a652605a26a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logger Setup\n",
    "# ---------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"bit_infoExtractor\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: show_image\n",
    "# ---------------------------------------------------------------------\n",
    "def show_image(title, img, cmap=None, size=(10,10)):\n",
    "    plt.figure(figsize=size)\n",
    "    if cmap:\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    else:\n",
    "        if len(img.shape) == 3:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# annotate_ocr_results\n",
    "# ---------------------------------------------------------------------\n",
    "def annotate_ocr_results(img, roi_texts):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes and OCR text on the image for debugging.\n",
    "    \"\"\"\n",
    "    annotated = img.copy()\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        cv2.rectangle(annotated, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "        cv2.putText(annotated, text, (x, y-5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    show_image(\"Annotated OCR Results\", annotated)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# safe_read_image\n",
    "# ---------------------------------------------------------------------\n",
    "def safe_read_image(img_path):\n",
    "    \"\"\"\n",
    "    Reads an image from a local or DBFS path.\n",
    "    \"\"\"\n",
    "    local_path = img_path.replace(\"dbfs:\", \"/dbfs\") if img_path.startswith(\"dbfs:\") else img_path\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {local_path}\")\n",
    "    return img\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# preprocess_image\n",
    "# ---------------------------------------------------------------------\n",
    "def preprocess_image(img, debug=False):\n",
    "    \"\"\"\n",
    "    Converts image to grayscale, applies morphological closing,\n",
    "    then applies adaptive thresholding.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if debug:\n",
    "        show_image(\"1) Grayscale\", gray, cmap=\"gray\")\n",
    "\n",
    "    # Morphological closing to help keep thin strokes (like 'I')\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))\n",
    "    closed = cv2.morphologyEx(gray, cv2.MORPH_CLOSE, kernel)\n",
    "    if debug:\n",
    "        show_image(\"2) After Morphological Closing\", closed, cmap=\"gray\")\n",
    "\n",
    "    # Adaptive threshold\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        closed, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 15, 9\n",
    "    )\n",
    "    if debug:\n",
    "        show_image(\"3) Thresholded\", thresh, cmap=\"gray\")\n",
    "    return thresh\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# detect_text_regions\n",
    "# ---------------------------------------------------------------------\n",
    "def detect_text_regions(thresh_img, debug=True):\n",
    "    \"\"\"\n",
    "    Detects text regions (bounding boxes) from the thresholded image.\n",
    "    We keep bounding boxes > (w=30, h=15).\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    rois = []\n",
    "    debug_img = cv2.cvtColor(thresh_img, cv2.COLOR_GRAY2BGR)\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "            cv2.rectangle(debug_img, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "    rois.sort(key=lambda b: (b[1], b[0]))\n",
    "    if debug:\n",
    "        logger.info(f\"[detect_text_regions] Found {len(rois)} bounding boxes passing size filter.\")\n",
    "        show_image(\"3) Detected Text Regions\", debug_img, size=(12,12))\n",
    "    return rois\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# perform_ocr_on_rois\n",
    "# ---------------------------------------------------------------------\n",
    "def perform_ocr_on_rois(img, rois, debug=True):\n",
    "    \"\"\"\n",
    "    Performs OCR on each detected text region with Tesseract config.\n",
    "    If Tesseract returns blank for a suspiciously narrow ROI,\n",
    "    we do a second pass with morphological dilation + PSM 10\n",
    "    (single char mode).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    n = len(rois)\n",
    "\n",
    "    if debug and n > 0:\n",
    "        cols = 5\n",
    "        rows = math.ceil(n / cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "        axes = axes.flatten() if rows > 1 else [axes]\n",
    "\n",
    "    # Normal Tesseract config (PSM 6, with a whitelist)\n",
    "    normal_config = (\n",
    "        \"--psm 6 \"\n",
    "        \"-c tessedit_char_whitelist=\"\n",
    "        \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "        \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        \"0123456789_./\\\\-+\"\n",
    "    )\n",
    "\n",
    "    # Second-pass config (PSM 10 for single character)\n",
    "    single_char_config = (\n",
    "        \"--psm 10 \"\n",
    "        \"-c tessedit_char_whitelist=\"\n",
    "        \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "        \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        \"0123456789_./\\\\-+\"\n",
    "    )\n",
    "\n",
    "    for i, (x, y, w, h) in enumerate(rois):\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "\n",
    "        # --- First pass OCR ---\n",
    "        text = pytesseract.image_to_string(roi, config=normal_config).strip()\n",
    "        if debug:\n",
    "            logger.info(f\"[perform_ocr_on_rois] ROI {i} (x={x}, y={y}, w={w}, h={h}) => First pass: '{text}'\")\n",
    "\n",
    "        # If blank and bounding box is narrow (e.g. w < 20),\n",
    "        # try second pass with morphological dilation + PSM 10\n",
    "        if (not text) and (w < 20):\n",
    "            if debug:\n",
    "                logger.info(f\"[perform_ocr_on_rois] ROI {i} => First pass blank & w<20; attempting second pass.\")\n",
    "\n",
    "            # Dilate the ROI a bit to emphasize thin strokes\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2,2))\n",
    "            roi_dilated = cv2.dilate(roi, kernel, iterations=1)\n",
    "\n",
    "            # Second pass\n",
    "            text2 = pytesseract.image_to_string(roi_dilated, config=single_char_config).strip()\n",
    "            if debug:\n",
    "                logger.info(f\"[perform_ocr_on_rois] ROI {i} => Second pass result: '{text2}'\")\n",
    "\n",
    "            if text2:\n",
    "                text = text2  # Use second-pass result\n",
    "            else:\n",
    "                text = \"[BLANK]\"\n",
    "        elif not text:\n",
    "            # If blank but not narrow, keep blank\n",
    "            text = \"[BLANK]\"\n",
    "\n",
    "        results.append((x, y, w, h, text))\n",
    "\n",
    "        if debug and i < len(axes):\n",
    "            roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "            axes[i].imshow(roi_rgb)\n",
    "            axes[i].set_title(f\"ROI {i+1}\\n{text[:30]}...\")\n",
    "            axes[i].axis(\"off\")\n",
    "\n",
    "    if debug and n > 0:\n",
    "        # Turn off any extra subplot axes\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            axes[j].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# build_bit_info_dict_from_rois\n",
    "# ---------------------------------------------------------------------\n",
    "def build_bit_info_dict_from_rois(roi_texts, debug=True):\n",
    "    \"\"\"\n",
    "    Groups bounding boxes into rows, merges text in left->right order,\n",
    "    then parses them into a final structure.\n",
    "    \"\"\"\n",
    "    row_tolerance = 3  # keep it small to avoid merging separate lines\n",
    "    grouped_rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            grouped_rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        grouped_rows.append(current_row)\n",
    "\n",
    "    row_strings = []\n",
    "    for i, row_cells in enumerate(grouped_rows):\n",
    "        row_cells.sort(key=lambda c: c[0])  # left->right\n",
    "        line = \" \".join(cell[4] for cell in row_cells)\n",
    "        line = line.replace(\"\\n\", \" \").strip()\n",
    "        if debug:\n",
    "            logger.info(f\"[build_bit_info_dict_from_rois] Row {i} => {line}\")\n",
    "        row_strings.append(line)\n",
    "\n",
    "    if len(row_strings) < 3:\n",
    "        logger.warning(\"[build_bit_info_dict_from_rois] Not enough rows found for this layout.\")\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "    # Row 0 => Title\n",
    "    super_header_line = row_strings[1] if len(row_strings) > 1 else \"\"\n",
    "    sub_header_line   = row_strings[2] if len(row_strings) > 2 else \"\"\n",
    "    data_lines        = row_strings[3:]  # subsequent lines\n",
    "\n",
    "    if debug:\n",
    "        logger.info(f\"[build_bit_info_dict_from_rois] Super Headers => {super_header_line}\")\n",
    "        logger.info(f\"[build_bit_info_dict_from_rois] Sub Headers => {sub_header_line}\")\n",
    "        logger.info(f\"[build_bit_info_dict_from_rois] Data Lines => {data_lines}\")\n",
    "\n",
    "    final_columns = [\n",
    "        \"Bit #\", \"Size\", \"Make\", \"Model\", \"Serial #\",         # 5\n",
    "        \"Nozzle-(Number x Size)\", \"Nozzle-TFA\",               # 2\n",
    "        \"Depth-In\", \"Depth-Out\", \"Depth-Feet\", \"Depth-ROP\",   # 4\n",
    "        \"Hours-Total\", \"Hours-On Btm\",                        # 2\n",
    "        \"Dull Grade-I\", \"Dull Grade-O1\", \"Dull Grade-D\", \"Dull Grade-L\",\n",
    "        \"Dull Grade-B\", \"Dull Grade-G\", \"Dull Grade-O2\", \"Dull Grade-RP\"  # 8\n",
    "    ]\n",
    "    expected_token_count = len(final_columns)\n",
    "\n",
    "    structured_data = []\n",
    "    for line in data_lines:\n",
    "        tokens = line.split()\n",
    "        if len(tokens) < expected_token_count:\n",
    "            tokens += [\"[BLANK]\"] * (expected_token_count - len(tokens))\n",
    "        elif len(tokens) > expected_token_count:\n",
    "            tokens = tokens[:expected_token_count]\n",
    "\n",
    "        row_dict = {}\n",
    "        for col_idx, col_name in enumerate(final_columns):\n",
    "            row_dict[col_name] = tokens[col_idx] if col_idx < len(tokens) else \"[BLANK]\"\n",
    "        if debug:\n",
    "            logger.info(f\"[build_bit_info_dict_from_rois] Parsed row => {row_dict}\")\n",
    "        structured_data.append(row_dict)\n",
    "\n",
    "    df = pd.DataFrame(structured_data)\n",
    "\n",
    "    # No forced \"I\" fix here (fix_suspicious_I removed)\n",
    "\n",
    "    if debug:\n",
    "        logger.info(\"[build_bit_info_dict_from_rois] DataFrame Preview:\")\n",
    "        logger.info(df.head().to_string())\n",
    "\n",
    "    df.to_csv(\"bit_info_data.csv\", index=False)\n",
    "    logger.info(\"Data saved successfully as CSV.\")\n",
    "\n",
    "    structured_data_json = df.to_dict(orient='records')\n",
    "    with open(\"bit_info_data.json\", \"w\") as json_file:\n",
    "        json.dump(structured_data_json, json_file, indent=4)\n",
    "    logger.info(\"Data saved successfully in JSON format.\")\n",
    "\n",
    "    return structured_data_json, df\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# main_bit_info_pipeline\n",
    "# ---------------------------------------------------------------------\n",
    "def main_bit_info_pipeline():\n",
    "    \"\"\"\n",
    "    Main pipeline for extracting the BIT DETAILS table from your layout.\n",
    "    \"\"\"\n",
    "    # Replace with your actual path\n",
    "    bit_info_img_path = \"/dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_6.png\"\n",
    "\n",
    "    try:\n",
    "        img = safe_read_image(bit_info_img_path)\n",
    "        logger.info(\"Image loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        return\n",
    "\n",
    "    # 1) Preprocess\n",
    "    thresh_img = preprocess_image(img, debug=True)\n",
    "\n",
    "    # 2) Detect bounding boxes\n",
    "    rois = detect_text_regions(thresh_img, debug=True)\n",
    "\n",
    "    # 3) Perform OCR\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=True)\n",
    "\n",
    "    # 4) Annotate and show OCR results on the image\n",
    "    annotate_ocr_results(img, roi_texts)\n",
    "\n",
    "    # 5) Build structured data\n",
    "    bit_info_list, df = build_bit_info_dict_from_rois(roi_texts, debug=True)\n",
    "\n",
    "    # 6) Show final JSON in logs\n",
    "    final_output = {\"BIT DETAILS\": bit_info_list}\n",
    "    logger.info(json.dumps(final_output, indent=4))\n",
    "    print(df)\n",
    "\n",
    "    # 7) Save final results\n",
    "    output_folder = \"/dbfs/mnt/mini-proj-dd/final_bit_info_results\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(os.path.join(output_folder, \"bit_info_data.json\"), \"w\") as f:\n",
    "        json.dump(final_output, f, indent=4)\n",
    "    df.to_csv(os.path.join(output_folder, \"bit_info_data.csv\"), index=False)\n",
    "    logger.info(\"Data saved successfully in output folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_bit_info_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e462ad9-f6eb-4d29-bfe7-53d0f2871522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logger Setup\n",
    "# ---------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"bit_infoExtractor\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: show_image\n",
    "# ---------------------------------------------------------------------\n",
    "def show_image(title, img, cmap=None, size=(10,10)):\n",
    "    plt.figure(figsize=size)\n",
    "    if cmap:\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    else:\n",
    "        if len(img.shape) == 3:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# New Utility: annotate_ocr_results\n",
    "# ---------------------------------------------------------------------\n",
    "def annotate_ocr_results(img, roi_texts):\n",
    "    \"\"\"\n",
    "    Draws bounding boxes and OCR text on the image for debugging.\n",
    "    \"\"\"\n",
    "    annotated = img.copy()\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        cv2.rectangle(annotated, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "        # Place the OCR text above the bounding box; adjust as needed.\n",
    "        cv2.putText(annotated, text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    show_image(\"Annotated OCR Results\", annotated)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# safe_read_image\n",
    "# ---------------------------------------------------------------------\n",
    "def safe_read_image(img_path):\n",
    "    \"\"\"\n",
    "    Reads an image from a local or DBFS path.\n",
    "    \"\"\"\n",
    "    local_path = img_path.replace(\"dbfs:\", \"/dbfs\") if img_path.startswith(\"dbfs:\") else img_path\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Failed to load image: {local_path}\")\n",
    "    return img\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# preprocess_image\n",
    "# ---------------------------------------------------------------------\n",
    "def preprocess_image(img, debug=False):\n",
    "    \"\"\"\n",
    "    Converts image to grayscale and applies adaptive thresholding.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if debug:\n",
    "        show_image(\"1) Grayscale\", gray, cmap=\"gray\")\n",
    "    \n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 15, 9\n",
    "    )\n",
    "    if debug:\n",
    "        show_image(\"2) Thresholded\", thresh, cmap=\"gray\")\n",
    "    return thresh\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# detect_text_regions\n",
    "# ---------------------------------------------------------------------\n",
    "def detect_text_regions(thresh_img, debug=False):\n",
    "    \"\"\"\n",
    "    Detects text regions (bounding boxes) from the thresholded image.\n",
    "    Only keeps bounding boxes larger than (width>30, height>15).\n",
    "    Sort them top-to-bottom, then left-to-right.\n",
    "    \"\"\"\n",
    "    contours, _ = cv2.findContours(thresh_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    rois = []\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w > 30 and h > 15:\n",
    "            rois.append((x, y, w, h))\n",
    "    \n",
    "    rois.sort(key=lambda b: (b[1], b[0]))  # top->bottom, then left->right\n",
    "\n",
    "    if debug:\n",
    "        debug_img = cv2.cvtColor(thresh_img, cv2.COLOR_GRAY2BGR)\n",
    "        for (x, y, w, h) in rois:\n",
    "            cv2.rectangle(debug_img, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "        show_image(\"3) Detected Regions\", debug_img)\n",
    "    \n",
    "    return rois\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# perform_ocr_on_rois\n",
    "# ---------------------------------------------------------------------\n",
    "def perform_ocr_on_rois(img, rois, debug=False):\n",
    "    \"\"\"\n",
    "    Performs OCR on each detected text region.\n",
    "    Returns list of tuples: (x, y, w, h, text).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, (x, y, w, h) in enumerate(rois):\n",
    "        roi = img[y:y+h, x:x+w]\n",
    "        # Using a whitelist can be added here if needed:\n",
    "        # config_str = \"--psm 6 -c tessedit_char_whitelist=ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "        text = pytesseract.image_to_string(roi, config=\"--psm 6\").strip() or \"[BLANK]\"\n",
    "        results.append((x, y, w, h, text))\n",
    "        if debug:\n",
    "            logger.info(f\"OCR Box {i}: {text}\")\n",
    "    return results\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# build_bit_info_dict_from_rois\n",
    "# ---------------------------------------------------------------------\n",
    "def build_bit_info_dict_from_rois(roi_texts, debug=False):\n",
    "    \"\"\"\n",
    "    Custom parsing for the multi-row header layout:\n",
    "      Row 0 => Table Title (e.g. \"DRILL BITS ...\")\n",
    "      Row 1 => Super Headers: \"Bit Data  Nozzles  Depth  Hours  Dull Grade\"\n",
    "      Row 2 => Sub-headers:   \"Bit # Size Make Model Serial #  Number x Size TFA  In Out Feet ROP  Total On Btm  I oO D L B G oO RP\"\n",
    "      Row 3 => Data row #1\n",
    "      Row 4 => Data row #2\n",
    "      ...\n",
    "    We'll parse row 1 and row 2 to define column groups. Then parse each subsequent row in chunks.\n",
    "    \"\"\"\n",
    "    # Step 1) Group bounding boxes by y-coordinate\n",
    "    row_tolerance = 10\n",
    "    grouped_rows = []\n",
    "    current_row = []\n",
    "    prev_y = None\n",
    "\n",
    "    for (x, y, w, h, text) in roi_texts:\n",
    "        if prev_y is None or abs(y - prev_y) <= row_tolerance:\n",
    "            current_row.append((x, y, w, h, text))\n",
    "        else:\n",
    "            grouped_rows.append(current_row)\n",
    "            current_row = [(x, y, w, h, text)]\n",
    "        prev_y = y\n",
    "    if current_row:\n",
    "        grouped_rows.append(current_row)\n",
    "\n",
    "    # Step 2) Convert each row group into a single string\n",
    "    row_strings = []\n",
    "    for i, row_cells in enumerate(grouped_rows):\n",
    "        row_cells.sort(key=lambda c: c[0])  # left->right\n",
    "        line = \" \".join(cell[4] for cell in row_cells)\n",
    "        line = line.replace(\"\\n\", \" \").strip()  # flatten\n",
    "        row_strings.append(line)\n",
    "        if debug:\n",
    "            logger.info(f\"Row {i} => {line}\")\n",
    "\n",
    "    # We expect something like:\n",
    "    # Row 0 => \"DRILL BITS DRILL BITS [BLANK]\"\n",
    "    # Row 1 => \"Bit Data Nozzles Depth Hours Dull Grade\"\n",
    "    # Row 2 => \"Bit # Size Make Model Serial # Number x Size TFA In Out Feet ROP Total On Btm I oO D L B G oO RP\"\n",
    "    # Row 3 => \"4 6.750 BAKER DD40+TWS 5355166 6X12 0.66 ...\"\n",
    "    # Row 4 => \"3 9.875 REED TKS6-H1 A308739 7X12 0.77 ...\"\n",
    "\n",
    "    # Step 3) Identify the row indices for:\n",
    "    #  - Title (row 0)\n",
    "    #  - Super headers (row 1)\n",
    "    #  - Sub-headers (row 2)\n",
    "    #  - Data rows (row 3, 4, ...)\n",
    "    if len(row_strings) < 3:\n",
    "        logger.warning(\"Not enough rows found for this layout.\")\n",
    "        return [], pd.DataFrame()\n",
    "\n",
    "    # We'll skip row 0 (table title).\n",
    "    super_header_line = row_strings[1] if len(row_strings) > 1 else \"\"\n",
    "    sub_header_line   = row_strings[2] if len(row_strings) > 2 else \"\"\n",
    "    data_lines        = row_strings[3:]  # everything after row 2\n",
    "\n",
    "    if debug:\n",
    "        logger.info(f\"Super Headers => {super_header_line}\")\n",
    "        logger.info(f\"Sub Headers => {sub_header_line}\")\n",
    "        logger.info(f\"Data Lines => {data_lines}\")\n",
    "\n",
    "    # Step 4) Define the \"super header\" groups and sub-headers\n",
    "    # We'll do a simpler approach: we know how many tokens each group has:\n",
    "    #  Bit Data => 5, Nozzles => 2, Depth => 4, Hours => 2, Dull Grade => 8 (Total = 21)\n",
    "    final_columns = [\n",
    "        \"Bit #\", \"Size\", \"Make\", \"Model\", \"Serial #\",         # 5\n",
    "        \"Nozzle-(Number x Size)\", \"Nozzle-TFA\",               # 2\n",
    "        \"Depth-In\", \"Depth-Out\", \"Depth-Feet\", \"Depth-ROP\",   # 4\n",
    "        \"Hours-Total\", \"Hours-On Btm\",                        # 2\n",
    "        \"Dull Grade-I\", \"Dull Grade-O1\", \"Dull Grade-D\", \"Dull Grade-L\", \n",
    "        \"Dull Grade-B\", \"Dull Grade-G\", \"Dull Grade-O2\", \"Dull Grade-RP\"  # 8\n",
    "    ]\n",
    "\n",
    "    # Step 5) Parse each data row in chunks of 21 tokens\n",
    "    structured_data = []\n",
    "    for line in data_lines:\n",
    "        tokens = line.split()\n",
    "        # We expect 21 tokens per data row; pad or truncate if necessary\n",
    "        if len(tokens) < 21:\n",
    "            tokens += [\"\"] * (21 - len(tokens))\n",
    "        elif len(tokens) > 21:\n",
    "            tokens = tokens[:21]\n",
    "\n",
    "        row_dict = {}\n",
    "        for col_idx, col_name in enumerate(final_columns):\n",
    "            row_dict[col_name] = tokens[col_idx] if col_idx < len(tokens) else \"\"\n",
    "\n",
    "        structured_data.append(row_dict)\n",
    "        if debug:\n",
    "            logger.info(f\"Parsed row => {row_dict}\")\n",
    "\n",
    "    # Step 6) Convert to DataFrame and JSON\n",
    "    df = pd.DataFrame(structured_data)\n",
    "    if debug:\n",
    "        logger.info(\"DataFrame Preview:\")\n",
    "        logger.info(df.head())\n",
    "\n",
    "    df.to_csv(\"bit_info_data.csv\", index=False)\n",
    "    logger.info(\"Data saved successfully as CSV.\")\n",
    "\n",
    "    structured_data_json = df.to_dict(orient='records')\n",
    "    with open(\"bit_info_data.json\", \"w\") as json_file:\n",
    "        json.dump(structured_data_json, json_file, indent=4)\n",
    "    logger.info(\"Data saved successfully in JSON format.\")\n",
    "\n",
    "    return structured_data_json, df\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# main_bit_info_pipeline\n",
    "# ---------------------------------------------------------------------\n",
    "def main_bit_info_pipeline():\n",
    "    \"\"\"\n",
    "    Main pipeline for extracting the BIT DETAILS table from your layout.\n",
    "    \"\"\"\n",
    "    # Replace with your actual path\n",
    "    bit_info_img_path = \"/dbfs/mnt/mini-proj-dd/cropped_sections/page_1_section_6.png\"\n",
    "\n",
    "    try:\n",
    "        img = safe_read_image(bit_info_img_path)\n",
    "        logger.info(\"Image loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        return\n",
    "\n",
    "    # 1) Preprocess\n",
    "    thresh_img = preprocess_image(img, debug=True)\n",
    "\n",
    "    # 2) Detect bounding boxes\n",
    "    rois = detect_text_regions(thresh_img, debug=True)\n",
    "\n",
    "    # 3) Perform OCR\n",
    "    roi_texts = perform_ocr_on_rois(img, rois, debug=True)\n",
    "    \n",
    "    # --- New Step: Annotate and show OCR results on the image ---\n",
    "    annotate_ocr_results(img, roi_texts)\n",
    "\n",
    "    # 4) Build structured data (tailored to your table layout)\n",
    "    bit_info_list, df = build_bit_info_dict_from_rois(roi_texts, debug=True)\n",
    "\n",
    "    # 5) Show final JSON in logs\n",
    "    final_output = {\"BIT DETAILS\": bit_info_list}\n",
    "    logger.info(json.dumps(final_output, indent=4))\n",
    "    print(df)\n",
    "\n",
    "    # 6) Save final results\n",
    "    output_folder = \"/dbfs/mnt/mini-proj-dd/final_bit_info_results\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    with open(os.path.join(output_folder, \"bit_info_data.json\"), \"w\") as f:\n",
    "        json.dump(final_output, f, indent=4)\n",
    "    df.to_csv(os.path.join(output_folder, \"bit_info_data.csv\"), index=False)\n",
    "    logger.info(\"Data saved successfully in output folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_bit_info_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa9e19b-a8dd-47ce-8de9-f72afc9c2a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4230086100460830,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "preprocess_sections",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
