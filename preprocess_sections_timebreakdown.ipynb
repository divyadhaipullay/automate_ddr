{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1930a03-bb23-4d42-8ba5-a587d4415f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6412e4c-f1d1-4d6a-8cde-3a946ce4918d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import cv2\n",
    "# import pytesseract\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "# import json\n",
    "# import matplotlib.pyplot as plt\n",
    "# from datetime import datetime\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # Logger Setup\n",
    "# # ---------------------------------------------------------------------\n",
    "# logger = logging.getLogger(\"timeBreakdownExtractor\")\n",
    "# logger.setLevel(logging.DEBUG)  # Set debug level for detailed logs\n",
    "# if not logger.handlers:\n",
    "#     handler = logging.StreamHandler()\n",
    "#     handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "#     logger.addHandler(handler)\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # Utility: show_image\n",
    "# # ---------------------------------------------------------------------\n",
    "# def show_image(title, img, cmap=None, size=(10,10)):\n",
    "#     plt.figure(figsize=size)\n",
    "#     if cmap:\n",
    "#         plt.imshow(img, cmap=cmap)\n",
    "#     else:\n",
    "#         if len(img.shape) == 3:\n",
    "#             plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "#         else:\n",
    "#             plt.imshow(img, cmap=\"gray\")\n",
    "#     plt.title(title)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # safe_read_image\n",
    "# # ---------------------------------------------------------------------\n",
    "# def safe_read_image(img_path):\n",
    "#     \"\"\"Reads an image from a DBFS or local path.\"\"\"\n",
    "#     if img_path.startswith(\"dbfs:\"):\n",
    "#         local_path = img_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "#     else:\n",
    "#         local_path = img_path\n",
    "#     logger.info(f\"Trying to read image from: {local_path}\")\n",
    "#     if not os.path.exists(local_path):\n",
    "#         logger.error(f\"File not found: {local_path}\")\n",
    "#         raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "#     img = cv2.imread(local_path)\n",
    "#     if img is None:\n",
    "#         logger.error(f\"OpenCV failed to read the image: {local_path}\")\n",
    "#         raise FileNotFoundError(f\"OpenCV failed to read the image: {local_path}\")\n",
    "#     logger.info(\"Image loaded successfully.\")\n",
    "#     return img\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # preprocess_image\n",
    "# # ---------------------------------------------------------------------\n",
    "# def preprocess_image(img, debug=False):\n",
    "#     \"\"\"Converts image to grayscale and applies adaptive thresholding.\"\"\"\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     if debug:\n",
    "#         show_image(\"1) Grayscale\", gray, cmap=\"gray\")\n",
    "#     thresh = cv2.adaptiveThreshold(\n",
    "#         gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "#         cv2.THRESH_BINARY, 15, 9\n",
    "#     )\n",
    "#     if debug:\n",
    "#         show_image(\"2) Thresholded\", thresh, cmap=\"gray\")\n",
    "#     logger.debug(\"Preprocessing complete.\")\n",
    "#     return thresh\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # single_box_ocr\n",
    "# # ---------------------------------------------------------------------\n",
    "# def single_box_ocr(img, debug=False):\n",
    "#     \"\"\"Performs OCR on the entire image region.\"\"\"\n",
    "#     h, w = img.shape[:2]\n",
    "#     roi = img[0:h, 0:w]\n",
    "#     text = pytesseract.image_to_string(roi, config=\"--psm 6\")\n",
    "#     logger.debug(f\"OCR produced {len(text)} characters.\")\n",
    "#     if debug:\n",
    "#         preview = \"\\n\".join(text.splitlines()[:5])\n",
    "#         logger.debug(\"OCR text preview:\\n\" + preview)\n",
    "#     return text\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # compute_hours\n",
    "# # ---------------------------------------------------------------------\n",
    "# def compute_hours(from_time, to_time):\n",
    "#     \"\"\"Computes the difference in hours between two times formatted as HH:MM.\"\"\"\n",
    "#     fmt = \"%H:%M\"\n",
    "#     try:\n",
    "#         t1 = datetime.strptime(from_time, fmt)\n",
    "#         t2 = datetime.strptime(to_time, fmt)\n",
    "#         # Handle midnight crossing: if to_time is less than from_time, add one day.\n",
    "#         if t2 < t1:\n",
    "#             t2 = t2.replace(day=t2.day+1)\n",
    "#         diff = t2 - t1\n",
    "#         hours = diff.seconds / 3600.0\n",
    "#         return f\"{hours:.2f}\"\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error computing hours: {e}\")\n",
    "#         return \"[UNKNOWN]\"\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # parse_operations_description\n",
    "# # ---------------------------------------------------------------------\n",
    "# def parse_operations_description(ops_text):\n",
    "#     \"\"\"\n",
    "#     Parses the operations description text into a structured dictionary.\n",
    "#     This template uses regex to extract key metrics; adjust as needed.\n",
    "#     \"\"\"\n",
    "#     ops_data = {\n",
    "#         \"Depth\": {\"From\": \"\", \"To\": \"\"},\n",
    "#         \"Performance\": {\"Feet\": \"\", \"FPH\": \"\"},\n",
    "#         \"Rotation_Slide\": {\"Rotate\": \"\", \"Slide\": \"\"},\n",
    "#         \"Rotation_Time\": {\"Rotate Time\": \"\", \"Slide Time\": \"\"},\n",
    "#         \"GPM\": \"\",\n",
    "#         \"MTR RPM\": \"\",\n",
    "#         \"SPP\": \"\",\n",
    "#         \"DIFF\": \"\",\n",
    "#         \"WOB\": \"\",\n",
    "#         \"ROT RPM\": \"\",\n",
    "#         \"ON BTM TRQ\": \"\",\n",
    "#         \"OFF BTM TRQ\": \"\",\n",
    "#         \"GAS\": {\"Units\": \"\", \"Flare\": \"\"},\n",
    "#         \"MW\": {\"In\": \"\", \"Out\": \"\"},\n",
    "#         \"Targets\": [],\n",
    "#         \"Observations\": []\n",
    "#     }\n",
    "#     lines = ops_text.splitlines()\n",
    "#     logger.debug(f\"Parsing operations description with {len(lines)} lines.\")\n",
    "#     for line in lines:\n",
    "#         l = line.strip()\n",
    "#         if not l:\n",
    "#             continue\n",
    "\n",
    "#         depth_match = re.search(r\"F/\\s*([\\d,']+)\\s*T/\\s*([\\d,']+)\", l, re.IGNORECASE)\n",
    "#         if depth_match:\n",
    "#             ops_data[\"Depth\"][\"From\"] = depth_match.group(1)\n",
    "#             ops_data[\"Depth\"][\"To\"] = depth_match.group(2)\n",
    "#         perf_match = re.search(r\"\\(([\\d,']+)\\s*@\\s*(\\d+)\\s*FPH\\)\", l, re.IGNORECASE)\n",
    "#         if perf_match:\n",
    "#             ops_data[\"Performance\"][\"Feet\"] = perf_match.group(1)\n",
    "#             ops_data[\"Performance\"][\"FPH\"] = perf_match.group(2)\n",
    "#         rs_match = re.search(r\"ROTATE\\s*([\\d.]+%)\\s*/\\s*SLIDE\\s*([\\d.]+%)\", l, re.IGNORECASE)\n",
    "#         if rs_match:\n",
    "#             ops_data[\"Rotation_Slide\"][\"Rotate\"] = rs_match.group(1)\n",
    "#             ops_data[\"Rotation_Slide\"][\"Slide\"] = rs_match.group(2)\n",
    "#         rt_match = re.search(r\"ROTATE\\s*TIME\\s*([\\d.]+%)\\s*/\\s*SLIDE\\s*TIME\\s*([\\d.]+%)\", l, re.IGNORECASE)\n",
    "#         if rt_match:\n",
    "#             ops_data[\"Rotation_Time\"][\"Rotate Time\"] = rt_match.group(1)\n",
    "#             ops_data[\"Rotation_Time\"][\"Slide Time\"] = rt_match.group(2)\n",
    "#         gpm = re.search(r\"GPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "#         if gpm:\n",
    "#             ops_data[\"GPM\"] = gpm.group(1)\n",
    "#         mtr = re.search(r\"MTR\\s*RPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "#         if mtr:\n",
    "#             ops_data[\"MTR RPM\"] = mtr.group(1)\n",
    "#         spp = re.search(r\"SPP:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "#         if spp:\n",
    "#             ops_data[\"SPP\"] = spp.group(1)\n",
    "#         diff = re.search(r\"DIFF:\\s*([\\d\\-]+)\", l, re.IGNORECASE)\n",
    "#         if diff:\n",
    "#             ops_data[\"DIFF\"] = diff.group(1)\n",
    "#         wob = re.search(r\"WOB:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "#         if wob:\n",
    "#             ops_data[\"WOB\"] = wob.group(1)\n",
    "#         rot_rpm = re.search(r\"ROT\\s*RPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "#         if rot_rpm:\n",
    "#             ops_data[\"ROT RPM\"] = rot_rpm.group(1)\n",
    "#         on_btm = re.search(r\"ON\\s*BTM\\s*TRQ[:;]?\\s*([\\d\\-K]+)\", l, re.IGNORECASE)\n",
    "#         if on_btm:\n",
    "#             ops_data[\"ON BTM TRQ\"] = on_btm.group(1)\n",
    "#         off_btm = re.search(r\"OFF\\s*BTM\\s*TRQ[:;]?\\s*([\\d\\-K]+)\", l, re.IGNORECASE)\n",
    "#         if off_btm:\n",
    "#             ops_data[\"OFF BTM TRQ\"] = off_btm.group(1)\n",
    "#         gas_units = re.search(r\"GAS:\\s*([\\d,]+)\\s*UNITS\", l, re.IGNORECASE)\n",
    "#         if gas_units:\n",
    "#             ops_data[\"GAS\"][\"Units\"] = gas_units.group(1)\n",
    "#         flare = re.search(r\"(NO\\s*FLARE|FLARE\\s*ON|FLARE\\s*\\S+)\", l, re.IGNORECASE)\n",
    "#         if flare:\n",
    "#             ops_data[\"GAS\"][\"Flare\"] = flare.group(1)\n",
    "#         mw_in = re.search(r\"MW\\s*IN\\s*([\\d.]+)\", l, re.IGNORECASE)\n",
    "#         if mw_in:\n",
    "#             ops_data[\"MW\"][\"In\"] = mw_in.group(1)\n",
    "#         mw_out = re.search(r\"OUT\\s*([\\d.]+)\", l, re.IGNORECASE)\n",
    "#         if mw_out:\n",
    "#             ops_data[\"MW\"][\"Out\"] = mw_out.group(1)\n",
    "#         if re.search(r\"TARGET\\s*#\\d+\", l, re.IGNORECASE):\n",
    "#             ops_data[\"Targets\"].append(l)\n",
    "#         if not re.search(r\"(F/|ROTATE|GPM:|MTR RPM:|SPP:)\", l, re.IGNORECASE):\n",
    "#             ops_data[\"Observations\"].append(l)\n",
    "#     ops_data[\"Observations\"] = list(dict.fromkeys(ops_data[\"Observations\"]))\n",
    "#     return ops_data\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # parse_time_breakdown_by_lines\n",
    "# # ---------------------------------------------------------------------\n",
    "# def parse_time_breakdown_by_lines(ocr_text):\n",
    "#     \"\"\"\n",
    "#     Splits the OCR text into lines, groups them into row groups when a line starts with a time stamp,\n",
    "#     and then parses each group into a row dictionary.\n",
    "#     The header line is expected to contain tokens in the order:\n",
    "#       From, To, Depth Start, Depth End, then Phase and Activity.\n",
    "#     Hours is computed from the time difference.\n",
    "#     \"\"\"\n",
    "#     lines = ocr_text.splitlines()\n",
    "#     logger.debug(f\"Total OCR lines: {len(lines)}\")\n",
    "    \n",
    "#     # Remove header lines containing fixed keywords.\n",
    "#     header_keywords = [\"Time Period\", \"From To Hours\", \"Depth Phase\", \"Operations Description\"]\n",
    "#     filtered_lines = [line.strip() for line in lines if line.strip() and not any(kw in line for kw in header_keywords)]\n",
    "#     logger.debug(f\"Filtered lines count: {len(filtered_lines)}\")\n",
    "    \n",
    "#     # Group lines: a new group starts when a line starts with a time stamp (HH:MM).\n",
    "#     groups = []\n",
    "#     current_group = []\n",
    "#     time_pattern = re.compile(r\"^\\d{2}:\\d{2}\")\n",
    "#     for line in filtered_lines:\n",
    "#         if time_pattern.match(line):\n",
    "#             if current_group:\n",
    "#                 groups.append(current_group)\n",
    "#                 logger.debug(f\"Created group with {len(current_group)} lines.\")\n",
    "#             current_group = [line]\n",
    "#         else:\n",
    "#             if current_group:\n",
    "#                 current_group.append(line)\n",
    "#     if current_group:\n",
    "#         groups.append(current_group)\n",
    "#         logger.debug(f\"Final group added with {len(current_group)} lines.\")\n",
    "#     logger.info(f\"Total row groups detected: {len(groups)}\")\n",
    "    \n",
    "#     parsed_rows = []\n",
    "#     for idx, group in enumerate(groups):\n",
    "#         header_line = group[0]\n",
    "#         tokens = re.split(r\"\\s+\", header_line)\n",
    "#         logger.debug(f\"Row {idx} header tokens: {tokens}\")\n",
    "#         if len(tokens) < 5:\n",
    "#             logger.warning(f\"Row {idx} skipped due to insufficient tokens: {tokens}\")\n",
    "#             continue\n",
    "        \n",
    "#         from_time = tokens[0]\n",
    "#         to_time = tokens[1]\n",
    "#         # Compute Hours from time difference.\n",
    "#         hours = compute_hours(from_time, to_time)\n",
    "#         depth_start = tokens[2]\n",
    "#         depth_end = tokens[3]\n",
    "        \n",
    "#         # Search tokens[4:] for the activity token (e.g. starting with \"DR-\")\n",
    "#         activity = None\n",
    "#         activity_idx = None\n",
    "#         for i, token in enumerate(tokens[4:], start=4):\n",
    "#             if re.match(r\"DR-\\S+\", token, re.IGNORECASE):\n",
    "#                 activity = token\n",
    "#                 activity_idx = i\n",
    "#                 break\n",
    "#         if activity is None:\n",
    "#             logger.warning(f\"Row {idx} could not find an activity token in {tokens[4:]}\")\n",
    "#             activity = \"[UNKNOWN]\"\n",
    "#             phase = \" \".join(tokens[4:])\n",
    "#             ops_tokens = group[1:] if len(group) > 1 else []\n",
    "#         else:\n",
    "#             phase = \" \".join(tokens[4:activity_idx])\n",
    "#             ops_tokens = tokens[activity_idx+1:]\n",
    "#             # Also include any additional lines from the group as part of operations.\n",
    "#             if len(group) > 1:\n",
    "#                 ops_tokens.extend(\" \".join(group[1:]).split())\n",
    "#         ops_text = \" \".join(ops_tokens)\n",
    "#         logger.debug(f\"Row {idx} parsed header: From={from_time}, To={to_time}, Hours={hours}, Depth Start={depth_start}, Depth End={depth_end}, Phase='{phase}', Activity='{activity}'\")\n",
    "#         logger.debug(f\"Row {idx} operations text: {ops_text[:100]}...\")\n",
    "#         ops_description = parse_operations_description(ops_text)\n",
    "        \n",
    "#         row_dict = {\n",
    "#             \"From\": from_time,\n",
    "#             \"To\": to_time,\n",
    "#             \"Hours\": hours,\n",
    "#             \"Depth Start\": depth_start,\n",
    "#             \"Depth End\": depth_end,\n",
    "#             \"Phase\": phase.strip(),\n",
    "#             \"Activity\": activity.strip(),\n",
    "#             \"Operations Description\": ops_description\n",
    "#         }\n",
    "#         parsed_rows.append(row_dict)\n",
    "    \n",
    "#     logger.info(f\"Total parsed rows: {len(parsed_rows)}\")\n",
    "#     return parsed_rows\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # main_time_breakdown_pipeline\n",
    "# # ---------------------------------------------------------------------\n",
    "# def main_time_breakdown_pipeline():\n",
    "#     # Set your image path (adjust as needed)\n",
    "#     img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_14.png\"\n",
    "#     try:\n",
    "#         img = safe_read_image(img_path)\n",
    "#     except Exception as e:\n",
    "#         logger.error(e)\n",
    "#         return\n",
    "\n",
    "#     # Preprocess the image.\n",
    "#     thresh_img = preprocess_image(img, debug=True)\n",
    "\n",
    "#     # Perform OCR on the entire image.\n",
    "#     full_text = single_box_ocr(thresh_img, debug=True)\n",
    "\n",
    "#     # Parse the OCR text into rows.\n",
    "#     time_breakdown_list = parse_time_breakdown_by_lines(full_text)\n",
    "#     if not time_breakdown_list:\n",
    "#         logger.error(\"No rows were detected. Please check the OCR text and header format.\")\n",
    "#         return\n",
    "\n",
    "#     # Build final JSON output.\n",
    "#     final_output = {\"TIME BREAKDOWN\": time_breakdown_list}\n",
    "#     logger.info(\"===== FINAL TIME BREAKDOWN DATA =====\")\n",
    "#     logger.info(json.dumps(final_output, indent=4))\n",
    "\n",
    "#     # Create a DataFrame (nested fields flattened).\n",
    "#     df = pd.json_normalize(final_output[\"TIME BREAKDOWN\"])\n",
    "#     print(\"----- Extracted Time Breakdown DataFrame -----\")\n",
    "#     print(df)\n",
    "\n",
    "#     # Save results as JSON and CSV.\n",
    "#     output_folder = \"dbfs:/mnt/mini-proj-dd/final_time_breakdown_results\"\n",
    "#     local_folder = output_folder.replace(\"dbfs:\", \"/dbfs\")\n",
    "#     os.makedirs(local_folder, exist_ok=True)\n",
    "#     out_json = os.path.join(local_folder, \"time_breakdown_data.json\")\n",
    "#     with open(out_json, \"w\") as f:\n",
    "#         json.dump(final_output, f, indent=4)\n",
    "#     logger.info(f\"JSON saved to {out_json}\")\n",
    "#     out_csv = os.path.join(local_folder, \"time_breakdown_data.csv\")\n",
    "#     df.to_csv(out_csv, index=False)\n",
    "#     logger.info(f\"CSV saved to {out_csv}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main_time_breakdown_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cb34f26-c121-4fae-9bf8-bb17788d6f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logger Setup\n",
    "# ---------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"timeBreakdownExtractor\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: show_image\n",
    "# ---------------------------------------------------------------------\n",
    "def show_image(title, img, cmap=None, size=(10,10)):\n",
    "    plt.figure(figsize=size)\n",
    "    if cmap:\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    else:\n",
    "        if len(img.shape) == 3:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# safe_read_image\n",
    "# ---------------------------------------------------------------------\n",
    "def safe_read_image(img_path):\n",
    "    \"\"\"Reads an image from a DBFS or local path.\"\"\"\n",
    "    if img_path.startswith(\"dbfs:\"):\n",
    "        local_path = img_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "    else:\n",
    "        local_path = img_path\n",
    "    logger.info(f\"Trying to read image from: {local_path}\")\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"OpenCV failed to read the image: {local_path}\")\n",
    "    return img\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# preprocess_image\n",
    "# ---------------------------------------------------------------------\n",
    "def preprocess_image(img, debug=False):\n",
    "    \"\"\"Converts image to grayscale and applies adaptive thresholding.\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if debug:\n",
    "        show_image(\"1) Grayscale\", gray, cmap=\"gray\")\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 15, 9\n",
    "    )\n",
    "    if debug:\n",
    "        show_image(\"2) Thresholded\", thresh, cmap=\"gray\")\n",
    "    return thresh\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# single_box_ocr\n",
    "# ---------------------------------------------------------------------\n",
    "def single_box_ocr(img, debug=False):\n",
    "    \"\"\"Performs OCR on the entire image region.\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    roi = img[0:h, 0:w]\n",
    "    text = pytesseract.image_to_string(roi, config=\"--psm 6\")\n",
    "    if debug:\n",
    "        logger.info(\"OCR text:\\n\" + text)\n",
    "    return text\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# parse_time_breakdown_by_lines\n",
    "# ---------------------------------------------------------------------\n",
    "def parse_time_breakdown_by_lines(ocr_text):\n",
    "    \"\"\"\n",
    "    Splits the OCR text line-by-line and groups lines into rows.\n",
    "    A new row is assumed to begin when a line starts with a time stamp (e.g. \"06:00\").\n",
    "    Lines that are clearly headers (like \"From To Hours\") are skipped.\n",
    "    Returns a list of dictionaries, one per row.\n",
    "    \"\"\"\n",
    "    lines = ocr_text.splitlines()\n",
    "\n",
    "    # Filter out obvious header lines.\n",
    "    header_keywords = [\"Time Period\", \"From To Hours\", \"Depth Phase\", \"Operations Description\"]\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        ls = line.strip()\n",
    "        if ls and not any(kw in ls for kw in header_keywords):\n",
    "            filtered_lines.append(ls)\n",
    "    \n",
    "    rows = []\n",
    "    current_group = []\n",
    "    time_pattern = re.compile(r\"^\\d{2}:\\d{2}\")  # line starting with HH:MM\n",
    "\n",
    "    for line in filtered_lines:\n",
    "        if time_pattern.match(line):\n",
    "            # If we already have a group, save it.\n",
    "            if current_group:\n",
    "                rows.append(\"\\n\".join(current_group))\n",
    "            current_group = [line]\n",
    "        else:\n",
    "            # Append to current group.\n",
    "            if current_group:\n",
    "                current_group.append(line)\n",
    "    if current_group:\n",
    "        rows.append(\"\\n\".join(current_group))\n",
    "    \n",
    "    parsed_rows = []\n",
    "    # Now process each group (row).\n",
    "    for group in rows:\n",
    "        # Split group into lines and tokens.\n",
    "        group_lines = group.splitlines()\n",
    "        if not group_lines:\n",
    "            continue\n",
    "        header_line = group_lines[0]\n",
    "        # Split header_line into tokens (whitespace split)\n",
    "        tokens = re.split(r\"\\s+\", header_line.strip())\n",
    "        # We assume header tokens are in order: From, To, Hours, Depth Start, Depth End, then remainder for Phase and Activity.\n",
    "        # Adjust expected count if needed.\n",
    "        if len(tokens) < 6:\n",
    "            continue  # Not enough data, skip.\n",
    "        from_time = tokens[0]\n",
    "        to_time = tokens[1]\n",
    "        hours = tokens[2]\n",
    "        depth_start = tokens[3]\n",
    "        depth_end = tokens[4]\n",
    "        # The rest of tokens in header: try to separate Phase and Activity.\n",
    "        remainder = tokens[5:]\n",
    "        phase = \" \".join(remainder[:-1]) if len(remainder) > 1 else remainder[0]\n",
    "        activity = remainder[-1] if len(remainder) > 1 else \"[UNKNOWN]\"\n",
    "        # The rest of the group (if any) becomes the raw operations text.\n",
    "        ops_text = \"\\n\".join(group_lines[1:]) if len(group_lines) > 1 else \"\"\n",
    "        ops_description = parse_operations_description(ops_text)\n",
    "        parsed_row = {\n",
    "            \"From\": from_time,\n",
    "            \"To\": to_time,\n",
    "            \"Hours\": hours,\n",
    "            \"Depth Start\": depth_start,\n",
    "            \"Depth End\": depth_end,\n",
    "            \"Phase\": phase,\n",
    "            \"Activity\": activity,\n",
    "            \"Operations Description\": ops_description\n",
    "        }\n",
    "        parsed_rows.append(parsed_row)\n",
    "    \n",
    "    return parsed_rows\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# parse_operations_description\n",
    "# ---------------------------------------------------------------------\n",
    "def parse_operations_description(ops_text):\n",
    "    \"\"\"\n",
    "    Parses the operations description text into a structured dictionary.\n",
    "    This template uses regex to extract key metrics. Adjust as needed.\n",
    "    \"\"\"\n",
    "    ops_data = {\n",
    "        \"Depth\": {\"From\": \"\", \"To\": \"\"},\n",
    "        \"Performance\": {\"Feet\": \"\", \"FPH\": \"\"},\n",
    "        \"Rotation_Slide\": {\"Rotate\": \"\", \"Slide\": \"\"},\n",
    "        \"Rotation_Time\": {\"Rotate Time\": \"\", \"Slide Time\": \"\"},\n",
    "        \"GPM\": \"\",\n",
    "        \"MTR RPM\": \"\",\n",
    "        \"SPP\": \"\",\n",
    "        \"DIFF\": \"\",\n",
    "        \"WOB\": \"\",\n",
    "        \"ROT RPM\": \"\",\n",
    "        \"ON BTM TRQ\": \"\",\n",
    "        \"OFF BTM TRQ\": \"\",\n",
    "        \"GAS\": {\"Units\": \"\", \"Flare\": \"\"},\n",
    "        \"MW\": {\"In\": \"\", \"Out\": \"\"},\n",
    "        \"Targets\": [],\n",
    "        \"Observations\": []\n",
    "    }\n",
    "\n",
    "    lines = ops_text.splitlines()\n",
    "    for line in lines:\n",
    "        l = line.strip()\n",
    "        if not l:\n",
    "            continue\n",
    "\n",
    "        # Depth extraction: e.g. \"F/ 17,469' T/ 18,163'\"\n",
    "        depth_match = re.search(r\"F/\\s*([\\d,']+)\\s*T/\\s*([\\d,']+)\", l, re.IGNORECASE)\n",
    "        if depth_match:\n",
    "            ops_data[\"Depth\"][\"From\"] = depth_match.group(1)\n",
    "            ops_data[\"Depth\"][\"To\"] = depth_match.group(2)\n",
    "\n",
    "        # Performance: e.g. \"(694' @ 115 FPH)\"\n",
    "        perf_match = re.search(r\"\\(([\\d,']+)\\s*@\\s*(\\d+)\\s*FPH\\)\", l, re.IGNORECASE)\n",
    "        if perf_match:\n",
    "            ops_data[\"Performance\"][\"Feet\"] = perf_match.group(1)\n",
    "            ops_data[\"Performance\"][\"FPH\"] = perf_match.group(2)\n",
    "\n",
    "        # Rotation/Slide percentages.\n",
    "        rs_match = re.search(r\"ROTATE\\s*([\\d.]+%)\\s*/\\s*SLIDE\\s*([\\d.]+%)\", l, re.IGNORECASE)\n",
    "        if rs_match:\n",
    "            ops_data[\"Rotation_Slide\"][\"Rotate\"] = rs_match.group(1)\n",
    "            ops_data[\"Rotation_Slide\"][\"Slide\"] = rs_match.group(2)\n",
    "\n",
    "        # Rotation Time percentages.\n",
    "        rt_match = re.search(r\"ROTATE\\s*TIME\\s*([\\d.]+%)\\s*/\\s*SLIDE\\s*TIME\\s*([\\d.]+%)\", l, re.IGNORECASE)\n",
    "        if rt_match:\n",
    "            ops_data[\"Rotation_Time\"][\"Rotate Time\"] = rt_match.group(1)\n",
    "            ops_data[\"Rotation_Time\"][\"Slide Time\"] = rt_match.group(2)\n",
    "\n",
    "        # Numeric parameters.\n",
    "        gpm = re.search(r\"GPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "        if gpm:\n",
    "            ops_data[\"GPM\"] = gpm.group(1)\n",
    "        mtr = re.search(r\"MTR\\s*RPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "        if mtr:\n",
    "            ops_data[\"MTR RPM\"] = mtr.group(1)\n",
    "        spp = re.search(r\"SPP:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "        if spp:\n",
    "            ops_data[\"SPP\"] = spp.group(1)\n",
    "        diff = re.search(r\"DIFF:\\s*([\\d\\-]+)\", l, re.IGNORECASE)\n",
    "        if diff:\n",
    "            ops_data[\"DIFF\"] = diff.group(1)\n",
    "        wob = re.search(r\"WOB:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "        if wob:\n",
    "            ops_data[\"WOB\"] = wob.group(1)\n",
    "        rot_rpm = re.search(r\"ROT\\s*RPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "        if rot_rpm:\n",
    "            ops_data[\"ROT RPM\"] = rot_rpm.group(1)\n",
    "        on_btm = re.search(r\"ON\\s*BTM\\s*TRQ[:;]?\\s*([\\d\\-K]+)\", l, re.IGNORECASE)\n",
    "        if on_btm:\n",
    "            ops_data[\"ON BTM TRQ\"] = on_btm.group(1)\n",
    "        off_btm = re.search(r\"OFF\\s*BTM\\s*TRQ[:;]?\\s*([\\d\\-K]+)\", l, re.IGNORECASE)\n",
    "        if off_btm:\n",
    "            ops_data[\"OFF BTM TRQ\"] = off_btm.group(1)\n",
    "        gas_units = re.search(r\"GAS:\\s*([\\d,]+)\\s*UNITS\", l, re.IGNORECASE)\n",
    "        if gas_units:\n",
    "            ops_data[\"GAS\"][\"Units\"] = gas_units.group(1)\n",
    "        flare = re.search(r\"(NO\\s*FLARE|FLARE\\s*ON|FLARE\\s*\\S+)\", l, re.IGNORECASE)\n",
    "        if flare:\n",
    "            ops_data[\"GAS\"][\"Flare\"] = flare.group(1)\n",
    "        mw_in = re.search(r\"MW\\s*IN\\s*([\\d.]+)\", l, re.IGNORECASE)\n",
    "        if mw_in:\n",
    "            ops_data[\"MW\"][\"In\"] = mw_in.group(1)\n",
    "        mw_out = re.search(r\"OUT\\s*([\\d.]+)\", l, re.IGNORECASE)\n",
    "        if mw_out:\n",
    "            ops_data[\"MW\"][\"Out\"] = mw_out.group(1)\n",
    "\n",
    "        # Capture Targets.\n",
    "        if re.search(r\"TARGET\\s*#\\d+\", l, re.IGNORECASE):\n",
    "            ops_data[\"Targets\"].append(l)\n",
    "        # Other lines become Observations if they don't match key patterns.\n",
    "        if not re.search(r\"(F/|ROTATE|GPM:|MTR RPM:|SPP:)\", l, re.IGNORECASE):\n",
    "            ops_data[\"Observations\"].append(l)\n",
    "\n",
    "    # Remove duplicate observations.\n",
    "    ops_data[\"Observations\"] = list(dict.fromkeys(ops_data[\"Observations\"]))\n",
    "    return ops_data\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# main_time_breakdown_pipeline\n",
    "# ---------------------------------------------------------------------\n",
    "def main_time_breakdown_pipeline():\n",
    "    # Set your image path (adjust as needed)\n",
    "    img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_14.png\"\n",
    "    try:\n",
    "        img = safe_read_image(img_path)\n",
    "        logger.info(\"Image loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        return\n",
    "\n",
    "    # Preprocess the image.\n",
    "    thresh_img = preprocess_image(img, debug=True)\n",
    "\n",
    "    # Perform OCR on the entire region.\n",
    "    full_text = single_box_ocr(thresh_img, debug=True)\n",
    "\n",
    "    # Dynamically split the OCR text into multiple rows using line-by-line grouping.\n",
    "    time_breakdown_list = parse_time_breakdown_by_lines(full_text)\n",
    "    if not time_breakdown_list:\n",
    "        logger.error(\"No rows were detected. Please check the OCR output and header format.\")\n",
    "        return\n",
    "\n",
    "    # Build final JSON output.\n",
    "    final_output = {\"TIME BREAKDOWN\": time_breakdown_list}\n",
    "    logger.info(\"===== FINAL TIME BREAKDOWN DATA =====\")\n",
    "    logger.info(json.dumps(final_output, indent=4))\n",
    "\n",
    "    # Create a DataFrame (nested fields flattened).\n",
    "    df = pd.json_normalize(final_output[\"TIME BREAKDOWN\"])\n",
    "    print(\"----- Extracted Time Breakdown DataFrame -----\")\n",
    "    print(df)\n",
    "\n",
    "    # Save results as JSON and CSV.\n",
    "    output_folder = \"dbfs:/mnt/mini-proj-dd/final_time_breakdown_results\"\n",
    "    local_folder = output_folder.replace(\"dbfs:\", \"/dbfs\")\n",
    "    os.makedirs(local_folder, exist_ok=True)\n",
    "    out_json = os.path.join(local_folder, \"time_breakdown_data.json\")\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(final_output, f, indent=4)\n",
    "    logger.info(f\"JSON saved to {out_json}\")\n",
    "    out_csv = os.path.join(local_folder, \"time_breakdown_data.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    logger.info(f\"CSV saved to {out_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_time_breakdown_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "136b08b3-0e59-4b12-b97b-f92f821db928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import cv2\n",
    "# import pytesseract\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import logging\n",
    "# import json\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # Logger Setup\n",
    "# # ---------------------------------------------------------------------\n",
    "# logger = logging.getLogger(\"timeBreakdownExtractor\")\n",
    "# logger.setLevel(logging.DEBUG)  # Set to DEBUG to see detailed logs.\n",
    "# if not logger.handlers:\n",
    "#     handler = logging.StreamHandler()\n",
    "#     handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "#     logger.addHandler(handler)\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # Utility: show_image\n",
    "# # ---------------------------------------------------------------------\n",
    "# def show_image(title, img, cmap=None, size=(10,10)):\n",
    "#     plt.figure(figsize=size)\n",
    "#     if cmap:\n",
    "#         plt.imshow(img, cmap=cmap)\n",
    "#     else:\n",
    "#         if len(img.shape) == 3:\n",
    "#             plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "#         else:\n",
    "#             plt.imshow(img, cmap=\"gray\")\n",
    "#     plt.title(title)\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # safe_read_image\n",
    "# # ---------------------------------------------------------------------\n",
    "# def safe_read_image(img_path):\n",
    "#     \"\"\"Reads an image from a DBFS or local path.\"\"\"\n",
    "#     if img_path.startswith(\"dbfs:\"):\n",
    "#         local_path = img_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "#     else:\n",
    "#         local_path = img_path\n",
    "#     logger.info(f\"Trying to read image from: {local_path}\")\n",
    "#     if not os.path.exists(local_path):\n",
    "#         logger.error(f\"File not found: {local_path}\")\n",
    "#         raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "#     img = cv2.imread(local_path)\n",
    "#     if img is None:\n",
    "#         logger.error(f\"OpenCV failed to read the image: {local_path}\")\n",
    "#         raise FileNotFoundError(f\"OpenCV failed to read the image: {local_path}\")\n",
    "#     logger.info(\"Image loaded successfully.\")\n",
    "#     return img\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # preprocess_image\n",
    "# # ---------------------------------------------------------------------\n",
    "# def preprocess_image(img, debug=False):\n",
    "#     \"\"\"Converts image to grayscale and applies adaptive thresholding.\"\"\"\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     if debug:\n",
    "#         show_image(\"1) Grayscale\", gray, cmap=\"gray\")\n",
    "#     thresh = cv2.adaptiveThreshold(\n",
    "#         gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "#         cv2.THRESH_BINARY, 15, 9\n",
    "#     )\n",
    "#     if debug:\n",
    "#         show_image(\"2) Thresholded\", thresh, cmap=\"gray\")\n",
    "#     logger.debug(\"Preprocessing complete.\")\n",
    "#     return thresh\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # single_box_ocr\n",
    "# # ---------------------------------------------------------------------\n",
    "# def single_box_ocr(img, debug=False):\n",
    "#     \"\"\"Performs OCR on the entire image region.\"\"\"\n",
    "#     h, w = img.shape[:2]\n",
    "#     roi = img[0:h, 0:w]\n",
    "#     text = pytesseract.image_to_string(roi, config=\"--psm 6\")\n",
    "#     logger.debug(f\"OCR produced {len(text)} characters.\")\n",
    "#     if debug:\n",
    "#         # Log the first few lines of OCR output\n",
    "#         logger.debug(\"OCR text preview:\\n\" + \"\\n\".join(text.splitlines()[:5]))\n",
    "#     return text\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # parse_operations_description\n",
    "# # ---------------------------------------------------------------------\n",
    "# def parse_operations_description(ops_text):\n",
    "#     \"\"\"\n",
    "#     Parses the operations description text into a structured dictionary.\n",
    "#     Adjust regexes as needed.\n",
    "#     \"\"\"\n",
    "#     ops_data = {\n",
    "#         \"Depth\": {\"From\": \"\", \"To\": \"\"},\n",
    "#         \"Performance\": {\"Feet\": \"\", \"FPH\": \"\"},\n",
    "#         \"Rotation_Slide\": {\"Rotate\": \"\", \"Slide\": \"\"},\n",
    "#         \"Rotation_Time\": {\"Rotate Time\": \"\", \"Slide Time\": \"\"},\n",
    "#         \"GPM\": \"\",\n",
    "#         \"MTR RPM\": \"\",\n",
    "#         \"SPP\": \"\",\n",
    "#         \"DIFF\": \"\",\n",
    "#         \"WOB\": \"\",\n",
    "#         \"ROT RPM\": \"\",\n",
    "#         \"ON BTM TRQ\": \"\",\n",
    "#         \"OFF BTM TRQ\": \"\",\n",
    "#         \"GAS\": {\"Units\": \"\", \"Flare\": \"\"},\n",
    "#         \"MW\": {\"In\": \"\", \"Out\": \"\"},\n",
    "#         \"Targets\": [],\n",
    "#         \"Observations\": []\n",
    "#     }\n",
    "    \n",
    "#     lines = ops_text.splitlines()\n",
    "#     logger.debug(f\"Parsing operations description with {len(lines)} lines.\")\n",
    "#     for line in lines:\n",
    "#         l = line.strip()\n",
    "#         if not l:\n",
    "#             continue\n",
    "\n",
    "#         # Depth extraction: e.g. \"F/ 17,469' T/ 18,163'\"\n",
    "#         depth_match = re.search(r\"F/\\s*([\\d,']+)\\s*T/\\s*([\\d,']+)\", l, re.IGNORECASE)\n",
    "#         if depth_match:\n",
    "#             ops_data[\"Depth\"][\"From\"] = depth_match.group(1)\n",
    "#             ops_data[\"Depth\"][\"To\"] = depth_match.group(2)\n",
    "#         # Performance extraction: e.g. \"(694' @ 115 FPH)\"\n",
    "#         perf_match = re.search(r\"\\(([\\d,']+)\\s*@\\s*(\\d+)\\s*FPH\\)\", l, re.IGNORECASE)\n",
    "#         if perf_match:\n",
    "#             ops_data[\"Performance\"][\"Feet\"] = perf_match.group(1)\n",
    "#             ops_data[\"Performance\"][\"FPH\"] = perf_match.group(2)\n",
    "#         # Rotation/Slide percentages.\n",
    "#         rs_match = re.search(r\"ROTATE\\s*([\\d.]+%)\\s*/\\s*SLIDE\\s*([\\d.]+%)\", l, re.IGNORECASE)\n",
    "#         if rs_match:\n",
    "#             ops_data[\"Rotation_Slide\"][\"Rotate\"] = rs_match.group(1)\n",
    "#             ops_data[\"Rotation_Slide\"][\"Slide\"] = rs_match.group(2)\n",
    "#         # Rotation Time percentages.\n",
    "#         rt_match = re.search(r\"ROTATE\\s*TIME\\s*([\\d.]+%)\\s*/\\s*SLIDE\\s*TIME\\s*([\\d.]+%)\", l, re.IGNORECASE)\n",
    "#         if rt_match:\n",
    "#             ops_data[\"Rotation_Time\"][\"Rotate Time\"] = rt_match.group(1)\n",
    "#             ops_data[\"Rotation_Time\"][\"Slide Time\"] = rt_match.group(2)\n",
    "#         # Numeric parameters.\n",
    "#         gpm = re.search(r\"GPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "#         if gpm:\n",
    "#             ops_data[\"GPM\"] = gpm.group(1)\n",
    "#         mtr = re.search(r\"MTR\\s*RPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "#         if mtr:\n",
    "#             ops_data[\"MTR RPM\"] = mtr.group(1)\n",
    "#         spp = re.search(r\"SPP:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "#         if spp:\n",
    "#             ops_data[\"SPP\"] = spp.group(1)\n",
    "#         diff = re.search(r\"DIFF:\\s*([\\d\\-]+)\", l, re.IGNORECASE)\n",
    "#         if diff:\n",
    "#             ops_data[\"DIFF\"] = diff.group(1)\n",
    "#         wob = re.search(r\"WOB:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "#         if wob:\n",
    "#             ops_data[\"WOB\"] = wob.group(1)\n",
    "#         rot_rpm = re.search(r\"ROT\\s*RPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "#         if rot_rpm:\n",
    "#             ops_data[\"ROT RPM\"] = rot_rpm.group(1)\n",
    "#         on_btm = re.search(r\"ON\\s*BTM\\s*TRQ[:;]?\\s*([\\d\\-K]+)\", l, re.IGNORECASE)\n",
    "#         if on_btm:\n",
    "#             ops_data[\"ON BTM TRQ\"] = on_btm.group(1)\n",
    "#         off_btm = re.search(r\"OFF\\s*BTM\\s*TRQ[:;]?\\s*([\\d\\-K]+)\", l, re.IGNORECASE)\n",
    "#         if off_btm:\n",
    "#             ops_data[\"OFF BTM TRQ\"] = off_btm.group(1)\n",
    "#         gas_units = re.search(r\"GAS:\\s*([\\d,]+)\\s*UNITS\", l, re.IGNORECASE)\n",
    "#         if gas_units:\n",
    "#             ops_data[\"GAS\"][\"Units\"] = gas_units.group(1)\n",
    "#         flare = re.search(r\"(NO\\s*FLARE|FLARE\\s*ON|FLARE\\s*\\S+)\", l, re.IGNORECASE)\n",
    "#         if flare:\n",
    "#             ops_data[\"GAS\"][\"Flare\"] = flare.group(1)\n",
    "#         mw_in = re.search(r\"MW\\s*IN\\s*([\\d.]+)\", l, re.IGNORECASE)\n",
    "#         if mw_in:\n",
    "#             ops_data[\"MW\"][\"In\"] = mw_in.group(1)\n",
    "#         mw_out = re.search(r\"OUT\\s*([\\d.]+)\", l, re.IGNORECASE)\n",
    "#         if mw_out:\n",
    "#             ops_data[\"MW\"][\"Out\"] = mw_out.group(1)\n",
    "#         # Capture Targets.\n",
    "#         if re.search(r\"TARGET\\s*#\\d+\", l, re.IGNORECASE):\n",
    "#             ops_data[\"Targets\"].append(l)\n",
    "#         # Other lines become Observations if not matching key patterns.\n",
    "#         if not re.search(r\"(F/|ROTATE|GPM:|MTR RPM:|SPP:)\", l, re.IGNORECASE):\n",
    "#             ops_data[\"Observations\"].append(l)\n",
    "\n",
    "#     # Remove duplicate observations.\n",
    "#     ops_data[\"Observations\"] = list(dict.fromkeys(ops_data[\"Observations\"]))\n",
    "#     return ops_data\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # parse_time_breakdown_by_lines\n",
    "# # ---------------------------------------------------------------------\n",
    "# def parse_time_breakdown_by_lines(ocr_text):\n",
    "#     \"\"\"\n",
    "#     Splits the OCR text line-by-line and groups lines into rows.\n",
    "#     A new row is assumed when a line starts with a time stamp (e.g., \"06:00\").\n",
    "#     Logs the number of lines, groups, and details of each group.\n",
    "#     Returns a list of dictionaries for each row.\n",
    "#     \"\"\"\n",
    "#     lines = ocr_text.splitlines()\n",
    "#     logger.debug(f\"Total OCR lines: {len(lines)}\")\n",
    "    \n",
    "#     # Filter out header lines.\n",
    "#     header_keywords = [\"Time Period\", \"From To Hours\", \"Depth Phase\", \"Operations Description\"]\n",
    "#     filtered_lines = []\n",
    "#     for line in lines:\n",
    "#         ls = line.strip()\n",
    "#         if ls and not any(kw in ls for kw in header_keywords):\n",
    "#             filtered_lines.append(ls)\n",
    "#     logger.debug(f\"Filtered OCR lines: {len(filtered_lines)}\")\n",
    "    \n",
    "#     # Group lines by detecting new row when a line starts with HH:MM.\n",
    "#     rows_grouped = []\n",
    "#     current_group = []\n",
    "#     time_pattern = re.compile(r\"^\\d{2}:\\d{2}\")\n",
    "    \n",
    "#     for line in filtered_lines:\n",
    "#         if time_pattern.match(line):\n",
    "#             if current_group:\n",
    "#                 rows_grouped.append(current_group)\n",
    "#                 logger.debug(f\"New row group created with {len(current_group)} lines.\")\n",
    "#             current_group = [line]\n",
    "#         else:\n",
    "#             if current_group:\n",
    "#                 current_group.append(line)\n",
    "#     if current_group:\n",
    "#         rows_grouped.append(current_group)\n",
    "#         logger.debug(f\"Final row group added with {len(current_group)} lines.\")\n",
    "#     logger.info(f\"Total row groups detected: {len(rows_grouped)}\")\n",
    "    \n",
    "#     # Process each group.\n",
    "#     parsed_rows = []\n",
    "#     for idx, group in enumerate(rows_grouped):\n",
    "#         header_line = group[0]\n",
    "#         tokens = re.split(r\"\\s+\", header_line.strip())\n",
    "#         if len(tokens) < 6:\n",
    "#             logger.warning(f\"Skipping row group {idx} due to insufficient tokens: {tokens}\")\n",
    "#             continue\n",
    "#         from_time = tokens[0]\n",
    "#         to_time = tokens[1]\n",
    "#         hours = tokens[2]\n",
    "#         depth_start = tokens[3]\n",
    "#         depth_end = tokens[4]\n",
    "#         remainder = tokens[5:]\n",
    "#         phase = \" \".join(remainder[:-1]) if len(remainder) > 1 else remainder[0]\n",
    "#         activity = remainder[-1] if len(remainder) > 1 else \"[UNKNOWN]\"\n",
    "#         ops_text = \"\\n\".join(group[1:]) if len(group) > 1 else \"\"\n",
    "#         ops_description = parse_operations_description(ops_text)\n",
    "#         row_dict = {\n",
    "#             \"From\": from_time,\n",
    "#             \"To\": to_time,\n",
    "#             \"Hours\": hours,\n",
    "#             \"Depth Start\": depth_start,\n",
    "#             \"Depth End\": depth_end,\n",
    "#             \"Phase\": phase,\n",
    "#             \"Activity\": activity,\n",
    "#             \"Operations Description\": ops_description\n",
    "#         }\n",
    "#         logger.debug(f\"Parsed row {idx}: {row_dict}\")\n",
    "#         parsed_rows.append(row_dict)\n",
    "    \n",
    "#     logger.info(f\"Total parsed rows: {len(parsed_rows)}\")\n",
    "#     return parsed_rows\n",
    "\n",
    "# # ---------------------------------------------------------------------\n",
    "# # main_time_breakdown_pipeline\n",
    "# # ---------------------------------------------------------------------\n",
    "# def main_time_breakdown_pipeline():\n",
    "#     # Set your image path.\n",
    "#     img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_14.png\"\n",
    "#     try:\n",
    "#         img = safe_read_image(img_path)\n",
    "#     except Exception as e:\n",
    "#         logger.error(e)\n",
    "#         return\n",
    "\n",
    "#     # Preprocess the image.\n",
    "#     thresh_img = preprocess_image(img, debug=True)\n",
    "\n",
    "#     # Perform OCR on the entire region.\n",
    "#     full_text = single_box_ocr(thresh_img, debug=True)\n",
    "\n",
    "#     # Parse the OCR text into rows.\n",
    "#     time_breakdown_list = parse_time_breakdown_by_lines(full_text)\n",
    "#     if not time_breakdown_list:\n",
    "#         logger.error(\"No rows were detected. Check the OCR text and header format.\")\n",
    "#         return\n",
    "\n",
    "#     # Build final JSON output.\n",
    "#     final_output = {\"TIME BREAKDOWN\": time_breakdown_list}\n",
    "#     logger.info(\"===== FINAL TIME BREAKDOWN DATA =====\")\n",
    "#     logger.info(json.dumps(final_output, indent=4))\n",
    "\n",
    "#     # Create a DataFrame (nested fields will be flattened).\n",
    "#     df = pd.json_normalize(final_output[\"TIME BREAKDOWN\"])\n",
    "#     print(\"----- Extracted Time Breakdown DataFrame -----\")\n",
    "#     print(df)\n",
    "\n",
    "#     # Save results as JSON and CSV.\n",
    "#     output_folder = \"dbfs:/mnt/mini-proj-dd/final_time_breakdown_results\"\n",
    "#     local_folder = output_folder.replace(\"dbfs:\", \"/dbfs\")\n",
    "#     os.makedirs(local_folder, exist_ok=True)\n",
    "#     out_json = os.path.join(local_folder, \"time_breakdown_data.json\")\n",
    "#     with open(out_json, \"w\") as f:\n",
    "#         json.dump(final_output, f, indent=4)\n",
    "#     logger.info(f\"JSON saved to {out_json}\")\n",
    "#     out_csv = os.path.join(local_folder, \"time_breakdown_data.csv\")\n",
    "#     df.to_csv(out_csv, index=False)\n",
    "#     logger.info(f\"CSV saved to {out_csv}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main_time_breakdown_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9773b127-b765-4bc3-b8c9-cee9850da0c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb818aaa-2630-4325-aac5-0fa3a006f8b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Logger Setup\n",
    "# ---------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"timeBreakdownExtractor\")\n",
    "logger.setLevel(logging.INFO)\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(logging.Formatter(\"%(levelname)s: %(message)s\"))\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: show_image\n",
    "# ---------------------------------------------------------------------\n",
    "def show_image(title, img, cmap=None, size=(10,10)):\n",
    "    plt.figure(figsize=size)\n",
    "    if cmap:\n",
    "        plt.imshow(img, cmap=cmap)\n",
    "    else:\n",
    "        if len(img.shape) == 3:\n",
    "            plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        else:\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# safe_read_image\n",
    "# ---------------------------------------------------------------------\n",
    "def safe_read_image(img_path):\n",
    "    \"\"\"\n",
    "    Reads an image from a DBFS or local path.\n",
    "    \"\"\"\n",
    "    if img_path.startswith(\"dbfs:\"):\n",
    "        local_path = img_path.replace(\"dbfs:\", \"/dbfs\")\n",
    "    else:\n",
    "        local_path = img_path\n",
    "    logger.info(f\"Trying to read image from: {local_path}\")\n",
    "    if not os.path.exists(local_path):\n",
    "        raise FileNotFoundError(f\"File not found: {local_path}\")\n",
    "    img = cv2.imread(local_path)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"OpenCV failed to read the image: {local_path}\")\n",
    "    return img\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# preprocess_image\n",
    "# ---------------------------------------------------------------------\n",
    "def preprocess_image(img, debug=False):\n",
    "    \"\"\"\n",
    "    Converts image to grayscale and applies adaptive thresholding.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    if debug:\n",
    "        show_image(\"1) Grayscale\", gray, cmap=\"gray\")\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY, 15, 9\n",
    "    )\n",
    "    if debug:\n",
    "        show_image(\"2) Thresholded\", thresh, cmap=\"gray\")\n",
    "    return thresh\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# single_box_ocr\n",
    "# ---------------------------------------------------------------------\n",
    "def single_box_ocr(img, debug=False):\n",
    "    \"\"\"\n",
    "    Performs OCR on the entire image (or table region) as one chunk.\n",
    "    \"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    roi = img[0:h, 0:w]\n",
    "    text = pytesseract.image_to_string(roi, config=\"--psm 6\")\n",
    "    if debug:\n",
    "        logger.info(\"OCR text:\\n\" + text)\n",
    "    return text\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# parse_operations_description\n",
    "# ---------------------------------------------------------------------\n",
    "def parse_operations_description(ops_text):\n",
    "    \"\"\"\n",
    "    Parses the operations description text into structured subfields.\n",
    "    This is a templateadjust regexes as needed.\n",
    "    \"\"\"\n",
    "    ops_data = {\n",
    "        \"Depth\": {\"From\": \"\", \"To\": \"\"},\n",
    "        \"Performance\": {\"Feet\": \"\", \"FPH\": \"\"},\n",
    "        \"Rotation_Slide\": {\"Rotate\": \"\", \"Slide\": \"\"},\n",
    "        \"Rotation_Time\": {\"Rotate Time\": \"\", \"Slide Time\": \"\"},\n",
    "        \"GPM\": \"\",\n",
    "        \"MTR RPM\": \"\",\n",
    "        \"SPP\": \"\",\n",
    "        \"DIFF\": \"\",\n",
    "        \"WOB\": \"\",\n",
    "        \"ROT RPM\": \"\",\n",
    "        \"ON BTM TRQ\": \"\",\n",
    "        \"OFF BTM TRQ\": \"\",\n",
    "        \"GAS\": {\"Units\": \"\", \"Flare\": \"\"},\n",
    "        \"MW\": {\"In\": \"\", \"Out\": \"\"},\n",
    "        \"Targets\": [],\n",
    "        \"Observations\": []\n",
    "    }\n",
    "\n",
    "    # Split operations text by lines (or spaces) and attempt regex extraction.\n",
    "    lines = ops_text.splitlines()\n",
    "    for line in lines:\n",
    "        l = line.strip()\n",
    "        if not l:\n",
    "            continue\n",
    "\n",
    "        # Depth, e.g. \"F/ 17,469' T/ 18,163'\"\n",
    "        depth_match = re.search(r\"F/\\s*([\\d,']+)\\s*T/\\s*([\\d,']+)\", l, re.IGNORECASE)\n",
    "        if depth_match:\n",
    "            ops_data[\"Depth\"][\"From\"] = depth_match.group(1)\n",
    "            ops_data[\"Depth\"][\"To\"] = depth_match.group(2)\n",
    "\n",
    "        # Performance, e.g. \"(694' @ 115 FPH)\"\n",
    "        perf_match = re.search(r\"\\(([\\d,']+)\\s*@\\s*(\\d+)\\s*FPH\\)\", l, re.IGNORECASE)\n",
    "        if perf_match:\n",
    "            ops_data[\"Performance\"][\"Feet\"] = perf_match.group(1)\n",
    "            ops_data[\"Performance\"][\"FPH\"] = perf_match.group(2)\n",
    "\n",
    "        # Rotation/Slide percentages.\n",
    "        rs_match = re.search(r\"ROTATE\\s*([\\d.]+%)\\s*/\\s*SLIDE\\s*([\\d.]+%)\", l, re.IGNORECASE)\n",
    "        if rs_match:\n",
    "            ops_data[\"Rotation_Slide\"][\"Rotate\"] = rs_match.group(1)\n",
    "            ops_data[\"Rotation_Slide\"][\"Slide\"] = rs_match.group(2)\n",
    "\n",
    "        # Rotation Time percentages.\n",
    "        rt_match = re.search(r\"ROTATE\\s*TIME\\s*([\\d.]+%)\\s*/\\s*SLIDE\\s*TIME\\s*([\\d.]+%)\", l, re.IGNORECASE)\n",
    "        if rt_match:\n",
    "            ops_data[\"Rotation_Time\"][\"Rotate Time\"] = rt_match.group(1)\n",
    "            ops_data[\"Rotation_Time\"][\"Slide Time\"] = rt_match.group(2)\n",
    "\n",
    "        # Numeric parameters.\n",
    "        gpm = re.search(r\"GPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "        if gpm:\n",
    "            ops_data[\"GPM\"] = gpm.group(1)\n",
    "        mtr = re.search(r\"MTR\\s*RPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "        if mtr:\n",
    "            ops_data[\"MTR RPM\"] = mtr.group(1)\n",
    "        spp = re.search(r\"SPP:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "        if spp:\n",
    "            ops_data[\"SPP\"] = spp.group(1)\n",
    "        diff = re.search(r\"DIFF:\\s*([\\d\\-]+)\", l, re.IGNORECASE)\n",
    "        if diff:\n",
    "            ops_data[\"DIFF\"] = diff.group(1)\n",
    "        wob = re.search(r\"WOB:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "        if wob:\n",
    "            ops_data[\"WOB\"] = wob.group(1)\n",
    "        rot_rpm = re.search(r\"ROT\\s*RPM:\\s*(\\d+)\", l, re.IGNORECASE)\n",
    "        if rot_rpm:\n",
    "            ops_data[\"ROT RPM\"] = rot_rpm.group(1)\n",
    "        on_btm = re.search(r\"ON\\s*BTM\\s*TRQ[:;]?\\s*([\\d\\-K]+)\", l, re.IGNORECASE)\n",
    "        if on_btm:\n",
    "            ops_data[\"ON BTM TRQ\"] = on_btm.group(1)\n",
    "        off_btm = re.search(r\"OFF\\s*BTM\\s*TRQ[:;]?\\s*([\\d\\-K]+)\", l, re.IGNORECASE)\n",
    "        if off_btm:\n",
    "            ops_data[\"OFF BTM TRQ\"] = off_btm.group(1)\n",
    "        gas_units = re.search(r\"GAS:\\s*([\\d,]+)\\s*UNITS\", l, re.IGNORECASE)\n",
    "        if gas_units:\n",
    "            ops_data[\"GAS\"][\"Units\"] = gas_units.group(1)\n",
    "        flare = re.search(r\"(NO\\s*FLARE|FLARE\\s*ON|FLARE\\s*\\S+)\", l, re.IGNORECASE)\n",
    "        if flare:\n",
    "            ops_data[\"GAS\"][\"Flare\"] = flare.group(1)\n",
    "        mw_in = re.search(r\"MW\\s*IN\\s*([\\d.]+)\", l, re.IGNORECASE)\n",
    "        if mw_in:\n",
    "            ops_data[\"MW\"][\"In\"] = mw_in.group(1)\n",
    "        mw_out = re.search(r\"OUT\\s*([\\d.]+)\", l, re.IGNORECASE)\n",
    "        if mw_out:\n",
    "            ops_data[\"MW\"][\"Out\"] = mw_out.group(1)\n",
    "    \n",
    "    \n",
    "        # Capture Targets.\n",
    "        if re.search(r\"TARGET\\s*#\\d+\", l, re.IGNORECASE):\n",
    "            ops_data[\"Targets\"].append(l)\n",
    "        # Other lines become Observations if they don't match key patterns.\n",
    "        if not re.search(r\"(F/|ROTATE|GPM:|MTR RPM:|SPP:)\", l, re.IGNORECASE):\n",
    "            ops_data[\"Observations\"].append(l)\n",
    "\n",
    "    # Remove duplicate observations.\n",
    "    ops_data[\"Observations\"] = list(dict.fromkeys(ops_data[\"Observations\"]))\n",
    "    return ops_data\n",
    "\n",
    "\n",
    "    #     # Capture Targets (lines with \"TARGET #\")\n",
    "    #     if re.search(r\"TARGET\\s*#\\d+\", l, re.IGNORECASE):\n",
    "    #         ops_data[\"Targets\"].append(l)\n",
    "    #     # Other lines become Observations if they dont match key patterns.\n",
    "    #     if not re.search(r\"(F/|ROTATE|GPM:|MTR RPM:|SPP:)\", l, re.IGNORECASE):\n",
    "    #         ops_data[\"Observations\"].append(l)\n",
    "\n",
    "    # # Remove duplicate observations.\n",
    "    # ops_data[\"Observations\"] = list(dict.fromkeys(ops_data[\"Observations\"]))\n",
    "    # return ops_data\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# parse_all_rows_from_text\n",
    "# ---------------------------------------------------------------------\n",
    "def parse_all_rows_from_text(ocr_text):\n",
    "    \"\"\"\n",
    "    Uses a lookahead regex to split the full OCR text into multiple row chunks.\n",
    "    Each row chunk is then processed by splitting into tokens.\n",
    "    Assumes the header for each row starts with two time stamps.\n",
    "    \"\"\"\n",
    "    # Use lookahead to split where a new row starts.\n",
    "    row_chunks = re.split(r\"(?=\\d{2}:\\d{2}\\s+\\d{2}:\\d{2})\", ocr_text)\n",
    "    rows = []\n",
    "    for chunk in row_chunks:\n",
    "        chunk = chunk.strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        # Split chunk into tokens.\n",
    "        tokens = re.split(r\"\\s+\", chunk)\n",
    "        if len(tokens) < 10:\n",
    "            continue  # Skip if too few tokens.\n",
    "        # Assign fixed tokens based on our assumed structure.\n",
    "        row_dict = {\n",
    "            \"From\": tokens[0],\n",
    "            \"To\": tokens[1],\n",
    "            \"Hours\": tokens[2],\n",
    "            \"Depth Start\": tokens[3],\n",
    "            \"Depth End\": tokens[4],\n",
    "            \"Phase\": \" \".join(tokens[5:9]),  # tokens 5 to 8\n",
    "            \"Activity\": tokens[9],\n",
    "            \"Operations Description\": {}\n",
    "        }\n",
    "        # The remaining tokens form the raw operations description.\n",
    "        operations_text = \" \".join(tokens[10:])\n",
    "        row_dict[\"Operations Description\"] = parse_operations_description(operations_text)\n",
    "        rows.append(row_dict)\n",
    "    return rows\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# main_time_breakdown_pipeline\n",
    "# ---------------------------------------------------------------------\n",
    "def main_time_breakdown_pipeline():\n",
    "    # Set your image path.\n",
    "    img_path = \"dbfs:/mnt/mini-proj-dd/cropped_sections/page_1_section_14.png\"\n",
    "    try:\n",
    "        img = safe_read_image(img_path)\n",
    "        logger.info(\"Image loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        return\n",
    "\n",
    "    # Preprocess the image.\n",
    "    thresh_img = preprocess_image(img, debug=True)\n",
    "\n",
    "    # OCR the entire image.\n",
    "    full_text = single_box_ocr(thresh_img, debug=True)\n",
    "\n",
    "    # Dynamically extract all rows using our lookahead-based parser.\n",
    "    time_breakdown_list = parse_all_rows_from_text(full_text)\n",
    "    if not time_breakdown_list:\n",
    "        logger.error(\"No rows were detected. Please check the OCR output and header format.\")\n",
    "        return\n",
    "\n",
    "    # Build final JSON output.\n",
    "    final_output = {\"TIME BREAKDOWN\": time_breakdown_list}\n",
    "    logger.info(\"===== FINAL TIME BREAKDOWN DATA =====\")\n",
    "    logger.info(json.dumps(final_output, indent=4))\n",
    "\n",
    "    # Create a DataFrame (note: nested fields will be flattened).\n",
    "    df = pd.json_normalize(final_output[\"TIME BREAKDOWN\"])\n",
    "    print(\"----- Extracted Time Breakdown DataFrame -----\")\n",
    "    print(df)\n",
    "\n",
    "    # Save results as JSON and CSV.\n",
    "    output_folder = \"dbfs:/mnt/mini-proj-dd/final_time_breakdown_results\"\n",
    "    local_folder = output_folder.replace(\"dbfs:\", \"/dbfs\")\n",
    "    os.makedirs(local_folder, exist_ok=True)\n",
    "    out_json = os.path.join(local_folder, \"time_breakdown_data.json\")\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(final_output, f, indent=4)\n",
    "    logger.info(f\"JSON saved to {out_json}\")\n",
    "    out_csv = os.path.join(local_folder, \"time_breakdown_data.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    logger.info(f\"CSV saved to {out_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_time_breakdown_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e4c2e6f-6746-4130-8a87-46eb1c24208b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2560510997848260,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "preprocess_sections_timebreakdown",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
